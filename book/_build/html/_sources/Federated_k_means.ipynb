{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated K-Means\n",
    "In this section, we attempt to form a federated k-means algorithm to preserve the privacy of the individual labs. \n",
    "\n",
    "**Federated learning:**  First introduced by Google, federated learning trains an algorithm across multiple decentralized servers/devices with local data samples. There is **no** data sharing between servers. This is in contrast to the standard way of training a machine learning algorithm where all of the training dataset is uploaded to one server. This technique address critical issues relating to data privacy and security \n",
    "\n",
    "```{figure} /images/Federated_K-Means.png\n",
    "---\n",
    "name: workflow-fkm\n",
    "---\n",
    "Workflow of Federated K-means *Note: child servers 2 to 7 are obscured*\n",
    "```\n",
    "\n",
    "## Child Server (Study):\n",
    "### 1. Base k-means:\n",
    "The original K-Means algorithm is only trained on a given dataset once. There is no update method. In this approach, we split a study’s dataset 75:25. 75% of a study’s data is trained using normal K-means as part of step 1. It is important to note that all child servers are independent of one another.\n",
    "### 2. Update:\n",
    "In an attempt to resemble real-world federated learning where a new data point is generated on a device, we add an update step to base K-means. Rather than recompute k-means, we iterate through the remaining 25% of data and perform the following steps \n",
    "1. Convert new data point to NumPy array\n",
    "2. Find the minimum Euclidean distance between that new point $X_{i}$  and the cluster centres (T) to find the closest cluster centre ($C_{i}$).\n",
    "$\n",
    "Minimum Distance = min((X_{i} - C_{1})^2.............(X_{i} - C_{T})^2)\n",
    "$\n",
    "3. Transform the cluster centre $C_{i}$ by doing the operation below. N equals the number of participants assigned to that cluster thus far i.e. before the new data point :\n",
    "$\n",
    "TransformedClusterCentre = \\frac{((C_{i} * N) + X_{i})}{N+1}\n",
    "$\n",
    "3. Then, the new data point is added to the cluster, and the transformed cluster centre is the new cluster centre.\n",
    "## Parent Server:\n",
    "### 3. Aggregate & Compute K-means run:\n",
    "Once all child devices have completed their update phase, their cluster centres are added to the parent server. Then, we compute another K-means run to find the optimal number of k centroids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_reduced_2d = pd.read_csv('data/dim_reduced_2d.tsv', sep=\"\\t\")\n",
    "dim_reduced_3d = pd.read_csv('data/dim_reduced_3d.tsv', sep=\"\\t\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we are just creating variables to hold the data from each study. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "studies_list = ['Horstmann', 'Kjome', 'Maia', 'SteingroverInPrep', 'Premkumar','Wood', 'Worthy', 'Ahn']\n",
    "for study in studies_list:\n",
    "    globals()[f'{study}_study'] = dim_reduced_2d[dim_reduced_2d['study'] == study]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Child K-means class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class child_kmeans(KMeans):\n",
    "    \"\"\"\n",
    "    A python class that executes the original k-means algorithm on 75% of the available data \n",
    "        from a study. Leftover data is used to update the cluster centres\n",
    "        as described above. Inherits from scikit-learn's K-means class\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                df,\n",
    "                n_clusters):\n",
    "        super().__init__(n_clusters=n_clusters, random_state=42)\n",
    "        self.df = df\n",
    "        self.update_df, self.base_df = np.split(df, [int(.25*len(df))])\n",
    "        # map cluster index to number of particpants e.g. <0:77> \n",
    "        self.cluster_index_num = dict()\n",
    "        # map cluster index to number of particpants e.g. <0:array([-0.96292967,  1.03276864])> \n",
    "        # Necessary as numpy array is unhashable \n",
    "        self.index_cluster_centre = dict()\n",
    "\n",
    "    \n",
    "    def find_closest_cluster(self, new_data_point):\n",
    "        min_dist =  float(\"inf\")\n",
    "        min_cluster_centre= None\n",
    "        cluster_index = None\n",
    "        for current_key, cluster in self.index_cluster_centre.items():\n",
    "            current_dist = (np.sum(np.square(new_data_point - cluster)))\n",
    "            if current_dist < min_dist:\n",
    "                min_cluster_centre = cluster\n",
    "                min_dist = current_dist\n",
    "                cluster_index = current_key\n",
    "        return cluster_index, min_cluster_centre\n",
    "\n",
    "    def update(self):\n",
    "        for index, row in self.update_df.iterrows():\n",
    "            new_data_point = np.array(row)\n",
    "            cluster_index, closest_cluster = self.find_closest_cluster(new_data_point)\n",
    "            num_subjects = self.cluster_index_num[cluster_index]\n",
    "            self.index_cluster_centre[cluster_index] = np.divide(((closest_cluster * num_subjects) + new_data_point), num_subjects+1)\n",
    "            self.cluster_index_num[cluster_index] += 1\n",
    "    \n",
    "\n",
    "    def create_maps(self):\n",
    "        cluster_indexes, counts = np.unique(self.labels_, return_counts=True)\n",
    "        self.cluster_index_num = {cluster_index:count for cluster_index, count in zip(cluster_indexes, counts) }\n",
    "        self.index_cluster_centre = {cluster_index: self.cluster_centers_[cluster_index] for cluster_index in cluster_indexes}\n",
    "\n",
    "    def run(self):\n",
    "        super().fit(self.base_df)\n",
    "        self.create_maps()\n",
    "        self.update() \n",
    "        updated_cluster_centres = [np.array(cluster_centre) for cluster_centre in self.index_cluster_centre.values()] \n",
    "        return updated_cluster_centres \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parent K-means class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class parent_kmeans(KMeans):\n",
    "    \"\"\"\n",
    "    A python class that retrieves cluster centres from\n",
    "        each study, and then computes another k-means algorithim \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,n_clusters) -> None:\n",
    "        super().__init__(n_clusters=n_clusters, random_state=42)\n",
    "        self.cluster_centres_studies = []\n",
    "        self.federated_cluster_centres = None\n",
    "\n",
    "    def add(self, cluster_centre):\n",
    "        self.cluster_centres_studies.extend(cluster_centre)\n",
    "\n",
    "    def update_cluster_centre(self):\n",
    "        super().fit(self.cluster_centres_studies)\n",
    "    \n",
    "    def get_new_centres(self):\n",
    "        return self.cluster_centers_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_server = parent_kmeans(n_clusters=3)\n",
    "\n",
    "for study in studies_list:\n",
    "    # First retrieving the cluster centres from a study\n",
    "    study_cluster_centres = child_kmeans(globals()[f'{study}_study'].iloc[:,2:], n_clusters=3).run()\n",
    "    # Adding that information to the parent server \n",
    "    parent_server.add(cluster_centre=study_cluster_centres)\n",
    "\n",
    "# Calculating the new federated cluster centres\n",
    "parent_server.update_cluster_centre()\n",
    "# Retrieving the cluster centres from Federated K-means and normal K-means    \n",
    "fkm_cluster_centres = parent_server.get_new_centres()\n",
    "km_clusters_centres = KMeans(n_clusters=3,random_state=42).fit(dim_reduced_2d.iloc[:,2:]).cluster_centers_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before evaluating the results, an important consideration should be noted. As with any K-Means algorithm, results may vary with each run or input seed of the algorithm as the algorithm's performance is heavily dependent on the initial clusters chosen. We evaluate our algorithm on the 2d dataset with k=3 and seed=42. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calulate_SSE(df, fkm_cluster_centres, km_clusters_centres):\n",
    "    \"\"\"\n",
    "    Calculates k-mean's objective function (Sum Square Error) for both federared K-means\n",
    "        algorithm and the original K-means algorithm \n",
    "    \n",
    "    :param df: Dataframe containing data from the many labs paper\n",
    "    :param fkm_cluster_centres: Cluster centres of the Federated K-means Algo.\n",
    "    :param km_clusters_centres: Cluster centres of the original K-means Algo.\n",
    "    \"\"\"\n",
    "    df[\"fkm_SSE\"] = None\n",
    "    df[\"km_SSE\"] = None\n",
    "    for index, subject in df.iterrows():\n",
    "        subject_dim = np.array(subject[[\"component_1\",\"component_2\"]])\n",
    "        df.iloc[index, -2] = min([(np.sum(np.square(subject_dim - cluster))) for cluster in fkm_cluster_centres])\n",
    "        df.iloc[index, -1] =  min([np.sum(np.square(subject_dim - cluster)) for cluster in km_clusters_centres])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_2d_df = calulate_SSE(dim_reduced_2d, fkm_cluster_centres, km_clusters_centres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Federated K-mean SSE: 1498.2800508864195\n",
      "K-mean SSE: 1362.1960103577228\n"
     ]
    }
   ],
   "source": [
    "print(f'Federated K-mean SSE: {evaluate_2d_df[\"fkm_SSE\"].sum()}')\n",
    "print(f'K-mean SSE: {evaluate_2d_df[\"km_SSE\"].sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our chosen approach only results in a approximate 10% increase in SSE compared to the original centralized K-means algorithm."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
