%% Generated by Sphinx.
\def\sphinxdocclass{jupyterBook}
\documentclass[letterpaper,10pt,english]{jupyterBook}
\ifdefined\pdfpxdimen
   \let\sphinxpxdimen\pdfpxdimen\else\newdimen\sphinxpxdimen
\fi \sphinxpxdimen=.75bp\relax
\ifdefined\pdfimageresolution
    \pdfimageresolution= \numexpr \dimexpr1in\relax/\sphinxpxdimen\relax
\fi
%% let collapsible pdf bookmarks panel have high depth per default
\PassOptionsToPackage{bookmarksdepth=5}{hyperref}
%% turn off hyperref patch of \index as sphinx.xdy xindy module takes care of
%% suitable \hyperpage mark-up, working around hyperref-xindy incompatibility
\PassOptionsToPackage{hyperindex=false}{hyperref}
%% memoir class requires extra handling
\makeatletter\@ifclassloaded{memoir}
{\ifdefined\memhyperindexfalse\memhyperindexfalse\fi}{}\makeatother

\PassOptionsToPackage{warn}{textcomp}

\catcode`^^^^00a0\active\protected\def^^^^00a0{\leavevmode\nobreak\ }
\usepackage{cmap}
\usepackage{fontspec}
\defaultfontfeatures[\rmfamily,\sffamily,\ttfamily]{}
\usepackage{amsmath,amssymb,amstext}
\usepackage{polyglossia}
\setmainlanguage{english}



\setmainfont{FreeSerif}[
  Extension      = .otf,
  UprightFont    = *,
  ItalicFont     = *Italic,
  BoldFont       = *Bold,
  BoldItalicFont = *BoldItalic
]
\setsansfont{FreeSans}[
  Extension      = .otf,
  UprightFont    = *,
  ItalicFont     = *Oblique,
  BoldFont       = *Bold,
  BoldItalicFont = *BoldOblique,
]
\setmonofont{FreeMono}[
  Extension      = .otf,
  UprightFont    = *,
  ItalicFont     = *Oblique,
  BoldFont       = *Bold,
  BoldItalicFont = *BoldOblique,
]



\usepackage[Bjarne]{fncychap}
\usepackage[,numfigreset=1,mathnumfig]{sphinx}

\fvset{fontsize=\small}
\usepackage{geometry}


% Include hyperref last.
\usepackage{hyperref}
% Fix anchor placement for figures with captions.
\usepackage{hypcap}% it must be loaded after hyperref.
% Set up styles of URL: it should be placed after hyperref.
\urlstyle{same}


\usepackage{sphinxmessages}



        % Start of preamble defined in sphinx-jupyterbook-latex %
         \usepackage[Latin,Greek]{ucharclasses}
        \usepackage{unicode-math}
        % fixing title of the toc
        \addto\captionsenglish{\renewcommand{\contentsname}{Contents}}
        \hypersetup{
            pdfencoding=auto,
            psdextra
        }
        % End of preamble defined in sphinx-jupyterbook-latex %
        

\title{Heart Disease Prediction Task}
\date{Nov 07, 2022}
\release{}
\author{Anthony Reidy}
\newcommand{\sphinxlogo}{\vbox{}}
\renewcommand{\releasename}{}
\makeindex
\begin{document}

\pagestyle{empty}
\sphinxmaketitle
\pagestyle{plain}
\sphinxtableofcontents
\pagestyle{normal}
\phantomsection\label{\detokenize{Introduction::doc}}


\sphinxAtStartPar
Heart disease, alternatively known as cardiovascular disease, encases various conditions that impact the heart and is the primary basis of death worldwide over the span of the past few decades. Approximately 10,000 people die in Ireland from Cardiovascular Disease each year, accounting for 36\% of deaths per annum {[}\hyperlink{cite.References:id3}{\sphinxstyleemphasis{Worrying statistics about heart disease}, 2020}{]}. That’s despite the fact that 80\% of all heart disease is deemed preventable. This investigations aims to explore a \sphinxhref{https://www.kaggle.com/code/mushfirat/heartdisease-eda-prediction/notebook}{kaggle dataset} and build a model that can predict the likelihood of a patient having heart disease. More specifically, with this dataset, we would like to see if we can develop a good model to predict if a person has heart disease and what \sphinxstyleemphasis{factors} can be attributed to heart disease most directly. We will be tackling this question with the usage of different regression techniques and algorithms.

\begin{DUlineblock}{0em}
\item[] \sphinxstylestrong{\Large Description of Dataset}
\end{DUlineblock}

\sphinxAtStartPar
We uses an existing dataset from \sphinxhref{https://www.kaggle.com/code/mushfirat/heartdisease-eda-prediction/notebook}{Kaggle}. The dataset comprises approx 320,0000 instances and 14 attributes.

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
No personal identifiable information of the patients are recorded in the dataset.
\end{sphinxadmonition}

\sphinxAtStartPar
The table below summarizes the multiple columns used in this investigation.


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|}
\hline
\sphinxstyletheadfamily 
\sphinxAtStartPar
Name
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Description
\\
\hline
\sphinxAtStartPar
HeartDisease
&
\sphinxAtStartPar
Respondents that have ever reported having coronary heart disease (CHD) or myocardial infarction (MI).
\\
\hline
\sphinxAtStartPar
BMI
&
\sphinxAtStartPar
Body Mass Index.
\\
\hline
\sphinxAtStartPar
Smoking
&
\sphinxAtStartPar
Have you smoked at least 100 cigarettes in your entire life?
\\
\hline
\sphinxAtStartPar
AlcoholDrinking
&
\sphinxAtStartPar
Heavy drinkers (adult men having more than 14 drinks per week and adult women having more than 7 drinks per week
\\
\hline
\sphinxAtStartPar
Stroke
&
\sphinxAtStartPar
(Ever told) (you had) a stroke?
\\
\hline
\sphinxAtStartPar
PhysicalHealth
&
\sphinxAtStartPar
Now thinking about your physical health, which includes physical illness and injury, for how many days during the past 30 days was your physical health not good? (0\sphinxhyphen{}30 days).
\\
\hline
\sphinxAtStartPar
MentalHealth
&
\sphinxAtStartPar
Thinking about your mental health, for how many days during the past 30 days was your mental health not good? (0\sphinxhyphen{}30 days).
\\
\hline
\sphinxAtStartPar
DiffWalking
&
\sphinxAtStartPar
Do you have serious difficulty walking or climbing stairs?
\\
\hline
\sphinxAtStartPar
Sex
&
\sphinxAtStartPar
Are you male or female?
\\
\hline
\sphinxAtStartPar
AgeCategory
&
\sphinxAtStartPar
Fourteen\sphinxhyphen{}level age category. (then calculated the mean)
\\
\hline
\sphinxAtStartPar
Race
&
\sphinxAtStartPar
Imputed race/ethnicity value.
\\
\hline
\sphinxAtStartPar
Diabetic
&
\sphinxAtStartPar
(Ever told) (you had) diabetes?
\\
\hline
\sphinxAtStartPar
PhysicalActivity
&
\sphinxAtStartPar
Adults who reported doing physical activity or exercise during the past 30 days other than their regular job.
\\
\hline
\sphinxAtStartPar
GenHealth
&
\sphinxAtStartPar
Would you say that in general your health is…
\\
\hline
\sphinxAtStartPar
SleepTime
&
\sphinxAtStartPar
On average, how many hours of sleep do you get in a 24\sphinxhyphen{}hour period?
\\
\hline
\sphinxAtStartPar
Asthma
&
\sphinxAtStartPar
(Ever told) (you had) asthma?
\\
\hline
\sphinxAtStartPar
KidneyDisease
&
\sphinxAtStartPar
Not including kidney stones, bladder infection or incontinence, were you ever told you had kidney disease?
\\
\hline
\sphinxAtStartPar
SkinCancer
&
\sphinxAtStartPar
(Ever told) (you had) skin cancer?
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

\begin{DUlineblock}{0em}
\item[] \sphinxstylestrong{\Large Methodology}
\end{DUlineblock}

\sphinxAtStartPar
Methodology will follows a typical data science project: from understanding the dataset through exploratory data analysis, data preparation, model buildings and finally model evaluation. We seek to build a model that predicts heart disease, a binary outcome
In this investigation, we seek to use the K\sphinxhyphen{}means clustering approach to segment the patients into well\sphinxhyphen{}defined groups.
To start, we perform an initial data exploration to perform transformations \& data sanitization checks; acquire rudimentary statistics of the datasets; perform data augmentation; create exploratory visualizations. Next, we perform cluster analysis and evaluate our clusters using metrics such as Silhouette Coefficient and an Elbow curve.
These clusters represent participants that exhibit similar risk factors for heart disease and may have similar underlying determinants of health such as their age, BMI, whether the smoke or have asthma. Next, we envision the probability of developing heart disease in the patients. Finally, we conclude with the most important outcomes of our work.

\sphinxstepscope


\chapter{Initial Data Exploration}
\label{\detokenize{Initial_Data_Exploration:initial-data-exploration}}\label{\detokenize{Initial_Data_Exploration::doc}}
\sphinxAtStartPar
The purpose of our initial data exploration is to:




\section{Importing required libraries}
\label{\detokenize{Initial_Data_Exploration:importing-required-libraries}}
\sphinxAtStartPar
Data processing

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{preprocessing} \PYG{k+kn}{import} \PYG{n}{StandardScaler}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{decomposition} \PYG{k+kn}{import} \PYG{n}{PCA}
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{import} \PYG{n+nn}{copy}
\PYG{k+kn}{import} \PYG{n+nn}{random}
\PYG{k+kn}{import} \PYG{n+nn}{pickle}
\PYG{k+kn}{import} \PYG{n+nn}{json}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
Data Visualization

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}
\PYG{k+kn}{import} \PYG{n+nn}{seaborn} \PYG{k}{as} \PYG{n+nn}{sns}
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{patches} \PYG{k}{as} \PYG{n+nn}{mpatches}
\PYG{k+kn}{import} \PYG{n+nn}{plotly}\PYG{n+nn}{.}\PYG{n+nn}{graph\PYGZus{}objects} \PYG{k}{as} \PYG{n+nn}{go}
\PYG{k+kn}{import} \PYG{n+nn}{squarify} 
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{style}\PYG{o}{.}\PYG{n}{use}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ggplot}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
Code Styling

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{typing} \PYG{k+kn}{import} \PYG{n}{List}\PYG{p}{,} \PYG{n}{Dict}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
Read in dataframe and bried inspection of the data.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{heart\PYGZus{}disease}\PYG{p}{:} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{data/heart\PYGZus{}2020\PYGZus{}cleaned.csv}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}

\section{Data Cleaning}
\label{\detokenize{Initial_Data_Exploration:data-cleaning}}
\sphinxAtStartPar
Check for missing values

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Does the heart disease dataset contain any null values? }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{heart\PYGZus{}disease}\PYG{o}{.}\PYG{n}{isnull}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{any}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{any}\PYG{p}{(}\PYG{p}{)}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
Does the heart disease dataset contain any null values? False
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Next, we calculate the basic statistics of each data set. This is a trivial step, and it is designed to increase understanding of the computational problem.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Unique values in the heart disease dataset, stratified by column:}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{sep}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{k}{for} \PYG{n}{col} \PYG{o+ow}{in} \PYG{n}{heart\PYGZus{}disease}\PYG{o}{.}\PYG{n}{columns}\PYG{p}{:}
    \PYG{k}{if} \PYG{n}{heart\PYGZus{}disease}\PYG{p}{[}\PYG{n}{col}\PYG{p}{]}\PYG{o}{.}\PYG{n}{nunique}\PYG{p}{(}\PYG{p}{)} \PYG{o}{\PYGZlt{}} \PYG{l+m+mi}{20}\PYG{p}{:}
        \PYG{k}{if} \PYG{n}{heart\PYGZus{}disease}\PYG{p}{[}\PYG{n}{col}\PYG{p}{]}\PYG{o}{.}\PYG{n}{nunique}\PYG{p}{(}\PYG{p}{)} \PYG{o}{\PYGZlt{}} \PYG{l+m+mi}{3}\PYG{p}{:} 
            \PYG{n}{heart\PYGZus{}disease}\PYG{p}{[}\PYG{n}{col}\PYG{p}{]}\PYG{o}{.}\PYG{n}{value\PYGZus{}counts}\PYG{p}{(}\PYG{n}{normalize}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{kind}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{barh}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{title}\PYG{o}{=}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Unique values of }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{col}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{ column}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{p}{)}
            \PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
Unique values in the heart disease dataset, stratified by column:
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Initial_Data_Exploration_11_1}.png}

\noindent\sphinxincludegraphics{{Initial_Data_Exploration_11_2}.png}

\noindent\sphinxincludegraphics{{Initial_Data_Exploration_11_3}.png}

\noindent\sphinxincludegraphics{{Initial_Data_Exploration_11_4}.png}

\noindent\sphinxincludegraphics{{Initial_Data_Exploration_11_5}.png}

\noindent\sphinxincludegraphics{{Initial_Data_Exploration_11_6}.png}

\noindent\sphinxincludegraphics{{Initial_Data_Exploration_11_7}.png}

\noindent\sphinxincludegraphics{{Initial_Data_Exploration_11_8}.png}

\noindent\sphinxincludegraphics{{Initial_Data_Exploration_11_9}.png}

\noindent\sphinxincludegraphics{{Initial_Data_Exploration_11_10}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
From this we note the this dataset is imbalanced .i.e, data set with skewed class proportions. This is a common problem in classification problems. We will address this issue later in the notebook.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{heart\PYGZus{}disease}\PYG{o}{.}\PYG{n}{describe}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
                 BMI  PhysicalHealth   MentalHealth      SleepTime
count  319795.000000    319795.00000  319795.000000  319795.000000
mean       28.325399         3.37171       3.898366       7.097075
std         6.356100         7.95085       7.955235       1.436007
min        12.020000         0.00000       0.000000       1.000000
25\PYGZpc{}        24.030000         0.00000       0.000000       6.000000
50\PYGZpc{}        27.340000         0.00000       0.000000       7.000000
75\PYGZpc{}        31.420000         2.00000       3.000000       8.000000
max        94.850000        30.00000      30.000000      24.000000
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{pairplot}\PYG{p}{(}\PYG{n}{heart\PYGZus{}disease}\PYG{p}{[}
    \PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{HeartDisease}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{BMI}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{PhysicalHealth}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{MentalHealth}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{SleepTime}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}
    \PYG{p}{]}\PYG{p}{,} \PYG{n}{hue}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{HeartDisease}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{plot\PYGZus{}kws}\PYG{o}{=}\PYG{n+nb}{dict}\PYG{p}{(}\PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{80}\PYG{p}{,} \PYG{n}{edgecolor}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{white}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{linewidth}\PYG{o}{=}\PYG{l+m+mf}{2.5}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{Initial_Data_Exploration_14_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Surprisingly, the distribution of BMI, mental health and sleep time don’t seem to be significantly different when comparing with people who \sphinxstylestrong{do or do not} have heart disease. However, the mental health do seem to have a strong correlation with heart disease.


\chapter{Data Visualization}
\label{\detokenize{Initial_Data_Exploration:data-visualization}}
\sphinxAtStartPar
To familiarize ourself with the data, we generate plots that seek to illuminate some research questions.
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Are people from certain backgrounds more likely to have heart disease, given their ethnicity?

\item {} 
\sphinxAtStartPar
Is the percentage of females and males with heart disease similar?

\item {} 
\sphinxAtStartPar
Do peoples \sphinxstyleemphasis{reported} physical activity correlate with their physical health?

\item {} 
\sphinxAtStartPar
Which risk factors are most correlated with heart disease?

\item {} 
\sphinxAtStartPar
What is the flow of patients from  their age to substance misuse to if they have an aliment listed in the dataset to  their likelihood of contracting heart disease?

\end{enumerate}

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{create\PYGZus{}stacked\PYGZus{}bar\PYGZus{}hart}\PYG{p}{(}\PYG{n}{heart\PYGZus{}disease}\PYG{p}{:} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{)} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZgt{}} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    Args:}
\PYG{l+s+sd}{        heart\PYGZus{}disease (pd.DataFrame): the original heart disease kaggle dataset}

\PYG{l+s+sd}{    Returns:}
\PYG{l+s+sd}{        plt.figure: a stacked bar chart of the percentage of people }
\PYG{l+s+sd}{            with heart disease and without heart disease, stratified by race}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}

    \PYG{c+c1}{\PYGZsh{} top bar \PYGZhy{}\PYGZgt{} sum all values(HeartDisease=No and HeartDisease=Yes) to find y position of the bars}
    \PYG{n}{total}\PYG{p}{:} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame} \PYG{o}{=} \PYG{n}{heart\PYGZus{}disease}\PYG{o}{.}\PYG{n}{groupby}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Race}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{o}{.}\PYG{n}{count}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{reset\PYGZus{}index}\PYG{p}{(}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} bar chart 1 \PYGZhy{}\PYGZgt{} top bars (group of \PYGZsq{}HeartDisease=No\PYGZsq{})}
    \PYG{n}{bar1}\PYG{p}{:} \PYG{n}{sns}\PYG{o}{.}\PYG{n}{barplot} \PYG{o}{=} \PYG{n}{sns}\PYG{o}{.}\PYG{n}{barplot}\PYG{p}{(}\PYG{n}{x}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Race}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}  \PYG{n}{y}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{HeartDisease}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{data}\PYG{o}{=}\PYG{n}{total}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{red}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} bottom bar \PYGZhy{}\PYGZgt{}  take only HeartDisease=Yes values from the data}
    \PYG{n}{HeartDisease}\PYG{p}{:} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame} \PYG{o}{=} \PYG{n}{heart\PYGZus{}disease}\PYG{p}{[}\PYG{n}{heart\PYGZus{}disease}\PYG{o}{.}\PYG{n}{HeartDisease}\PYG{o}{==}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Yes}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{groupby}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Race}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{o}{.}\PYG{n}{count}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{reset\PYGZus{}index}\PYG{p}{(}\PYG{p}{)}


    \PYG{c+c1}{\PYGZsh{} bar chart 2 \PYGZhy{}\PYGZgt{} bottom bars (group of \PYGZsq{}HeartDisease=Yes\PYGZsq{})}
    \PYG{n}{bar2}\PYG{p}{:} \PYG{n}{sns}\PYG{o}{.}\PYG{n}{barplot} \PYG{o}{=} \PYG{n}{sns}\PYG{o}{.}\PYG{n}{barplot}\PYG{p}{(}\PYG{n}{x}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Race}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{y}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{HeartDisease}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{data}\PYG{o}{=}\PYG{n}{HeartDisease}\PYG{p}{,} \PYG{n}{errorbar}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{,}  \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{green}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} add legend}
    \PYG{n}{top\PYGZus{}bar}\PYG{p}{:} \PYG{n}{mpatches}\PYG{o}{.}\PYG{n}{Patch} \PYG{o}{=} \PYG{n}{mpatches}\PYG{o}{.}\PYG{n}{Patch}\PYG{p}{(}\PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{red}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{HeartDisease = No}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
    \PYG{n}{bottom\PYGZus{}bar}\PYG{p}{:} \PYG{n}{mpatches}\PYG{o}{.}\PYG{n}{Patch} \PYG{o}{=} \PYG{n}{mpatches}\PYG{o}{.}\PYG{n}{Patch}\PYG{p}{(}\PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{green}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{HeartDisease = Yes}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{legend}\PYG{p}{(}\PYG{n}{handles}\PYG{o}{=}\PYG{p}{[}\PYG{n}{top\PYGZus{}bar}\PYG{p}{,} \PYG{n}{bottom\PYGZus{}bar}\PYG{p}{]}\PYG{p}{)}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Number of Patients}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{xticks}\PYG{p}{(}\PYG{n}{rotation}\PYG{o}{=}\PYG{l+m+mi}{45}\PYG{p}{,}\PYG{n}{ha}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{right}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
    \PYG{c+c1}{\PYGZsh{} show the graph}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{create\PYGZus{}tree\PYGZus{}diagram}\PYG{p}{(}\PYG{n}{heart\PYGZus{}disease}\PYG{p}{:}\PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{)} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZgt{}} \PYG{n}{squarify}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    Args:}
\PYG{l+s+sd}{        heart\PYGZus{}disease (pd.DataFrame): the original heart disease kaggle dataset}

\PYG{l+s+sd}{    Returns:}
\PYG{l+s+sd}{        squarify.plot: A tree diagram displaying the number of males and females }
\PYG{l+s+sd}{            that do and don\PYGZsq{}t have heart disease}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{n}{heart\PYGZus{}disease} \PYG{o}{=} \PYG{n}{heart\PYGZus{}disease}\PYG{o}{.}\PYG{n}{copy}\PYG{p}{(}\PYG{p}{)}
    \PYG{n}{tree\PYGZus{}diagram\PYGZus{}df}\PYG{p}{:} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame} \PYG{o}{=} \PYG{n}{heart\PYGZus{}disease}\PYG{o}{.}\PYG{n}{groupby}\PYG{p}{(}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Sex}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{HeartDisease}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}\PYG{o}{.}\PYG{n}{count}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{reset\PYGZus{}index}\PYG{p}{(}\PYG{p}{)}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Sex}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{HeartDisease}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{BMI}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}
    \PYG{n}{tree\PYGZus{}diagram\PYGZus{}df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Gender\PYGZus{}choice}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{tree\PYGZus{}diagram\PYGZus{}df}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Sex}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{HeartDisease}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}\PYG{o}{.}\PYG{n}{agg}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{, }\PYG{l+s+s1}{\PYGZsq{}}\PYG{o}{.}\PYG{n}{join}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
    \PYG{n}{tree\PYGZus{}diagram\PYGZus{}df}\PYG{o}{.}\PYG{n}{rename}\PYG{p}{(}\PYG{n}{columns}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{BMI}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{count}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{\PYGZcb{}}\PYG{p}{,} \PYG{n}{inplace}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
    \PYG{n}{squarify}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{sizes}\PYG{o}{=}\PYG{n}{tree\PYGZus{}diagram\PYGZus{}df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{count}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{n}{tree\PYGZus{}diagram\PYGZus{}df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Gender\PYGZus{}choice}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{alpha}\PYG{o}{=}\PYG{l+m+mf}{.4}\PYG{p}{)}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{axis}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{off}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}

\PYG{k}{def} \PYG{n+nf}{create\PYGZus{}histplot}\PYG{p}{(}\PYG{n}{heart\PYGZus{}disease}\PYG{p}{:}\PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{)} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZgt{}} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    Args:}
\PYG{l+s+sd}{        heart\PYGZus{}disease (pd.DataFrame): the original heart disease kaggle dataset}

\PYG{l+s+sd}{    Returns:}
\PYG{l+s+sd}{        plt.figure: Two histograms displaying the number of days in the past 30 days}
\PYG{l+s+sd}{                    in which patients were physically active. }
\PYG{l+s+sd}{                    Stratified by people\PYGZsq{}s reported physical health}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{n}{heart\PYGZus{}disease} \PYG{o}{=} \PYG{n}{heart\PYGZus{}disease}\PYG{o}{.}\PYG{n}{copy}\PYG{p}{(}\PYG{p}{)}
    \PYG{n}{sns}\PYG{o}{.}\PYG{n}{set}\PYG{p}{(}\PYG{n}{style}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{darkgrid}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
    \PYG{n}{heart\PYGZus{}disease} \PYG{o}{=} \PYG{n}{heart\PYGZus{}disease}\PYG{p}{[}\PYG{n}{heart\PYGZus{}disease}\PYG{o}{.}\PYG{n}{PhysicalHealth} \PYG{o}{\PYGZgt{}} \PYG{l+m+mi}{0}\PYG{p}{]}
    \PYG{c+c1}{\PYGZsh{} plotting both distibutions on the same figure}
    \PYG{n}{sns}\PYG{o}{.}\PYG{n}{histplot}\PYG{p}{(}\PYG{n}{heart\PYGZus{}disease}\PYG{p}{[}\PYG{n}{heart\PYGZus{}disease}\PYG{o}{.}\PYG{n}{PhysicalActivity} \PYG{o}{==} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Yes}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{PhysicalHealth}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{skyblue}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{People who report doing physical activity}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
    \PYG{n}{sns}\PYG{o}{.}\PYG{n}{histplot}\PYG{p}{(}\PYG{n}{heart\PYGZus{}disease}\PYG{p}{[}\PYG{n}{heart\PYGZus{}disease}\PYG{o}{.}\PYG{n}{PhysicalActivity} \PYG{o}{==} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{No}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{PhysicalHealth}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{red}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{People who do not physical activity}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{legend}\PYG{p}{(}\PYG{p}{)}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{People who report that their physical health (physical illness and injury) was not good? (0\PYGZhy{}30 days).}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
    
    
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{create\PYGZus{}corr\PYGZus{}plot}\PYG{p}{(}\PYG{n}{heart\PYGZus{}disease}\PYG{p}{:} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    Args:}
\PYG{l+s+sd}{        heart\PYGZus{}disease (pd.DataFrame): the original heart disease kaggle dataset}

\PYG{l+s+sd}{    Returns:}
\PYG{l+s+sd}{        squarify.plot: A bar chart displaying the correlation of risk factors vs  heart disease }
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{n}{heart\PYGZus{}disease} \PYG{o}{=} \PYG{n}{heart\PYGZus{}disease}\PYG{o}{.}\PYG{n}{copy}\PYG{p}{(}\PYG{p}{)}
    \PYG{n}{binary\PYGZus{}vars}\PYG{p}{:} \PYG{n}{List}\PYG{p}{[}\PYG{n+nb}{str}\PYG{p}{]} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{HeartDisease}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} 
                        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Smoking}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
                        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{AlcoholDrinking}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
                        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Stroke}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} 
                        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{DiffWalking}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
                        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Diabetic}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
                        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{PhysicalActivity}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
                        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Asthma}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
                        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{KidneyDisease}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
                        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{SkinCancer}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}
    \PYG{k}{for} \PYG{n}{binary\PYGZus{}var} \PYG{o+ow}{in} \PYG{n}{binary\PYGZus{}vars}\PYG{p}{:}
        \PYG{n}{heart\PYGZus{}disease}\PYG{p}{[}\PYG{n}{binary\PYGZus{}var}\PYG{p}{]} \PYG{o}{=} \PYG{n}{heart\PYGZus{}disease}\PYG{p}{[}\PYG{n}{binary\PYGZus{}var}\PYG{p}{]}\PYG{o}{.}\PYG{n}{replace}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Yes}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{No}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+m+mi}{0}\PYG{p}{\PYGZcb{}}\PYG{p}{)}
    \PYG{n}{heart\PYGZus{}disease}\PYG{o}{.}\PYG{n}{Diabetic} \PYG{o}{=} \PYG{n}{heart\PYGZus{}disease}\PYG{o}{.}\PYG{n}{Diabetic}\PYG{o}{.}\PYG{n}{replace}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{No, borderline diabetes}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Yes (during pregnancy)}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{:} \PYG{l+m+mi}{1}\PYG{p}{\PYGZcb{}}\PYG{p}{)}\PYG{o}{.}\PYG{n}{astype}\PYG{p}{(}\PYG{n+nb}{int}\PYG{p}{)}
    \PYG{n}{heart\PYGZus{}disease}\PYG{o}{.}\PYG{n}{corr}\PYG{p}{(}\PYG{p}{)}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{HeartDisease}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}\PYG{o}{.}\PYG{n}{plot}\PYG{o}{.}\PYG{n}{bar}\PYG{p}{(}\PYG{n}{legend}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Correlations of HeartDisease variable vs risk factors}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
    
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{create\PYGZus{}sankey\PYGZus{}chart}\PYG{p}{(}\PYG{n}{sankey\PYGZus{}data}\PYG{p}{:}\PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{)} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZgt{}} \PYG{n}{go}\PYG{o}{.}\PYG{n}{Figure}\PYG{p}{:}
  \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{  Args:}
\PYG{l+s+sd}{      heart\PYGZus{}disease (pd.DataFrame): contains three columns; source target value. }
\PYG{l+s+sd}{                                    Indicates the amount of patients moving through each stage of the sankey chart}

\PYG{l+s+sd}{  Returns:}
\PYG{l+s+sd}{      go.Figure: A sankey chart indicating the flow of patients in the following pattern:}
\PYG{l+s+sd}{                    Age \PYGZhy{}\PYGZgt{} Substance Abuse \PYGZhy{}\PYGZgt{} Alinement \PYGZhy{}\PYGZgt{} Heart Disease outcome}
\PYG{l+s+sd}{        }
\PYG{l+s+sd}{  \PYGZdq{}\PYGZdq{}\PYGZdq{}}
  \PYG{n}{sankey\PYGZus{}data} \PYG{o}{=} \PYG{n}{sankey\PYGZus{}data}\PYG{o}{.}\PYG{n}{copy}\PYG{p}{(}\PYG{p}{)}
  \PYG{c+c1}{\PYGZsh{} Get unique values for both source and target columns}
  \PYG{n}{unique\PYGZus{}source\PYGZus{}target\PYGZus{}values}  \PYG{o}{=} \PYG{n+nb}{list}\PYG{p}{(}\PYG{n}{pd}\PYG{o}{.}\PYG{n}{unique}\PYG{p}{(}\PYG{n}{sankey\PYGZus{}data}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{source}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{target}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}\PYG{o}{.}\PYG{n}{values}\PYG{o}{.}\PYG{n}{ravel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{K}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}

  \PYG{n}{source\PYGZus{}target\PYGZus{}mapping} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{n}{k}\PYG{p}{:}\PYG{n}{v} \PYG{k}{for} \PYG{n}{v}\PYG{p}{,}\PYG{n}{k} \PYG{o+ow}{in} \PYG{n+nb}{enumerate}\PYG{p}{(}\PYG{n}{unique\PYGZus{}source\PYGZus{}target\PYGZus{}values}\PYG{p}{)}\PYG{p}{\PYGZcb{}}
  \PYG{k}{with} \PYG{n+nb}{open}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{data/color\PYGZus{}sankey\PYGZus{}dict.json}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)} \PYG{k}{as} \PYG{n}{f}\PYG{p}{:}
      \PYG{n}{colour\PYGZus{}dict}\PYG{o}{=} \PYG{n}{json}\PYG{o}{.}\PYG{n}{loads}\PYG{p}{(}\PYG{n}{f}\PYG{o}{.}\PYG{n}{read}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
  \PYG{n}{links} \PYG{o}{=} \PYG{n}{sankey\PYGZus{}data}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{source}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{target}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{value}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}\PYG{o}{.}\PYG{n}{copy}\PYG{p}{(}\PYG{p}{)}

  \PYG{n}{links}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{source}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{links}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{source}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{map}\PYG{p}{(}\PYG{n}{source\PYGZus{}target\PYGZus{}mapping}\PYG{p}{)}
  \PYG{n}{links}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{target}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{links}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{target}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{map}\PYG{p}{(}\PYG{n}{source\PYGZus{}target\PYGZus{}mapping}\PYG{p}{)}
  \PYG{n}{links\PYGZus{}dict} \PYG{o}{=} \PYG{n}{links}\PYG{o}{.}\PYG{n}{to\PYGZus{}dict}\PYG{p}{(}\PYG{n}{orient}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{list}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

  \PYG{k}{with} \PYG{n+nb}{open}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{data/link\PYGZus{}colours.pkl}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{rb}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)} \PYG{k}{as} \PYG{n}{f}\PYG{p}{:}
      \PYG{n}{link\PYGZus{}colours} \PYG{o}{=} \PYG{n}{pickle}\PYG{o}{.}\PYG{n}{load}\PYG{p}{(}\PYG{n}{f}\PYG{p}{)}
      
  \PYG{n}{fig} \PYG{o}{=} \PYG{n}{go}\PYG{o}{.}\PYG{n}{Figure}\PYG{p}{(}\PYG{n}{data} \PYG{o}{=} \PYG{p}{[}\PYG{n}{go}\PYG{o}{.}\PYG{n}{Sankey}\PYG{p}{(}
      \PYG{n}{valueformat} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{.0f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{valuesuffix} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{TWh}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{c+c1}{\PYGZsh{} Define nodes}
      \PYG{n}{node} \PYG{o}{=} \PYG{n+nb}{dict}\PYG{p}{(}
        \PYG{n}{pad} \PYG{o}{=} \PYG{l+m+mi}{15}\PYG{p}{,}
        \PYG{n}{thickness} \PYG{o}{=} \PYG{l+m+mi}{15}\PYG{p}{,}
        \PYG{n}{line} \PYG{o}{=} \PYG{n+nb}{dict}\PYG{p}{(}\PYG{n}{color} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{black}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{width} \PYG{o}{=} \PYG{l+m+mf}{0.2}\PYG{p}{)}\PYG{p}{,}
        \PYG{n}{label} \PYG{o}{=}  \PYG{n}{unique\PYGZus{}source\PYGZus{}target\PYGZus{}values}\PYG{p}{,}
        \PYG{n}{color} \PYG{o}{=}  \PYG{n+nb}{list}\PYG{p}{(}\PYG{n}{colour\PYGZus{}dict}\PYG{o}{.}\PYG{n}{values}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
      \PYG{p}{)}\PYG{p}{,}
      \PYG{n}{link} \PYG{o}{=} \PYG{n+nb}{dict}\PYG{p}{(}
        \PYG{n}{source} \PYG{o}{=} \PYG{n}{links\PYGZus{}dict}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{source}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{,}
        \PYG{n}{target} \PYG{o}{=} \PYG{n}{links\PYGZus{}dict}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{target}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{,}
        \PYG{n}{value} \PYG{o}{=} \PYG{n}{links\PYGZus{}dict}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{value}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{,}
        \PYG{n}{color}\PYG{o}{=} \PYG{n}{link\PYGZus{}colours}\PYG{p}{,}
      \PYG{p}{)}
          
      \PYG{p}{)}\PYG{p}{]}
  \PYG{p}{)}
  \PYG{n}{fig}\PYG{o}{.}\PYG{n}{update\PYGZus{}layout}\PYG{p}{(}\PYG{n}{title\PYGZus{}text}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Patient Flow \PYGZlt{}br\PYGZgt{} Age \PYGZhy{}\PYGZgt{} Substance Abuse \PYGZhy{}\PYGZgt{} Alinement \PYGZhy{}\PYGZgt{} Heart Disease outcome}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
                    \PYG{n}{font\PYGZus{}size}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{)}
  \PYG{n}{fig}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{create\PYGZus{}stacked\PYGZus{}bar\PYGZus{}hart}\PYG{p}{(}\PYG{n}{heart\PYGZus{}disease}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{Initial_Data_Exploration_21_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
White people appear to both appear most often in this dataset and have the highest prevalence of heart disease in the their sub\sphinxhyphen{}population. The race with the second most heart disease prevalance is heart disease.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{create\PYGZus{}tree\PYGZus{}diagram}\PYG{p}{(}\PYG{n}{heart\PYGZus{}disease}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{Initial_Data_Exploration_23_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
It is evident that relataive rate of heart disease is higher in men than in women.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{create\PYGZus{}histplot}\PYG{p}{(}\PYG{n}{heart\PYGZus{}disease}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{Initial_Data_Exploration_25_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Surprisingly, it seems that peoples reported physical health does not necessarily align with their physical activity levels.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{sankey\PYGZus{}data}\PYG{p}{:} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{data/full\PYGZus{}sankey\PYGZus{}data.csv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{create\PYGZus{}sankey\PYGZus{}chart}\PYG{p}{(}\PYG{n}{sankey\PYGZus{}data}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}

\section{Feature Engineering}
\label{\detokenize{Initial_Data_Exploration:feature-engineering}}
\sphinxAtStartPar
A lot of our features contain categorical data. We will need to convert these to numerical values. Feature extraction projects the original high\sphinxhyphen{}dimensional features to a new feature space with low dimensionality, while feature selection directly selects a subset of relevant features. Both feature extraction
and feature selection can improve learning performance, increase computational efficiency, decrease memory storage, and build better
generalization models. However, as feature extraction creates a set of new features, further analysis is problematic as we cannot retain the
physical meanings of these features. In contrast, by keeping some of the original features, feature selection maintains the physical
meanings of the original features and gives models better readability and interpretability.

\sphinxAtStartPar
The columns broadly fall into three categories:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZhy{} Binary Categorical variables
  \PYGZhy{}  HeartDisease
  \PYGZhy{}  Smoking 
  \PYGZhy{}  AlcoholDrinking
  \PYGZhy{}  Stroke 
  \PYGZhy{}  DiffWalking
  \PYGZhy{}  Diabet   
  \PYGZhy{}  Physical Activity
  \PYGZhy{}  Asthma
  \PYGZhy{}  Kidney Disease
  \PYGZhy{}  Skin Cancer
  \PYGZhy{}  Sex
\PYGZhy{} Continuous variables
  \PYGZhy{} BMI
\PYGZhy{} Discrete variables
  \PYGZhy{} Sleeptime
  \PYGZhy{} PhsycialHealth
  \PYGZhy{} MentalHealth
\PYGZhy{} Polytomous variables; these are variables with more than two categories
  \PYGZhy{} AgeCategory
  \PYGZhy{} Race 
  \PYGZhy{} GenHealth
\end{sphinxVerbatim}

\sphinxAtStartPar
Binary variables with yes or no values are converted to 1 and 0 respectively. The diabetes column currently has four categories: ‘Yes’, ‘No’, ‘No, borderline diabetes’, ‘Yes (during pregnancy)’. Our earlier analysis revealed that  the presence of heart dieasease and diabetes are weakly correlated. Therefore, we will combine the ‘Yes (during pregnancy)’ and ‘No, borderline diabetes’ categories into the ‘Yes’ category.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{heart\PYGZus{}disease}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Diabetic}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{unique}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
array([\PYGZsq{}Yes\PYGZsq{}, \PYGZsq{}No\PYGZsq{}, \PYGZsq{}No, borderline diabetes\PYGZsq{}, \PYGZsq{}Yes (during pregnancy)\PYGZsq{}],
      dtype=object)
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{binary\PYGZus{}vars}\PYG{p}{:} \PYG{n}{List}\PYG{p}{[}\PYG{n+nb}{str}\PYG{p}{]} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{HeartDisease}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} 
                        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Smoking}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
                        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{AlcoholDrinking}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
                        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Stroke}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} 
                        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{DiffWalking}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
                        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Diabetic}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
                        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{PhysicalActivity}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
                        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Asthma}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
                        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{KidneyDisease}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
                        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{SkinCancer}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{for} \PYG{n}{binary\PYGZus{}var} \PYG{o+ow}{in} \PYG{n}{binary\PYGZus{}vars}\PYG{p}{:}
    \PYG{n}{heart\PYGZus{}disease}\PYG{p}{[}\PYG{n}{binary\PYGZus{}var}\PYG{p}{]} \PYG{o}{=} \PYG{n}{heart\PYGZus{}disease}\PYG{p}{[}\PYG{n}{binary\PYGZus{}var}\PYG{p}{]}\PYG{o}{.}\PYG{n}{replace}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Yes}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{No}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+m+mi}{0}\PYG{p}{\PYGZcb{}}\PYG{p}{)}
\PYG{n}{heart\PYGZus{}disease}\PYG{o}{.}\PYG{n}{Diabetic}\PYG{p}{:} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{Series} \PYG{o}{=} \PYG{n}{heart\PYGZus{}disease}\PYG{o}{.}\PYG{n}{Diabetic}\PYG{o}{.}\PYG{n}{replace}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{No, borderline diabetes}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Yes (during pregnancy)}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{:} \PYG{l+m+mi}{1}\PYG{p}{\PYGZcb{}}\PYG{p}{)}\PYG{o}{.}\PYG{n}{astype}\PYG{p}{(}\PYG{n+nb}{int}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} One\PYGZhy{}hot encoding }
\PYG{n}{sex\PYGZus{}one\PYGZus{}hot} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{get\PYGZus{}dummies}\PYG{p}{(}\PYG{n}{heart\PYGZus{}disease}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Sex}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{heart\PYGZus{}disease} \PYG{o}{=} \PYG{n}{heart\PYGZus{}disease}\PYG{o}{.}\PYG{n}{join}\PYG{p}{(}\PYG{n}{sex\PYGZus{}one\PYGZus{}hot}\PYG{p}{)}
\PYG{n}{heart\PYGZus{}disease}\PYG{o}{.}\PYG{n}{drop}\PYG{p}{(}\PYG{n}{columns}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Sex}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{inplace}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
In the dataset, ordinal attributes are present. There is a natural ordering to the categories (i.e. GenHealth, AgeCategory). Thus, we use the \sphinxcode{\sphinxupquote{replace()}} function to label encode them.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{heart\PYGZus{}disease}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{AgeCategory}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{replace}\PYG{p}{(}\PYG{p}{\PYGZob{}}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{18\PYGZhy{}24}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+m+mi}{1}\PYG{p}{,}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{25\PYGZhy{}29}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+m+mi}{2}\PYG{p}{,}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{30\PYGZhy{}34}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+m+mi}{3}\PYG{p}{,}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{35\PYGZhy{}39}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+m+mi}{4}\PYG{p}{,}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{40\PYGZhy{}44}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+m+mi}{5}\PYG{p}{,}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{45\PYGZhy{}49}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+m+mi}{6}\PYG{p}{,}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{50\PYGZhy{}54}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+m+mi}{7}\PYG{p}{,}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{55\PYGZhy{}59}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+m+mi}{8}\PYG{p}{,}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{60\PYGZhy{}64}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+m+mi}{9}\PYG{p}{,}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{65\PYGZhy{}69}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+m+mi}{10}\PYG{p}{,}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{70\PYGZhy{}74}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+m+mi}{11}\PYG{p}{,}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{75\PYGZhy{}79}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+m+mi}{12}\PYG{p}{,}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{80 or older}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+m+mi}{13}
\PYG{p}{\PYGZcb{}}\PYG{p}{,} \PYG{n}{inplace}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{heart\PYGZus{}disease}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{GenHealth}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{replace}\PYG{p}{(}\PYG{p}{\PYGZob{}}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Poor}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{1}\PYG{p}{,}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Fair}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+m+mi}{2}\PYG{p}{,} 
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Good}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+m+mi}{3}\PYG{p}{,} 
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Very good}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+m+mi}{4}\PYG{p}{,}  
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Excellent}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+m+mi}{5}\PYG{p}{\PYGZcb{}}\PYG{p}{,}
    \PYG{n}{inplace}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
Race has no order. Therefore, we decide to take two approaches.
\begin{itemize}
\item {} 
\sphinxAtStartPar
One\sphinxhyphen{}hot\sphinxhyphen{}encoding:Increase the dimensions to facilite a better feature selection space step later which produces a final lower dimension feature vector (i.e. every race is a binary feature)

\item {} 
\sphinxAtStartPar
Frequency encoding:  Frequency Encoding is an encoding technique that encodes categorical feature values to their respected frequencies. This will preserve the information about the values of distributions. We normalize the frequencies that result in getting the sum of unique values as 1. This is done to avoid the curse of dimensionality and reduce the sparsity in the dataset.

\end{itemize}

\sphinxAtStartPar
We will evaluate both approaches in our model evaluation.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} One\PYGZhy{}hot encoding }
\PYG{n}{race\PYGZus{}one\PYGZus{}hot}\PYG{p}{:} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{get\PYGZus{}dummies}\PYG{p}{(}\PYG{n}{heart\PYGZus{}disease}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Race}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{heart\PYGZus{}disease}\PYG{p}{:} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame} \PYG{o}{=} \PYG{n}{heart\PYGZus{}disease}\PYG{o}{.}\PYG{n}{join}\PYG{p}{(}\PYG{n}{race\PYGZus{}one\PYGZus{}hot}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Frequency encoding using value\PYGZus{}counts function }
\PYG{n}{race\PYGZus{}freq}\PYG{p}{:} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{Series} \PYG{o}{=} \PYG{n}{heart\PYGZus{}disease}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Race}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{value\PYGZus{}counts}\PYG{p}{(}\PYG{n}{normalize}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Mapping the encoded values with original data }
\PYG{n}{heart\PYGZus{}disease}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Race\PYGZus{}freq}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{:} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{Series} \PYG{o}{=} \PYG{n}{heart\PYGZus{}disease}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Race}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{apply}\PYG{p}{(}\PYG{k}{lambda} \PYG{n}{x} \PYG{p}{:} \PYG{n}{race\PYGZus{}freq}\PYG{p}{[}\PYG{n}{x}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{heart\PYGZus{}disease}\PYG{o}{.}\PYG{n}{drop}\PYG{p}{(}\PYG{n}{columns}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Race}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{inplace}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
Outliers are extreme values. Outliers can skew the distribution but these are exceptional but genuine data points.  Such distributions can impact certain algorithms such as regression type models (Lasso), k Nearest neighbors and Naive Bayes. However, decision trees and its ensemble (random forest) are not impacted.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{heart\PYGZus{}disease}\PYG{o}{.}\PYG{n}{hist}\PYG{p}{(}\PYG{n}{column}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{BMI}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
     \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{SleepTime}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
     \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{PhysicalHealth}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
     \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{MentalHealth}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
     \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{AgeCategory}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
     \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Race\PYGZus{}freq}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} 
     \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{GenHealth}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{layout}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{4}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{tight\PYGZus{}layout}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{Initial_Data_Exploration_40_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{green\PYGZus{}diamond}\PYG{p}{:} \PYG{n}{Dict}\PYG{p}{[}\PYG{n+nb}{str}\PYG{p}{,} \PYG{n+nb}{str}\PYG{p}{]} \PYG{o}{=} \PYG{n+nb}{dict}\PYG{p}{(}\PYG{n}{markerfacecolor}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{g}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{marker}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{D}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{fig1}\PYG{p}{,} \PYG{n}{ax1} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{ax1}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Changed Outlier Symbols}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{ax1}\PYG{o}{.}\PYG{n}{boxplot}\PYG{p}{(}\PYG{n}{heart\PYGZus{}disease}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{BMI}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{flierprops}\PYG{o}{=}\PYG{n}{green\PYGZus{}diamond}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZob{}\PYGZsq{}whiskers\PYGZsq{}: [\PYGZlt{}matplotlib.lines.Line2D at 0x7f681586d130\PYGZgt{},
  \PYGZlt{}matplotlib.lines.Line2D at 0x7f681579d100\PYGZgt{}],
 \PYGZsq{}caps\PYGZsq{}: [\PYGZlt{}matplotlib.lines.Line2D at 0x7f68157f6460\PYGZgt{},
  \PYGZlt{}matplotlib.lines.Line2D at 0x7f681575d460\PYGZgt{}],
 \PYGZsq{}boxes\PYGZsq{}: [\PYGZlt{}matplotlib.lines.Line2D at 0x7f6815835eb0\PYGZgt{}],
 \PYGZsq{}medians\PYGZsq{}: [\PYGZlt{}matplotlib.lines.Line2D at 0x7f6813023d00\PYGZgt{}],
 \PYGZsq{}fliers\PYGZsq{}: [\PYGZlt{}matplotlib.lines.Line2D at 0x7f68109ae490\PYGZgt{}],
 \PYGZsq{}means\PYGZsq{}: []\PYGZcb{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Initial_Data_Exploration_41_1}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
We note that many outliers exist in the BMI column. To rectify this, we take two approaches
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Turn it into a ordinal variable}: The WHO uses BMI as a convenient rule of thumb used to broadly categorize a person as underweight, normal weight, overweight, or obese based on tissue mass (muscle, fat, and bone) and height. Major adult BMI classifications are underweight (under 18.5 kg/m2), normal weight (18.5 to 24.9), overweight (25 to 29.9), and obese (30 or more). We will encode these describe caegories into ordinal values.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Take log(x) of the BMI column}: Taking the log of a feature is a common trick to reduce the effect of outliers. This is because the log function is monotonically increasing. Therefore, the effect of outliers is reduced.
Again, both aproaches will be evaluated in our model evaluation.

\end{itemize}

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{bins}\PYG{p}{:} \PYG{n}{List}\PYG{p}{[}\PYG{n+nb}{int}\PYG{p}{]} \PYG{o}{=} \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mf}{18.5}\PYG{p}{,} \PYG{l+m+mf}{24.9}\PYG{p}{,} \PYG{l+m+mf}{29.9}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{inf}\PYG{p}{]}
\PYG{n}{names}\PYG{p}{:} \PYG{n}{List}\PYG{p}{[}\PYG{n+nb}{int}\PYG{p}{]} \PYG{o}{=} \PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{,} \PYG{l+m+mi}{4}\PYG{p}{]}
\PYG{n}{heart\PYGZus{}disease}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{BMI\PYGZus{}Bin}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{:} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{Series} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{cut}\PYG{p}{(}\PYG{n}{heart\PYGZus{}disease}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{BMI}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{bins}\PYG{p}{,} \PYG{n}{labels}\PYG{o}{=}\PYG{n}{names}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{heart\PYGZus{}disease}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{LOG\PYGZus{}BMI}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{:} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{Series} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{log}\PYG{p}{(}\PYG{n}{heart\PYGZus{}disease}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{BMI}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{heart\PYGZus{}disease}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{LOG\PYGZus{}BMI}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{hist}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}AxesSubplot: \PYGZgt{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Initial_Data_Exploration_44_1}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}

\subsection{PCA}
\label{\detokenize{Initial_Data_Exploration:pca}}
\sphinxAtStartPar
Standardization is typically used for features of incomparable units. E.g. someone reporting the number of times they were physical active in the last thirty days and BMI. Standardisation will be applied to all features (both orginal and engineered) except the binary variables. We will also standardize the features due to k\sphinxhyphen{}means “isotropic” nature. In this case, if we left our variances unequal; we would inversely be putting more weight on features with high variance. In addition, we will perform principal component analysis due to avoid the curse of dimensionality that k\sphinxhyphen{}means can suffer from. The function of PCA is to reduce the dimensionality of a data set consisting of many variables correlated with each other, either heavily or lightly, while retaining the variation present in the data set to the maximum extent.

\sphinxAtStartPar
The same is done by transforming the variables (i.e. features) to a new set of variables, which are known as the principal components, ordered such that the retention of variation present decreases as we move down the order of components.

\sphinxAtStartPar
In addition, we will perform principal component analysis due to avoid the curse of dimensionality that some algorithims can suffer from. Initally, PCA is only perfoormed on the \sphinxstyleemphasis{features present within the orginal dataset}. Later on,  we will perform A feature extraction method u by applying different subsets  of training  data to estimate the accuracy of these subsets for all used
classifiers and measure the quality of the generated subsets  per classification algorithm, and the results of the classifier are shown. We plan to use PCA again for those best performing subets to see if any improvement is made.

\sphinxAtStartPar
The function of PCA is to reduce the dimensionality of a data set consisting of many variables correlated with each other,
either heavily or lightly, while retaining the variation present in the data set to the maximum extent.

\sphinxAtStartPar
The same is done by transforming the variables (i.e. features) to a new set of variables, which are known as the principal components, ordered such that the retention of variation present decreases as we move down the order of components.

\sphinxAtStartPar
The procedure of PCA involves five steps: 
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Standardize the data 

\item {} 
\sphinxAtStartPar
Compute covariance matrix 

\item {} 
\sphinxAtStartPar
Identify the eigenvalues and eigenvectors of the covariance matrix and order them according to the eigenvalues 

\item {} 
\sphinxAtStartPar
Compute a feature vector 

\item {} 
\sphinxAtStartPar
Recast the data 

\end{enumerate}


\subsubsection{Standardisation}
\label{\detokenize{Initial_Data_Exploration:standardisation}}
\sphinxAtStartPar
We now standardize the data using the following formulae:
\begin{equation*}
\begin{split}
X_i = X_i - \bar{X}~~~~~~~~~~~~~~~~~~X_i = \frac{X_i}{\sigma}
\end{split}
\end{equation*}
\sphinxAtStartPar
The standard deviation should equal 1 after standardization

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{heart\PYGZus{}disease}\PYG{p}{[}
    \PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{PhysicalHealth}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{MentalHealth}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{AgeCategory}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{GenHealth}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{SleepTime}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{BMI\PYGZus{}Bin}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
    \PYG{p}{]} \PYG{o}{=} \PYG{n}{StandardScaler}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{fit\PYGZus{}transform}\PYG{p}{(}\PYG{n}{heart\PYGZus{}disease}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{PhysicalHealth}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{MentalHealth}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{AgeCategory}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{GenHealth}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{SleepTime}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{BMI\PYGZus{}Bin}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{heart\PYGZus{}disease}\PYG{o}{.}\PYG{n}{to\PYGZus{}csv}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{data/standardised\PYGZus{}heart\PYGZus{}disease.csv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{index}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
We will use the \sphinxcode{\sphinxupquote{PCA}} function supplied by the \sphinxcode{\sphinxupquote{Scikit\sphinxhyphen{}learn}} library for dimensionality reduction.  But how do we find the optimal number of components? Which eigenvalues are important?  The scree plot below describes the cumulative explained variance for each component. We reach 80\% explained variance at the three component mark.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{orginal\PYGZus{}features\PYGZus{}df}\PYG{p}{:} \PYG{n}{List}\PYG{p}{[}\PYG{n+nb}{str}\PYG{p}{]} \PYG{o}{=} \PYG{n}{heart\PYGZus{}disease}\PYG{p}{[}\PYG{p}{[} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Smoking}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{AlcoholDrinking}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Stroke}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
       \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{PhysicalHealth}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{MentalHealth}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{DiffWalking}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{AgeCategory}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{PhysicalActivity}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{GenHealth}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{SleepTime}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Asthma}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
       \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{KidneyDisease}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{SkinCancer}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Female}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Male}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
       \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{American Indian/Alaskan Native}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Asian}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Black}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Hispanic}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Other}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
       \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{White}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{pca} \PYG{o}{=} \PYG{n}{PCA}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{orginal\PYGZus{}features\PYGZus{}df}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{cumsum}\PYG{p}{(}\PYG{n}{pca}\PYG{o}{.}\PYG{n}{explained\PYGZus{}variance\PYGZus{}ratio\PYGZus{}}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{number of components}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{cumulative explained variance}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Scree plot}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{Initial_Data_Exploration_53_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
We note a slight indent at the the 5th principal compent mark. According to the average\sphinxhyphen{}eigenvalue test (Kaiser\sphinxhyphen{}Guttman test) we should retain only those eigenvalues that are above the average which is 1.0. 
Jolliffe relaxes this criterium and suggest to retain eigenvalues greater than 0.7.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{kasier\PYGZus{}criterion}\PYG{p}{:} \PYG{n}{np}\PYG{o}{.}\PYG{n}{int64} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{where}\PYG{p}{(}\PYG{n}{pca}\PYG{o}{.}\PYG{n}{explained\PYGZus{}variance\PYGZus{}} \PYG{o}{\PYGZgt{}} \PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}
\PYG{n+nb}{print}\PYG{p}{(}
        \PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Kasier criterion optimal component number: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{kasier\PYGZus{}criterion}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{, explained variance: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{np}\PYG{o}{.}\PYG{n}{cumsum}\PYG{p}{(}\PYG{n}{pca}\PYG{o}{.}\PYG{n}{explained\PYGZus{}variance\PYGZus{}ratio\PYGZus{}}\PYG{p}{)}\PYG{p}{[}\PYG{n}{kasier\PYGZus{}criterion}\PYG{p}{]}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}
    \PYG{p}{)}
\PYG{n}{jolliffe\PYGZus{}criterion}\PYG{p}{:} \PYG{n}{np}\PYG{o}{.}\PYG{n}{int64}  \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{where}\PYG{p}{(}\PYG{n}{pca}\PYG{o}{.}\PYG{n}{explained\PYGZus{}variance\PYGZus{}} \PYG{o}{\PYGZgt{}} \PYG{l+m+mf}{0.7}\PYG{p}{)}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}
\PYG{n+nb}{print}\PYG{p}{(}
    \PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Jolliffe criterion optimal component number: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{jolliffe\PYGZus{}criterion}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{ , explained variance: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{np}\PYG{o}{.}\PYG{n}{cumsum}\PYG{p}{(}\PYG{n}{pca}\PYG{o}{.}\PYG{n}{explained\PYGZus{}variance\PYGZus{}ratio\PYGZus{}}\PYG{p}{)}\PYG{p}{[}\PYG{n}{jolliffe\PYGZus{}criterion}\PYG{p}{]}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}
    \PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
Kasier criterion optimal component number: 1, explained variance: 0.4486201019244787
Jolliffe criterion optimal component number: 2 , explained variance: 0.5825444541168093
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
For the purpose of this investigation, we decide to go with \sphinxstylestrong{both} the Jolaliffe criterion, we will retain the first two  components.

\sphinxAtStartPar
Finally, we fit the \sphinxcode{\sphinxupquote{pca}} model with the dataframes containing top 2 components , apply the dimensionality reduction on those respective dataframe and save the resulting dataframes.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{pca\PYGZus{}2d} \PYG{o}{=} \PYG{n}{PCA}\PYG{p}{(}\PYG{n}{n\PYGZus{}components}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{n}{dim\PYGZus{}reduced\PYGZus{}2d}\PYG{p}{:} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame} \PYG{o}{=} \PYG{n}{pca\PYGZus{}2d}\PYG{o}{.}\PYG{n}{fit\PYGZus{}transform}\PYG{p}{(}\PYG{n}{orginal\PYGZus{}features\PYGZus{}df}\PYG{p}{)}
\PYG{n}{dim\PYGZus{}reduced\PYGZus{}2d} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{(}\PYG{n}{data}\PYG{o}{=}\PYG{n}{dim\PYGZus{}reduced\PYGZus{}2d}\PYG{p}{,} \PYG{n}{columns}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{component\PYGZus{}}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{num}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}} \PYG{k}{for} \PYG{n}{num} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{dim\PYGZus{}reduced\PYGZus{}2d} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{merge}\PYG{p}{(}
        \PYG{n}{heart\PYGZus{}disease}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{HeartDisease}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}\PYG{p}{,} \PYG{n}{dim\PYGZus{}reduced\PYGZus{}2d}\PYG{p}{,} \PYG{n}{left\PYGZus{}index}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,} \PYG{n}{right\PYGZus{}index}\PYG{o}{=}\PYG{k+kc}{True}
        \PYG{p}{)}
\PYG{n}{dim\PYGZus{}reduced\PYGZus{}2d}\PYG{o}{.}\PYG{n}{to\PYGZus{}csv}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{data/dim\PYGZus{}reduced\PYGZus{}2d.csv}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{index}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
\sphinxstylestrong{Note}, later on we will use PCA to identify the variables that contribute most to the variation in the dataset.

\sphinxstepscope


\chapter{Cluster Analysis}
\label{\detokenize{Clustering_Analysis:cluster-analysis}}\label{\detokenize{Clustering_Analysis::doc}}
\sphinxAtStartPar
The purpose of our cluster analysis is to:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Measure clustering \& central tendency.

\item {} 
\sphinxAtStartPar
Perform k\sphinxhyphen{}means

\item {} 
\sphinxAtStartPar
Evaluate the clusters, \sphinxstylestrong{particularly}:
\begin{itemize}
\item {} 
\sphinxAtStartPar
does the dataset naturally cluster into people who do and do not have heart disease?

\item {} 
\sphinxAtStartPar
people with alchol abuse issues

\item {} 
\sphinxAtStartPar
underweight vs normal weight vs overweight vs obese

\end{itemize}

\end{itemize}


\section{Import libaries}
\label{\detokenize{Clustering_Analysis:import-libaries}}

\subsection{Data Processing}
\label{\detokenize{Clustering_Analysis:data-processing}}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}

\subsection{Scientific computing}
\label{\detokenize{Clustering_Analysis:scientific-computing}}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{neighbors} \PYG{k+kn}{import} \PYG{n}{NearestNeighbors}
\PYG{k+kn}{from} \PYG{n+nn}{random} \PYG{k+kn}{import} \PYG{n}{sample}
\PYG{k+kn}{from} \PYG{n+nn}{numpy}\PYG{n+nn}{.}\PYG{n+nn}{random} \PYG{k+kn}{import} \PYG{n}{uniform}
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{from} \PYG{n+nn}{math} \PYG{k+kn}{import} \PYG{n}{isnan}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{preprocessing} \PYG{k+kn}{import} \PYG{n}{StandardScaler}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}

\subsection{Clustering}
\label{\detokenize{Clustering_Analysis:clustering}}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{cluster} \PYG{k+kn}{import} \PYG{n}{KMeans}\PYG{p}{,} \PYG{n}{MiniBatchKMeans}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{metrics} \PYG{k+kn}{import} \PYG{n}{silhouette\PYGZus{}score}
\PYG{k+kn}{from} \PYG{n+nn}{pyclustertend} \PYG{k+kn}{import} \PYG{n}{ivat}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gt}{\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}}
\PYG{n+ne}{ImportError}\PYG{g+gWhitespace}{                               }Traceback (most recent call last)
\PYG{n}{Cell} \PYG{n}{In} \PYG{p}{[}\PYG{l+m+mi}{3}\PYG{p}{]}\PYG{p}{,} \PYG{n}{line} \PYG{l+m+mi}{3}
\PYG{g+gWhitespace}{      }\PYG{l+m+mi}{1} \PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{cluster} \PYG{k+kn}{import} \PYG{n}{KMeans}\PYG{p}{,} \PYG{n}{MiniBatchKMeans}
\PYG{g+gWhitespace}{      }\PYG{l+m+mi}{2} \PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{metrics} \PYG{k+kn}{import} \PYG{n}{silhouette\PYGZus{}score}
\PYG{n+ne}{\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZgt{} }\PYG{l+m+mi}{3} \PYG{k+kn}{from} \PYG{n+nn}{pyclustertend} \PYG{k+kn}{import} \PYG{n}{ivat}

\PYG{n}{File} \PYG{o}{\PYGZti{}}\PYG{o}{/}\PYG{n}{optum}\PYG{o}{/}\PYG{n}{repos}\PYG{o}{/}\PYG{n}{HeartDiseaseAreidy1}\PYG{o}{/}\PYG{n}{venv}\PYG{o}{/}\PYG{n}{lib}\PYG{o}{/}\PYG{n}{python3}\PYG{l+m+mf}{.8}\PYG{o}{/}\PYG{n}{site}\PYG{o}{\PYGZhy{}}\PYG{n}{packages}\PYG{o}{/}\PYG{n}{pyclustertend}\PYG{o}{/}\PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{o}{.}\PYG{n}{py}\PYG{p}{:}\PYG{l+m+mi}{6}
\PYG{g+gWhitespace}{      }\PYG{l+m+mi}{1} \PYG{k+kn}{from} \PYG{n+nn}{.}\PYG{n+nn}{hopkins} \PYG{k+kn}{import} \PYG{n}{hopkins}  \PYG{c+c1}{\PYGZsh{} noqa: F401}
\PYG{g+gWhitespace}{      }\PYG{l+m+mi}{2} \PYG{k+kn}{from} \PYG{n+nn}{.}\PYG{n+nn}{metric} \PYG{k+kn}{import} \PYG{p}{(}  \PYG{c+c1}{\PYGZsh{} noqa: F401}
\PYG{g+gWhitespace}{      }\PYG{l+m+mi}{3}     \PYG{n}{assess\PYGZus{}tendency\PYGZus{}by\PYGZus{}mean\PYGZus{}metric\PYGZus{}score}\PYG{p}{,}
\PYG{g+gWhitespace}{      }\PYG{l+m+mi}{4}     \PYG{n}{assess\PYGZus{}tendency\PYGZus{}by\PYGZus{}metric}\PYG{p}{,}
\PYG{g+gWhitespace}{      }\PYG{l+m+mi}{5} \PYG{p}{)}
\PYG{n+ne}{\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZgt{} }\PYG{l+m+mi}{6} \PYG{k+kn}{from} \PYG{n+nn}{.}\PYG{n+nn}{visual\PYGZus{}assessment\PYGZus{}of\PYGZus{}tendency} \PYG{k+kn}{import} \PYG{p}{(}  \PYG{c+c1}{\PYGZsh{} noqa: F401}
\PYG{g+gWhitespace}{      }\PYG{l+m+mi}{7}     \PYG{n}{vat}\PYG{p}{,}
\PYG{g+gWhitespace}{      }\PYG{l+m+mi}{8}     \PYG{n}{compute\PYGZus{}ordered\PYGZus{}dissimilarity\PYGZus{}matrix}\PYG{p}{,}
\PYG{g+gWhitespace}{      }\PYG{l+m+mi}{9}     \PYG{n}{ivat}\PYG{p}{,}
\PYG{g+gWhitespace}{     }\PYG{l+m+mi}{10}     \PYG{n}{compute\PYGZus{}ivat\PYGZus{}ordered\PYGZus{}dissimilarity\PYGZus{}matrix}\PYG{p}{,}
\PYG{g+gWhitespace}{     }\PYG{l+m+mi}{11} \PYG{p}{)}

\PYG{n}{File} \PYG{o}{\PYGZti{}}\PYG{o}{/}\PYG{n}{optum}\PYG{o}{/}\PYG{n}{repos}\PYG{o}{/}\PYG{n}{HeartDiseaseAreidy1}\PYG{o}{/}\PYG{n}{venv}\PYG{o}{/}\PYG{n}{lib}\PYG{o}{/}\PYG{n}{python3}\PYG{l+m+mf}{.8}\PYG{o}{/}\PYG{n}{site}\PYG{o}{\PYGZhy{}}\PYG{n}{packages}\PYG{o}{/}\PYG{n}{pyclustertend}\PYG{o}{/}\PYG{n}{visual\PYGZus{}assessment\PYGZus{}of\PYGZus{}tendency}\PYG{o}{.}\PYG{n}{py}\PYG{p}{:}\PYG{l+m+mi}{6}
\PYG{g+gWhitespace}{      }\PYG{l+m+mi}{4} \PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{g+gWhitespace}{      }\PYG{l+m+mi}{5} \PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{metrics} \PYG{k+kn}{import} \PYG{n}{pairwise\PYGZus{}distances}
\PYG{n+ne}{\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZgt{} }\PYG{l+m+mi}{6} \PYG{k+kn}{from} \PYG{n+nn}{numba} \PYG{k+kn}{import} \PYG{n}{njit}
\PYG{g+gWhitespace}{      }\PYG{l+m+mi}{9} \PYG{k}{def} \PYG{n+nf}{vat}\PYG{p}{(}\PYG{n}{data}\PYG{p}{:} \PYG{n}{np}\PYG{o}{.}\PYG{n}{ndarray}\PYG{p}{,} \PYG{n}{return\PYGZus{}odm}\PYG{p}{:} \PYG{n+nb}{bool} \PYG{o}{=} \PYG{k+kc}{False}\PYG{p}{,} \PYG{n}{figure\PYGZus{}size}\PYG{p}{:} \PYG{n}{Tuple} \PYG{o}{=} \PYG{p}{(}\PYG{l+m+mi}{10}\PYG{p}{,} \PYG{l+m+mi}{10}\PYG{p}{)}\PYG{p}{)}\PYG{p}{:}
\PYG{g+gWhitespace}{     }\PYG{l+m+mi}{10}     \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}VAT means Visual assessment of tendency. basically, it allow to asses cluster tendency}
\PYG{g+gWhitespace}{     }\PYG{l+m+mi}{11}\PYG{l+s+sd}{     through a map based on the dissimilarity matrix.}
\PYG{g+gWhitespace}{     }\PYG{l+m+mi}{12}\PYG{l+s+sd}{ }
\PYG{l+s+sd}{   (...)}
\PYG{g+gWhitespace}{     }\PYG{l+m+mi}{32}\PYG{l+s+sd}{ }
\PYG{g+gWhitespace}{     }\PYG{l+m+mi}{33}\PYG{l+s+sd}{     \PYGZdq{}\PYGZdq{}\PYGZdq{}}

\PYG{n}{File} \PYG{o}{\PYGZti{}}\PYG{o}{/}\PYG{n}{optum}\PYG{o}{/}\PYG{n}{repos}\PYG{o}{/}\PYG{n}{HeartDiseaseAreidy1}\PYG{o}{/}\PYG{n}{venv}\PYG{o}{/}\PYG{n}{lib}\PYG{o}{/}\PYG{n}{python3}\PYG{l+m+mf}{.8}\PYG{o}{/}\PYG{n}{site}\PYG{o}{\PYGZhy{}}\PYG{n}{packages}\PYG{o}{/}\PYG{n}{numba}\PYG{o}{/}\PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{o}{.}\PYG{n}{py}\PYG{p}{:}\PYG{l+m+mi}{198}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{195}     \PYG{k}{return} \PYG{k+kc}{False}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{197} \PYG{n}{\PYGZus{}ensure\PYGZus{}llvm}\PYG{p}{(}\PYG{p}{)}
\PYG{n+ne}{\PYGZhy{}\PYGZhy{}\PYGZgt{} }\PYG{l+m+mi}{198} \PYG{n}{\PYGZus{}ensure\PYGZus{}critical\PYGZus{}deps}\PYG{p}{(}\PYG{p}{)}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{200} \PYG{c+c1}{\PYGZsh{} we know llvmlite is working as the above tests passed, import it now as SVML}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{201} \PYG{c+c1}{\PYGZsh{} needs to mutate runtime options (sets the `\PYGZhy{}vector\PYGZhy{}library`).}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{202} \PYG{k+kn}{import} \PYG{n+nn}{llvmlite}

\PYG{n+nn}{File \PYGZti{}/optum/repos/HeartDiseaseAreidy1/venv/lib/python3.8/site\PYGZhy{}packages/numba/\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}.py:138,} in \PYG{n+ni}{\PYGZus{}ensure\PYGZus{}critical\PYGZus{}deps}\PYG{n+nt}{()}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{136}     \PYG{k}{raise} \PYG{n+ne}{ImportError}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Numba needs NumPy 1.17 or greater}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{137} \PYG{k}{elif} \PYG{n}{numpy\PYGZus{}version} \PYG{o}{\PYGZgt{}} \PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{20}\PYG{p}{)}\PYG{p}{:}
\PYG{n+ne}{\PYGZhy{}\PYGZhy{}\PYGZgt{} }\PYG{l+m+mi}{138}     \PYG{k}{raise} \PYG{n+ne}{ImportError}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Numba needs NumPy 1.20 or less}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{140} \PYG{k}{try}\PYG{p}{:}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{141}     \PYG{k+kn}{import} \PYG{n+nn}{scipy}

\PYG{n+ne}{ImportError}: Numba needs NumPy 1.20 or less
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}

\subsection{Data Visualisation}
\label{\detokenize{Clustering_Analysis:data-visualisation}}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}
\PYG{k+kn}{from} \PYG{n+nn}{yellowbrick}\PYG{n+nn}{.}\PYG{n+nn}{cluster} \PYG{k+kn}{import} \PYG{n}{SilhouetteVisualizer}
\PYG{k+kn}{import} \PYG{n+nn}{seaborn} \PYG{k}{as} \PYG{n+nn}{sns}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}

\section{Measure Cluster Tendency}
\label{\detokenize{Clustering_Analysis:measure-cluster-tendency}}
\sphinxAtStartPar
Clustering algorithms such as k\sphinxhyphen{}means are used to determine the structure of multi\sphinxhyphen{}dimensional data. Clusters are disjoint natural groups. However, K\sphinxhyphen{}means will find clusters in data even if none “actually” exist. Therefore, a fundamental question before applying any clustering algorithms is: Are clusters present at all?
We will measure the clustering tendency of both datasets before subjecting it to k\sphinxhyphen{}means. These datasets contain the top \sphinxstylestrong{two principal components (2D)}. To do this, we employ
\begin{itemize}
\item {} 
\sphinxAtStartPar
Hopkins’s statistic  of randomness

\end{itemize}


\subsection{Hopkins statistics}
\label{\detokenize{Clustering_Analysis:hopkins-statistics}}
\sphinxAtStartPar
Hopkins statistics {[}\hyperlink{cite.References:id8}{Banerjee and Dave, 2004}{]} tests the spatial randomness of a dataset i.e. it measures the probability that a given dataset aligns with a uniform distribution. It is based on the difference between the distance from a real point to its nearest neighbour, U, and the distance from a uniformly generated point within the data space to the nearest real data point, W.
\begin{itemize}
\item {} 
\sphinxAtStartPar
\(H_{0}\): The dataset \sphinxstylestrong{is} uniformly distributed

\item {} 
\sphinxAtStartPar
\(H_{1}\): The dataset \sphinxstylestrong{is not} uniformly distributed

\end{itemize}
\begin{equation*}
\begin{split}
H = \frac{\sum_{i=1}^{m} u_{i}^{d}}{\sum_{i=1}^{m} u_{i}^{d} + \sum_{i=1}^{m} w_{i}^{d}}
\end{split}
\end{equation*}
\sphinxAtStartPar
If the value of the Hopkins statistic(H) is close to 1 (above 0.5), we reject \(H_{0}\) and can conclude that the dataset is considered significantly clusterable.  Otherwise, we fail to reject \(H_{0}\) and can conclude that the dataset is considered significantly uniformly distributed.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{hopkins}\PYG{p}{(}\PYG{n}{X}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    Hopkins statistic. Code snippet:}
\PYG{l+s+sd}{        https://matevzkunaver.wordpress.com/2017/06/20/hopkins\PYGZhy{}test\PYGZhy{}for\PYGZhy{}cluster\PYGZhy{}tendency/}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{n}{d} \PYG{o}{=} \PYG{n}{X}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}
    \PYG{n}{n} \PYG{o}{=} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{X}\PYG{p}{)} 
    \PYG{n}{m} \PYG{o}{=} \PYG{n+nb}{int}\PYG{p}{(}\PYG{l+m+mf}{0.1} \PYG{o}{*} \PYG{n}{n}\PYG{p}{)} 
    \PYG{n}{nbrs} \PYG{o}{=} \PYG{n}{NearestNeighbors}\PYG{p}{(}\PYG{n}{n\PYGZus{}neighbors}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{X}\PYG{o}{.}\PYG{n}{values}\PYG{p}{)}
 
    \PYG{n}{rand\PYGZus{}X} \PYG{o}{=} \PYG{n}{sample}\PYG{p}{(}\PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{n}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{,} \PYG{n}{m}\PYG{p}{)}
 
    \PYG{n}{ujd} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}
    \PYG{n}{wjd} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}
    \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{m}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{u\PYGZus{}dist}\PYG{p}{,} \PYG{n}{\PYGZus{}} \PYG{o}{=} \PYG{n}{nbrs}\PYG{o}{.}\PYG{n}{kneighbors}\PYG{p}{(}\PYG{n}{uniform}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{amin}\PYG{p}{(}\PYG{n}{X}\PYG{p}{,}\PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}\PYG{p}{,}\PYG{n}{np}\PYG{o}{.}\PYG{n}{amax}\PYG{p}{(}\PYG{n}{X}\PYG{p}{,}\PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}\PYG{p}{,}\PYG{n}{d}\PYG{p}{)}\PYG{o}{.}\PYG{n}{reshape}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{return\PYGZus{}distance}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
        \PYG{n}{ujd}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{u\PYGZus{}dist}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}
        \PYG{n}{w\PYGZus{}dist}\PYG{p}{,} \PYG{n}{\PYGZus{}} \PYG{o}{=} \PYG{n}{nbrs}\PYG{o}{.}\PYG{n}{kneighbors}\PYG{p}{(}\PYG{n}{X}\PYG{o}{.}\PYG{n}{iloc}\PYG{p}{[}\PYG{n}{rand\PYGZus{}X}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}\PYG{p}{]}\PYG{o}{.}\PYG{n}{values}\PYG{o}{.}\PYG{n}{reshape}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{return\PYGZus{}distance}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
        \PYG{n}{wjd}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{w\PYGZus{}dist}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}
 
    \PYG{n}{H} \PYG{o}{=} \PYG{n+nb}{sum}\PYG{p}{(}\PYG{n}{ujd}\PYG{p}{)} \PYG{o}{/} \PYG{p}{(}\PYG{n+nb}{sum}\PYG{p}{(}\PYG{n}{ujd}\PYG{p}{)} \PYG{o}{+} \PYG{n+nb}{sum}\PYG{p}{(}\PYG{n}{wjd}\PYG{p}{)}\PYG{p}{)}
    \PYG{k}{if} \PYG{n}{isnan}\PYG{p}{(}\PYG{n}{H}\PYG{p}{)}\PYG{p}{:}
        \PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{ujd}\PYG{p}{,} \PYG{n}{wjd}\PYG{p}{)}
        \PYG{n}{H} \PYG{o}{=} \PYG{l+m+mi}{0}
 
    \PYG{k}{return} \PYG{n}{H}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{dim\PYGZus{}reduced\PYGZus{}2d} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{data/dim\PYGZus{}reduced\PYGZus{}2d.csv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} Get labels as defined in the markdown cell above to compare how k\PYGZhy{}means cluster patients}
\PYG{n}{original\PYGZus{}heart\PYGZus{}disease} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{data/heart\PYGZus{}2020\PYGZus{}cleaned.csv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{bins} \PYG{o}{=} \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mf}{18.5}\PYG{p}{,} \PYG{l+m+mf}{24.9}\PYG{p}{,} \PYG{l+m+mf}{29.9}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{inf}\PYG{p}{]}
\PYG{n}{names} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{UnderWeught}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{NormalWeight}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Overweight}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Obese}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}
\PYG{n}{dim\PYGZus{}reduced\PYGZus{}2d}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{BMI\PYGZus{}Bin}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{cut}\PYG{p}{(}\PYG{n}{original\PYGZus{}heart\PYGZus{}disease}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{BMI}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{bins}\PYG{p}{,} \PYG{n}{labels}\PYG{o}{=}\PYG{n}{names}\PYG{p}{)}
\PYG{n}{dim\PYGZus{}reduced\PYGZus{}2d}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{AlcoholDrinking}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{original\PYGZus{}heart\PYGZus{}disease}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{AlcoholDrinking}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\PYG{n}{dim\PYGZus{}reduced\PYGZus{}2d}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{BMI\PYGZus{}Bin}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{original\PYGZus{}heart\PYGZus{}disease}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Smoking}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
For both datasets, we reject \(H_{0}\) and can conclude that the dataset has a significant tendency to cluster.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{2D}\PYG{l+s+s2}{\PYGZsq{}}\PYG{l+s+s2}{s hopkins statistic }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{hopkins}\PYG{p}{(}\PYG{n}{dim\PYGZus{}reduced\PYGZus{}2d}\PYG{o}{.}\PYG{n}{iloc}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{:}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{)}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
2D\PYGZsq{}s hopkins statistic 0.9914319273681726
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}

\section{K\sphinxhyphen{}Means}
\label{\detokenize{Clustering_Analysis:k-means}}
\sphinxAtStartPar
K\sphinxhyphen{}means is a common clustering algorithm. Although a simple clustering algorithm, it has vast application areas, including customer segmentation and image compression. K\sphinxhyphen{}means is a centroid based algorithm that aims to minimize the sum of distances between the points and their respective cluster centroid.  The main steps of this algorithm are:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Step 1}: Choose the number (k) of clusters

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Step 2}: Select k random points, which will become the initial centroids

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Step 3}: Assign all data points to the nearest centroid.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Step 4}: Compute the centroid of the newly formed clusters by taking the
mean of data instances currently associated with that cluster.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Step 5}: Repeat steps 3 and 4 until either:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Centroids of newly formed clusters do not change

\item {} 
\sphinxAtStartPar
Points remain in the same cluster

\item {} 
\sphinxAtStartPar
Maximum number of iterations are reached

\end{itemize}

\end{itemize}

\sphinxAtStartPar
We utlise the \sphinxcode{\sphinxupquote{MiniBatchKMeans()}} from the sckit\sphinxhyphen{}learn package, given the largness of this dataset.

\sphinxAtStartPar
But how do we find the optimal number of clusters?
\begin{itemize}
\item {} 
\sphinxAtStartPar
Elbow method

\item {} 
\sphinxAtStartPar
Silhouette coefficient

\end{itemize}


\subsection{Elbow method}
\label{\detokenize{Clustering_Analysis:elbow-method}}
\sphinxAtStartPar
The Elbow method calculates the error or ‘distortion’ between the data points (\(y_{i}\)) and their corresponding centroid (\(ŷ_{i}\)) of N data points for k clusters where k ⋹ \{1…10\}. The error metric used is the Sum of Squared Error (SSE):
\begin{equation*}
\begin{split}
SSE = \sum_{i=1}^{N} {(y_i - ŷ_i)^2}
\end{split}
\end{equation*}
\sphinxAtStartPar
We plot these values in an attempt to find an ‘elbow’ within the curve.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{Sum\PYGZus{}of\PYGZus{}squared\PYGZus{}distances} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}
\PYG{k}{for} \PYG{n}{k} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{10}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{km\PYGZus{}2d} \PYG{o}{=} \PYG{n}{MiniBatchKMeans}\PYG{p}{(}\PYG{n}{n\PYGZus{}clusters}\PYG{o}{=}\PYG{n}{k}\PYG{p}{,} \PYG{n}{random\PYGZus{}state}\PYG{o}{=}\PYG{l+m+mi}{42}\PYG{p}{)}
    \PYG{n}{km\PYGZus{}2d} \PYG{o}{=} \PYG{n}{km\PYGZus{}2d}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{dim\PYGZus{}reduced\PYGZus{}2d}\PYG{o}{.}\PYG{n}{iloc}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{:}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{)}
    \PYG{n}{Sum\PYGZus{}of\PYGZus{}squared\PYGZus{}distances}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{km\PYGZus{}2d}\PYG{o}{.}\PYG{n}{inertia\PYGZus{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{10}\PYG{p}{)}\PYG{p}{,} \PYG{n}{Sum\PYGZus{}of\PYGZus{}squared\PYGZus{}distances}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{bx\PYGZhy{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{No of Clusters}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Sum\PYGZus{}of\PYGZus{}squared\PYGZus{}distances}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Elbow Method For Optimal k for 2D dataset}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{Clustering_Analysis_15_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
We can see that the optimal number of clusters occur at k=2. A more suitable dip is noted at k\sphinxhyphen{}4.


\section{Sillhoute method}
\label{\detokenize{Clustering_Analysis:sillhoute-method}}
\sphinxAtStartPar
This method is another method of finding the correct number of clusters(k). Silhouette coefficient for a particular data point (\(i\)) is defined as:
\begin{equation*}
\begin{split}
s_{i} = \frac{b_{i} - a_{i}}{max(b_{i}, a_{i})}
\end{split}
\end{equation*}
\sphinxAtStartPar
where:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\(s_{i}\): the silhouette coefficient, ranging from \sphinxhyphen{}1 to 1. A score of 1 (the best) means that data point \(i\) is compact in its cluster and far away from other clusters. Conversely, the worst value is \sphinxhyphen{}1, while values near 0 denote overlapping clusters.

\item {} 
\sphinxAtStartPar
\(b_{i}\): average distance between \(i\) and all the other data points in its cluster.

\item {} 
\sphinxAtStartPar
\(a_{i}\): minimum average distance from \(i\) to all clusters to which \(i\) does not belong to

\end{itemize}

\sphinxAtStartPar
We evaluate using silhouette plots. These plots display how close each point in one cluster is to points in the neighbouring clusters.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{yellowbrick}\PYG{n+nn}{.}\PYG{n+nn}{cluster} \PYG{k+kn}{import} \PYG{n}{SilhouetteVisualizer}
\PYG{k}{for} \PYG{n}{k} \PYG{o+ow}{in}  \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{5}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{km} \PYG{o}{=} \PYG{n}{MiniBatchKMeans}\PYG{p}{(}\PYG{n}{n\PYGZus{}clusters}\PYG{o}{=}\PYG{n}{k}\PYG{p}{,} \PYG{n}{random\PYGZus{}state}\PYG{o}{=}\PYG{l+m+mi}{42}\PYG{p}{)}
    \PYG{n}{visualizer} \PYG{o}{=} \PYG{n}{SilhouetteVisualizer}\PYG{p}{(}\PYG{n}{km}\PYG{p}{,} \PYG{n}{colors}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{yellowbrick}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
    \PYG{n}{visualizer}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{dim\PYGZus{}reduced\PYGZus{}2d}\PYG{o}{.}\PYG{n}{iloc}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{:}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{)}
    \PYG{n}{visualizer}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{Clustering_Analysis_19_0}.png}

\noindent\sphinxincludegraphics{{Clustering_Analysis_19_1}.png}

\noindent\sphinxincludegraphics{{Clustering_Analysis_19_2}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
The results of the silhoute analysis are more ambiguous.  K=2 seem to be sub\sphinxhyphen{}optimal due to wide fluctuations in size of the silhouette plot.  However, the fluctuation at k=4 seems to be more uniform compared to 2. Thus, we select the optimal number of clusters as 4.


\section{Findings}
\label{\detokenize{Clustering_Analysis:findings}}
\sphinxAtStartPar
As mentioned previously, clusters can be considered as disjoint groups. In this context, these clusters seek to represent people with similar latent physiological processes and/or possible health statuses. We attempt to relate the groupings to the health disease status of individual pateints.


\subsection{Top 2 principal component dataset}
\label{\detokenize{Clustering_Analysis:top-2-principal-component-dataset}}
\sphinxAtStartPar
Healthy vs Unhealthy

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{km\PYGZus{}2d} \PYG{o}{=} \PYG{n}{KMeans}\PYG{p}{(}\PYG{n}{n\PYGZus{}clusters}\PYG{o}{=}\PYG{l+m+mi}{4}\PYG{p}{,} \PYG{n}{random\PYGZus{}state}\PYG{o}{=}\PYG{l+m+mi}{42}\PYG{p}{)}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{dim\PYGZus{}reduced\PYGZus{}2d}\PYG{o}{.}\PYG{n}{iloc}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{:}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{dim\PYGZus{}reduced\PYGZus{}2d}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{clusters}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{km\PYGZus{}2d}\PYG{o}{.}\PYG{n}{labels\PYGZus{}} \PYG{o}{+} \PYG{l+m+mi}{1}
\PYG{n}{f}\PYG{p}{,} \PYG{p}{(}\PYG{n}{ax1}\PYG{p}{,} \PYG{n}{ax2}\PYG{p}{)} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{sharey}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{scatterplot}\PYG{p}{(}\PYG{n}{x}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{component\PYGZus{}1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{y}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{component\PYGZus{}2}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{data}\PYG{o}{=}\PYG{n}{dim\PYGZus{}reduced\PYGZus{}2d}\PYG{p}{,} \PYG{n}{hue}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{clusters}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{ax}\PYG{o}{=}\PYG{n}{ax1}\PYG{p}{)}
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{scatterplot}\PYG{p}{(}\PYG{n}{x}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{component\PYGZus{}1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{y}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{component\PYGZus{}2}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{data}\PYG{o}{=}\PYG{n}{dim\PYGZus{}reduced\PYGZus{}2d}\PYG{p}{,} \PYG{n}{hue}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{HeartDisease}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{ax}\PYG{o}{=}\PYG{n}{ax2}\PYG{p}{)}
\PYG{n}{results\PYGZus{}df} \PYG{o}{=} \PYG{n}{dim\PYGZus{}reduced\PYGZus{}2d}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{HeartDisease}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{clusters}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}index}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{HeartDisease}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{results\PYGZus{}df} \PYG{o}{=} \PYG{n}{results\PYGZus{}df}\PYG{o}{.}\PYG{n}{apply}\PYG{p}{(}\PYG{n}{pd}\PYG{o}{.}\PYG{n}{Series}\PYG{o}{.}\PYG{n}{value\PYGZus{}counts}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{.}\PYG{n}{groupby}\PYG{p}{(}\PYG{n}{by}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{HeartDisease}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{)}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{results\PYGZus{}df}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
                     1        2        3         4
HeartDisease                                      
0             116237.0  37359.0  22403.0  116423.0
1              16315.0   1832.0   7538.0    1688.0
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Clustering_Analysis_22_1}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
The majority of people with heart disease fall in cluster 1. Although a significant proportion also fall in cluster 2. This lines with our hypothesis that people who suffer from herat disease exhibit similar risk factors.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{scatterplot}\PYG{p}{(}\PYG{n}{x}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{component\PYGZus{}1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{y}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{component\PYGZus{}2}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{data}\PYG{o}{=}\PYG{n}{dim\PYGZus{}reduced\PYGZus{}2d}\PYG{p}{,} \PYG{n}{hue}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{BMI\PYGZus{}Bin}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{results\PYGZus{}df} \PYG{o}{=} \PYG{n}{dim\PYGZus{}reduced\PYGZus{}2d}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{BMI\PYGZus{}Bin}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{clusters}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}index}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{BMI\PYGZus{}Bin}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{results\PYGZus{}df} \PYG{o}{=} \PYG{n}{results\PYGZus{}df}\PYG{o}{.}\PYG{n}{apply}\PYG{p}{(}\PYG{n}{pd}\PYG{o}{.}\PYG{n}{Series}\PYG{o}{.}\PYG{n}{value\PYGZus{}counts}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{.}\PYG{n}{groupby}\PYG{p}{(}\PYG{n}{by}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{BMI\PYGZus{}Bin}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{)}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{results\PYGZus{}df}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
                    1        2        3        4
BMI\PYGZus{}Bin                                         
UnderWeught    1805.0    816.0    803.0   1690.0
NormalWeight  36996.0  10932.0   6614.0  40592.0
Overweight    50829.0  11545.0   8885.0  43493.0
Obese         42922.0  15898.0  13639.0  32336.0
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Clustering_Analysis_24_1}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Besides the majority of overweight and obese patients falling in clusters 1 and 4, participants in each BMI group seem to be equally distributed among the clusters. If patients  could be accurately clustered by study, this would suggest that BMI would be a strong indicator of heart disease. However, this is not the case. It is possible that the clustering is not accurate enough to make such a conclusion, or that BMI has a mild influence on heart disease outcome.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{visualise\PYGZus{}cluster\PYGZus{}heat\PYGZus{}map}\PYG{p}{(}\PYG{n}{focus\PYGZus{}feature}\PYG{p}{,} \PYG{n}{k}\PYG{p}{,} \PYG{n}{df}\PYG{p}{,} \PYG{n}{cmap}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    Visualizes the clusters as heat maps , where the squares represents }
\PYG{l+s+sd}{        the number of particpants in k clusters grouped by the focus feature }
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    :param focus\PYGZus{}feature: focus\PYGZus{}feature respresents the feature we want to}
\PYG{l+s+sd}{        drill down by i.e. health status or study}
\PYG{l+s+sd}{    :param k: Number of desired clusters}
\PYG{l+s+sd}{    :param df: Dataframe containing study, health status and the top 3 principal}
\PYG{l+s+sd}{        components}
\PYG{l+s+sd}{    :param cmap: matplotlib colormap name or object, or list of colors, optional}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    

    \PYG{n}{km\PYGZus{}3d} \PYG{o}{=} \PYG{n}{KMeans}\PYG{p}{(}\PYG{n}{n\PYGZus{}clusters}\PYG{o}{=}\PYG{n}{k}\PYG{p}{,} \PYG{n}{random\PYGZus{}state}\PYG{o}{=}\PYG{l+m+mi}{42}\PYG{p}{)}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{dim\PYGZus{}reduced\PYGZus{}2d}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{component\PYGZus{}1}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{component\PYGZus{}2}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}
    \PYG{n}{df}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{clusters}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{km\PYGZus{}3d}\PYG{o}{.}\PYG{n}{labels\PYGZus{}} \PYG{o}{+} \PYG{l+m+mi}{1}
    \PYG{n}{df} \PYG{o}{=} \PYG{n}{df}\PYG{p}{[}\PYG{p}{[}\PYG{n}{focus\PYGZus{}feature}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{clusters}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}index}\PYG{p}{(}\PYG{n}{focus\PYGZus{}feature}\PYG{p}{)}
    \PYG{n}{df} \PYG{o}{=} \PYG{n}{df}\PYG{o}{.}\PYG{n}{apply}\PYG{p}{(}\PYG{n}{pd}\PYG{o}{.}\PYG{n}{Series}\PYG{o}{.}\PYG{n}{value\PYGZus{}counts}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{.}\PYG{n}{groupby}\PYG{p}{(}\PYG{n}{by}\PYG{o}{=}\PYG{p}{[}\PYG{n}{focus\PYGZus{}feature}\PYG{p}{]}\PYG{p}{)}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{p}{)}
    \PYG{n}{akws} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{ha}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{left}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{va}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{top}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{\PYGZcb{}}
    \PYG{n}{ax} \PYG{o}{=} \PYG{n}{sns}\PYG{o}{.}\PYG{n}{heatmap}\PYG{p}{(}\PYG{n}{df}\PYG{p}{,} \PYG{n}{annot}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,} \PYG{n}{cmap}\PYG{o}{=}\PYG{n}{cmap}\PYG{p}{,} \PYG{n}{fmt}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{g}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{annot\PYGZus{}kws}\PYG{o}{=}\PYG{n}{akws}\PYG{p}{)}

    \PYG{k}{for} \PYG{n}{t} \PYG{o+ow}{in} \PYG{n}{ax}\PYG{o}{.}\PYG{n}{texts}\PYG{p}{:}
        \PYG{n}{trans} \PYG{o}{=} \PYG{n}{t}\PYG{o}{.}\PYG{n}{get\PYGZus{}transform}\PYG{p}{(}\PYG{p}{)}
        \PYG{n}{offs} \PYG{o}{=} \PYG{n}{matplotlib}\PYG{o}{.}\PYG{n}{transforms}\PYG{o}{.}\PYG{n}{ScaledTranslation}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.34}\PYG{p}{,} \PYG{l+m+mf}{0.34}\PYG{p}{,}
                        \PYG{n}{matplotlib}\PYG{o}{.}\PYG{n}{transforms}\PYG{o}{.}\PYG{n}{IdentityTransform}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
        \PYG{n}{t}\PYG{o}{.}\PYG{n}{set\PYGZus{}transform}\PYG{p}{(} \PYG{n}{offs} \PYG{o}{+} \PYG{n}{trans} \PYG{p}{)}
    
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{clusters}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{visualise\PYGZus{}cluster\PYGZus{}heat\PYGZus{}map}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{AlcoholDrinking}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+m+mi}{4}\PYG{p}{,} \PYG{n}{dim\PYGZus{}reduced\PYGZus{}2d}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Blues}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{Clustering_Analysis_27_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
The healthy participants seem to be evenly spread among the two clusters. Unhealthy participants have a tendency to appear in cluster 1 and 4. K\sphinxhyphen{}means seems to be reasonable able to cluster alcoholic  patients.

\sphinxAtStartPar
Finally, we decide to append the clusters to our standardized dataset. Our feature selection method later on, will tell us if they were of use or not.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{standradised\PYGZus{}dataset} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{data/standardised\PYGZus{}heart\PYGZus{}disease.csv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{standradised\PYGZus{}dataset}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{clusters}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]} \PYG{o}{=} \PYG{p}{(}\PYG{n}{dim\PYGZus{}reduced\PYGZus{}2d}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{clusters}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{n}{dim\PYGZus{}reduced\PYGZus{}2d}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{clusters}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}\PYG{o}{/}\PYG{n}{dim\PYGZus{}reduced\PYGZus{}2d}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{clusters}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{std}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{standradised\PYGZus{}dataset}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{clusters}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{to\PYGZus{}csv}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{data/standardised\PYGZus{}heart\PYGZus{}disease.csv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{index}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
/opt/anaconda3/lib/python3.7/site\PYGZhy{}packages/ipykernel\PYGZus{}launcher.py:3: FutureWarning: The signature of `Series.to\PYGZus{}csv` was aligned to that of `DataFrame.to\PYGZus{}csv`, and argument \PYGZsq{}header\PYGZsq{} will change its default value from False to True: please pass an explicit value to suppress this warning.
  This is separate from the ipykernel package so we can avoid doing imports until
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxstepscope


\chapter{Model Building, Evaluation \& Sensitivity Analysis}
\label{\detokenize{Model_evaluation:model-building-evaluation-sensitivity-analysis}}\label{\detokenize{Model_evaluation::doc}}
\sphinxAtStartPar
In this section, we will present a comparative analysis of the heart disease classification problem using different classification algorithms. We use the 80:20 train\sphinxhyphen{}test split rule to evaluate our models.
\sphinxstylestrong{Note}, a small amount of patients were used as  as hold\sphinxhyphen{}out set for hyper\sphinxhyphen{}parameter sensitivity analysis.


\section{Models}
\label{\detokenize{Model_evaluation:models}}
\sphinxAtStartPar
We choose numerous shallow predictive methods to predict heart disease.
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxhref{https://en.wikipedia.org/wiki/Logistic\_regression}{Logistic Regression}

\item {} 
\sphinxAtStartPar
\sphinxhref{https://en.wikipedia.org/wiki/Gradient\_boosting}{Gradient Boost Classifier}

\item {} 
\sphinxAtStartPar
\sphinxhref{https://en.wikipedia.org/wiki/Decision\_tree\_learning}{Decision Tree Classifier}

\item {} 
\sphinxAtStartPar
\sphinxhref{https://en.wikipedia.org/wiki/Random\_forest}{Random Forest Classifier}

\item {} 
\sphinxAtStartPar
\sphinxhref{https://en.wikipedia.org/wiki/Naive\_Bayes\_classifier}{Naive Bayes}

\end{itemize}

\sphinxAtStartPar
For more information on these algorithms, please click on the relevant links


\section{Evaluation metrics}
\label{\detokenize{Model_evaluation:evaluation-metrics}}
\sphinxAtStartPar
One of the key requirements in developing any algorithm is to measure it’s effectiveness. Accuracy is the most simple measure. It tells us the he number of correctly classified examples over the total number of examples. More formally,
\$\(
 Accuracy = \frac{TruePositive + TrueNegative}{TruePositive + TrueNegative + FalsePositive + FalseNegative  }
\)\$
But is accuracy telling the whole picture ?
Well, let’s consider those two examples:
\begin{itemize}
\item {} 
\sphinxAtStartPar
A classifier which, if a person has the heart disease, will always correctly diagnose it, but gets half of the healthy people wrong. You can see that announcing to a healthy person that he or she has the disease could lead to adverse consequences.

\item {} 
\sphinxAtStartPar
A classifier that gets the diagnose right for every healthy person, but also miss half of the disease cases. That wouldn’t be a very good algorithm would it?

\end{itemize}

\sphinxAtStartPar
Depending on the distribution of sick to healthy patients those two classifiers could have high accuracy while not being considered very good. Therefore, we decide to employ three further metrics
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleemphasis{Precision:} dertimnes what proportion of the negative class got correctly classified.
\$\(
\frac{TruePositive}{TruePositive +   FalsePositive    }
\)\$

\item {} 
\sphinxAtStartPar
\sphinxstyleemphasis{Recall}: determine what proportion of the actual sick people were correctly detected by the model.
\$\(
\frac{TruePositive}{TruePositive +    FalseNegative   }
\)\$

\end{itemize}


\section{Import libaries}
\label{\detokenize{Model_evaluation:import-libaries}}

\subsection{Data Processing}
\label{\detokenize{Model_evaluation:data-processing}}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{pandas}  \PYG{k}{as} \PYG{n+nn}{pd}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{model\PYGZus{}selection} \PYG{k+kn}{import} \PYG{n}{train\PYGZus{}test\PYGZus{}split}
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{decomposition} \PYG{k+kn}{import} \PYG{n}{PCA}
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{preprocessing} \PYG{k+kn}{import} \PYG{n}{StandardScaler}
\PYG{k+kn}{import} \PYG{n+nn}{warnings}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{feature\PYGZus{}selection} \PYG{k+kn}{import} \PYG{n}{mutual\PYGZus{}info\PYGZus{}regression}\PYG{p}{,} \PYG{n}{mutual\PYGZus{}info\PYGZus{}classif}
\PYG{n}{warnings}\PYG{o}{.}\PYG{n}{filterwarnings}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ignore}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}

\subsection{Model building and evaluation}
\label{\detokenize{Model_evaluation:model-building-and-evaluation}}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{linear\PYGZus{}model} \PYG{k+kn}{import} \PYG{n}{LogisticRegression}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{neighbors} \PYG{k+kn}{import} \PYG{n}{KNeighborsClassifier}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{tree} \PYG{k+kn}{import} \PYG{n}{DecisionTreeClassifier}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{ensemble} \PYG{k+kn}{import} \PYG{n}{RandomForestClassifier}\PYG{p}{,} \PYG{n}{GradientBoostingClassifier}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{neighbors} \PYG{k+kn}{import} \PYG{n}{KNeighborsClassifier}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{svm} \PYG{k+kn}{import} \PYG{n}{SVC}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{metrics} \PYG{k+kn}{import} \PYG{n}{plot\PYGZus{}confusion\PYGZus{}matrix}

\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{model\PYGZus{}selection} \PYG{k+kn}{import} \PYG{n}{GridSearchCV}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{metrics} \PYG{k+kn}{import} \PYG{n}{confusion\PYGZus{}matrix}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{metrics} \PYG{k+kn}{import} \PYG{n}{ConfusionMatrixDisplay}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{feature\PYGZus{}selection} \PYG{k+kn}{import} \PYG{n}{mutual\PYGZus{}info\PYGZus{}regression}\PYG{p}{,} \PYG{n}{mutual\PYGZus{}info\PYGZus{}classif}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{naive\PYGZus{}bayes} \PYG{k+kn}{import} \PYG{n}{GaussianNB}
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{metrics} \PYG{k+kn}{import} \PYG{n}{classification\PYGZus{}report}\PYG{p}{,} \PYG{n}{roc\PYGZus{}curve}\PYG{p}{,} \PYG{n}{auc}\PYG{p}{,} \PYG{n}{roc\PYGZus{}auc\PYGZus{}score}


\PYG{k+kn}{from} \PYG{n+nn}{imblearn}\PYG{n+nn}{.}\PYG{n+nn}{under\PYGZus{}sampling} \PYG{k+kn}{import} \PYG{p}{(}
    \PYG{n}{RandomUnderSampler}\PYG{p}{,}
    \PYG{n}{CondensedNearestNeighbour}\PYG{p}{,}
    \PYG{n}{TomekLinks}\PYG{p}{,}
    \PYG{n}{OneSidedSelection}\PYG{p}{,}
    \PYG{n}{EditedNearestNeighbours}\PYG{p}{,}
    \PYG{n}{RepeatedEditedNearestNeighbours}\PYG{p}{,}
    \PYG{n}{AllKNN}\PYG{p}{,}
    \PYG{n}{NeighbourhoodCleaningRule}\PYG{p}{,}
    \PYG{n}{NearMiss}
\PYG{p}{)}
\PYG{k+kn}{from} \PYG{n+nn}{imblearn}\PYG{n+nn}{.}\PYG{n+nn}{over\PYGZus{}sampling} \PYG{k+kn}{import} \PYG{p}{(}
    \PYG{n}{RandomOverSampler}\PYG{p}{,}
    \PYG{n}{SMOTE}\PYG{p}{,}
    \PYG{n}{ADASYN}\PYG{p}{,}
    \PYG{n}{BorderlineSMOTE}\PYG{p}{,}
    \PYG{n}{SVMSMOTE}\PYG{p}{,}
\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}

\subsection{Model Serialisation}
\label{\detokenize{Model_evaluation:model-serialisation}}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{pickle}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}

\subsection{Data Visualisation}
\label{\detokenize{Model_evaluation:data-visualisation}}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}

\subsection{Code Type Hints}
\label{\detokenize{Model_evaluation:code-type-hints}}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{typing} \PYG{k+kn}{import} \PYG{n}{List}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{dict\PYGZus{}classifiers} \PYG{o}{=} \PYG{p}{\PYGZob{}}
    \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Logistic Regression }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{n}{LogisticRegression}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,}
    \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Gradient Boost Classifier}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{n}{GradientBoostingClassifier}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,}
    \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Decision Tree Classifier}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{n}{DecisionTreeClassifier}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,}
    \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Random Forest Classifier}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{n}{RandomForestClassifier}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,}
    \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Naive Bayes}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{n}{GaussianNB}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,}
\PYG{p}{\PYGZcb{}}

\PYG{n}{heart\PYGZus{}disease\PYGZus{}dataset\PYGZus{}standardized} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{data/standardised\PYGZus{}heart\PYGZus{}disease.csv}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Initial Attempt}
\PYG{n}{train\PYGZus{}heart\PYGZus{}disease\PYGZus{}df} \PYG{o}{=} \PYG{n}{heart\PYGZus{}disease\PYGZus{}dataset\PYGZus{}standardized}\PYG{o}{.}\PYG{n}{drop}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{HeartDisease}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{axis}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{columns}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{test\PYGZus{}heart\PYGZus{}diease\PYGZus{}df} \PYG{o}{=} \PYG{n}{heart\PYGZus{}disease\PYGZus{}dataset\PYGZus{}standardized}\PYG{o}{.}\PYG{n}{HeartDisease}\PYG{o}{.}\PYG{n}{astype}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{float32}\PYG{p}{)}
\PYG{n}{X\PYGZus{}train}\PYG{p}{,} \PYG{n}{X\PYGZus{}test}\PYG{p}{,} \PYG{n}{y\PYGZus{}train}\PYG{p}{,} \PYG{n}{y\PYGZus{}test} \PYG{o}{=} \PYG{n}{train\PYGZus{}test\PYGZus{}split}\PYG{p}{(}\PYG{n}{train\PYGZus{}heart\PYGZus{}disease\PYGZus{}df}\PYG{p}{,} \PYG{n}{test\PYGZus{}heart\PYGZus{}diease\PYGZus{}df}\PYG{p}{,} \PYG{n}{test\PYGZus{}size}\PYG{o}{=}\PYG{l+m+mf}{0.2}\PYG{p}{,} \PYG{n}{random\PYGZus{}state}\PYG{o}{=}\PYG{l+m+mi}{15}\PYG{p}{,} \PYG{n}{stratify}\PYG{o}{=}\PYG{n}{test\PYGZus{}heart\PYGZus{}diease\PYGZus{}df}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Ratio of classes in training set:}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{y\PYGZus{}train}\PYG{o}{.}\PYG{n}{value\PYGZus{}counts}\PYG{p}{(}\PYG{n}{normalize}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Ratio of classes in test set:}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{y\PYGZus{}test}\PYG{o}{.}\PYG{n}{value\PYGZus{}counts}\PYG{p}{(}\PYG{n}{normalize}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
      
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
Ratio of classes in training set:
0.0    0.914406
1.0    0.085594
Name: HeartDisease, dtype: float64

Ratio of classes in test set:
0.0    0.914398
1.0    0.085602
Name: HeartDisease, dtype: float64
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
We balance the test dataset, to ensure accuracy is a fair measure of model performance.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{test\PYGZus{}df} \PYG{o}{=} \PYG{n}{X\PYGZus{}test}\PYG{o}{.}\PYG{n}{copy}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{test\PYGZus{}df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{HeartDisease}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{y\PYGZus{}test}
\PYG{n}{class\PYGZus{}0} \PYG{o}{=} \PYG{n}{test\PYGZus{}df}\PYG{p}{[}\PYG{n}{test\PYGZus{}df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{HeartDisease}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{==} \PYG{l+m+mi}{0}\PYG{p}{]}
\PYG{n}{class\PYGZus{}1} \PYG{o}{=} \PYG{n}{test\PYGZus{}df}\PYG{p}{[}\PYG{n}{test\PYGZus{}df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{HeartDisease}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{==} \PYG{l+m+mi}{1}\PYG{p}{]}

\PYG{n}{class\PYGZus{}1} \PYG{o}{=} \PYG{n}{class\PYGZus{}1}\PYG{o}{.}\PYG{n}{sample}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{class\PYGZus{}0}\PYG{p}{)}\PYG{p}{,}\PYG{n}{replace}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\PYG{n}{test\PYGZus{}df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{concat}\PYG{p}{(}\PYG{p}{[}\PYG{n}{class\PYGZus{}0}\PYG{p}{,} \PYG{n}{class\PYGZus{}1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Data in Test:}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{test\PYGZus{}df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{HeartDisease}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{value\PYGZus{}counts}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{X\PYGZus{}test} \PYG{o}{=} \PYG{n}{test\PYGZus{}df}\PYG{o}{.}\PYG{n}{drop}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{HeartDisease}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{axis}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{columns}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{y\PYGZus{}test} \PYG{o}{=} \PYG{n}{test\PYGZus{}df}\PYG{o}{.}\PYG{n}{HeartDisease}\PYG{o}{.}\PYG{n}{astype}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{float32}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
Data in Test:
0.0    58484
1.0    58484
Name: HeartDisease, dtype: int64
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{run\PYGZus{}exps}\PYG{p}{(}
             \PYG{n}{X\PYGZus{}train}\PYG{p}{,} \PYG{n}{X\PYGZus{}test}\PYG{p}{,} \PYG{n}{y\PYGZus{}train}\PYG{p}{,} \PYG{n}{y\PYGZus{}test}\PYG{p}{)} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZgt{}} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZsq{}\PYGZsq{}\PYGZsq{}}
\PYG{l+s+sd}{    Lightweight script to test many models and find winners}
\PYG{l+s+sd}{    :param X\PYGZus{}train: training split}
\PYG{l+s+sd}{    :param y\PYGZus{}train: training target vector}
\PYG{l+s+sd}{    :param X\PYGZus{}test: test split}
\PYG{l+s+sd}{    :param y\PYGZus{}test: test target vector}
\PYG{l+s+sd}{    :return: None}
\PYG{l+s+sd}{    \PYGZsq{}\PYGZsq{}\PYGZsq{}}
    
    \PYG{n}{results} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{(}\PYG{p}{)}
    \PYG{k}{for} \PYG{n}{model\PYGZus{}name}\PYG{p}{,} \PYG{n}{model} \PYG{o+ow}{in} \PYG{n}{dict\PYGZus{}classifiers}\PYG{o}{.}\PYG{n}{items}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{model}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{X\PYGZus{}train}\PYG{p}{,} \PYG{n}{y\PYGZus{}train}\PYG{p}{)}
        \PYG{n}{y\PYGZus{}pred} \PYG{o}{=} \PYG{n}{model}\PYG{o}{.}\PYG{n}{predict}\PYG{p}{(}\PYG{n}{X\PYGZus{}test}\PYG{p}{)}
        \PYG{n}{class\PYGZus{}report} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{(}\PYG{n}{classification\PYGZus{}report}\PYG{p}{(}\PYG{n}{y\PYGZus{}test}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred}\PYG{p}{,} \PYG{n}{target\PYGZus{}names}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{No Heart Disease}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Heart Disease}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,}\PYG{n}{output\PYGZus{}dict}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}\PYG{p}{)}\PYG{o}{.}\PYG{n}{transpose}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{reset\PYGZus{}index}\PYG{p}{(}\PYG{p}{)}
        \PYG{n}{class\PYGZus{}report}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{model}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{p}{[}\PYG{n}{model\PYGZus{}name}\PYG{p}{]} \PYG{o}{*} \PYG{n}{class\PYGZus{}report}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}
        \PYG{n}{class\PYGZus{}report}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{auc}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{p}{[}\PYG{n}{roc\PYGZus{}auc\PYGZus{}score}\PYG{p}{(}\PYG{n}{y\PYGZus{}test}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred}\PYG{p}{)}\PYG{p}{]} \PYG{o}{*} \PYG{n}{class\PYGZus{}report}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}
        \PYG{n}{class\PYGZus{}report} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{concat}\PYG{p}{(}\PYG{p}{[}\PYG{n}{class\PYGZus{}report}\PYG{p}{]}\PYG{p}{,} \PYG{n}{keys}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Model}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{names}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Firstlevel}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
        \PYG{n}{results} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{concat}\PYG{p}{(}\PYG{p}{[}\PYG{n}{results}\PYG{p}{,} \PYG{n}{class\PYGZus{}report}\PYG{p}{]}\PYG{p}{,} \PYG{n}{ignore\PYGZus{}index}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
    \PYG{n}{results}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{dummy}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{k+kc}{None}

    \PYG{n}{fig}\PYG{p}{,} \PYG{n}{axes} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{n}{nrows}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{n}{ncols}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{15}\PYG{p}{,}\PYG{l+m+mi}{10}\PYG{p}{)}\PYG{p}{)}

    \PYG{k}{for} \PYG{n+nb+bp}{cls}\PYG{p}{,} \PYG{n}{ax} \PYG{o+ow}{in} \PYG{n+nb}{zip}\PYG{p}{(}\PYG{n}{dict\PYGZus{}classifiers}\PYG{o}{.}\PYG{n}{values}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{axes}\PYG{o}{.}\PYG{n}{flatten}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{plot\PYGZus{}confusion\PYGZus{}matrix}\PYG{p}{(}\PYG{n+nb+bp}{cls}\PYG{p}{,} 
                            \PYG{n}{X\PYGZus{}test}\PYG{p}{,} 
                            \PYG{n}{y\PYGZus{}test}\PYG{p}{,} 
                            \PYG{n}{ax}\PYG{o}{=}\PYG{n}{ax}\PYG{p}{,} 
                            \PYG{n}{cmap}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Blues}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
                            \PYG{n}{display\PYGZus{}labels}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{No Heart Disease}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Heart Disease}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
        \PYG{n}{ax}\PYG{o}{.}\PYG{n}{title}\PYG{o}{.}\PYG{n}{set\PYGZus{}text}\PYG{p}{(}\PYG{n+nb}{type}\PYG{p}{(}\PYG{n+nb+bp}{cls}\PYG{p}{)}\PYG{o}{.}\PYG{n+nv+vm}{\PYGZus{}\PYGZus{}name\PYGZus{}\PYGZus{}}\PYG{p}{)}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{tight\PYGZus{}layout}\PYG{p}{(}\PYG{p}{)}  
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}

    \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}\PYG{l+s+s2}{Algorithm with the highest accuracy: }\PYG{l+s+si}{\PYGZob{}}
        \PYG{n}{results}\PYG{p}{[}\PYG{n}{results}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{index}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{==} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{accuracy}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{sort\PYGZus{}values}\PYG{p}{(}
            \PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{support}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{ascending}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{model}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{support}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}\PYG{o}{.}\PYG{n}{values}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{tolist}\PYG{p}{(}\PYG{p}{)}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}\PYG{p}{)}
    
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}\PYG{l+s+s2}{Algorithm with the highest macro recall:}
\PYG{l+s+s2}{        }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{results}\PYG{p}{[}\PYG{n}{results}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{index}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{==} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{macro avg}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{sort\PYGZus{}values}\PYG{p}{(}
            \PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{recall}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{ascending}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{model}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{recall}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}\PYG{o}{.}\PYG{n}{values}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{tolist}\PYG{p}{(}\PYG{p}{)}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}\PYG{p}{)}
    
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}\PYG{l+s+s2}{Algorithm with the highest macro precision:}
\PYG{l+s+s2}{        }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{results}\PYG{p}{[}\PYG{n}{results}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{index}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{==} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{macro avg}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{sort\PYGZus{}values}\PYG{p}{(}
            \PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{precision}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{ascending}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{model}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{precision}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}\PYG{o}{.}\PYG{n}{values}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{tolist}\PYG{p}{(}\PYG{p}{)}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}\PYG{p}{)}
    
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}\PYG{l+s+s2}{Algorithm with the highest AUC:}
\PYG{l+s+s2}{        }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{results}\PYG{o}{.}\PYG{n}{sort\PYGZus{}values}\PYG{p}{(}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{auc}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{ascending}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{[}
            \PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{model}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{auc}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}\PYG{o}{.}\PYG{n}{values}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{tolist}\PYG{p}{(}\PYG{p}{)}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}\PYG{p}{)}
    
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{results}\PYG{o}{.}\PYG{n}{groupby}\PYG{p}{(}
        \PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{model}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{index}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{precision}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{recall}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{f1\PYGZhy{}score}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{support}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}  \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{auc}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
        \PYG{p}{)}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{dummy}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{count}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
    
    \PYG{k}{return} \PYG{n}{results}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}

\section{Imbalanced data}
\label{\detokenize{Model_evaluation:imbalanced-data}}
\sphinxAtStartPar
As you can see above, our data is extremely imbalanced. Imbalanced datasets are those where there is a severe skew in the class distribution, such as 1:100 or 1:1000 examples in the minority class to the majority class.

\sphinxAtStartPar
This bias in the training dataset can influence many machine learning algorithms, leading some to ignore the minority class entirely. This is a problem as it is typically the minority class on which predictions are most important (i.e. predicting heart disease in our case).

\sphinxAtStartPar
One approach to addressing the problem of class imbalance is to randomly resample the training dataset. The two main approaches to randomly resampling an imbalanced dataset are to delete examples from the majority class, called undersampling, and to duplicate examples from the minority class, called oversampling.


\section{Undersampling}
\label{\detokenize{Model_evaluation:undersampling}}
\sphinxAtStartPar
The follwoing undersampling methods were choosen:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{RandomUnderSampler}}: Random undersampling consists in extracting at random samples from the majority class, until they reach a certain proportion compared to the minority class, typically 50:50.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{CondensedNearestNeighbour}}: The algorithms works as follows:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Put all minority class observations in a group, typically group O

\item {} 
\sphinxAtStartPar
Add 1 sample (at random) from the majority class to group O

\item {} 
\sphinxAtStartPar
Train a KNN with group O

\item {} 
\sphinxAtStartPar
Take a sample of the majority class that is not in group O yet

\item {} 
\sphinxAtStartPar
Predict its class with the KNN from point 3

\item {} 
\sphinxAtStartPar
If the prediction was correct, go to 4 and repeat

\item {} 
\sphinxAtStartPar
If the prediction was incorrect, add that sample to group O, go to 3 and repeat

\item {} 
\sphinxAtStartPar
Continue until all samples of the majority class were either assigned to O or left out

\item {} 
\sphinxAtStartPar
Final version of Group O is our undersampled dataset

\end{itemize}

\sphinxAtStartPar
This algorithm tends to pick points near the fuzzy boundary between the classes, and transfer those to the group O, in our example. If the classes are similar, group O will contain a fair amount of both classes. If the classes are very different, group O would contain mostly 1 class, the minority class.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{TomekLinks}}: Tomek links are 2 samples from a different class, which are nearest neighbours to each other. In other words, if 2 observations are nearest neighbours, and from a different class, they are Tomek Links. This procedures removes either the sample from the majority class if it is a Tomek Link, or alternatively, both observations, the one from the majority and the one from the minority class.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{OneSidedSelection}}: First finds the hardest instances to classify correctly from the majority class. Then removes noisy observations with Tomek Links.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{EditedNearestNeighbours}}: Train a KNN algorithm on the data (user defines number of neighbours, typically 3)
\begin{itemize}
\item {} 
\sphinxAtStartPar
Find the 3 nearest neighbour to each observation (or the number defined by the user in 1)

\item {} 
\sphinxAtStartPar
Find the label of each of the neighbours (we know it, is the target in the dataset)

\item {} 
\sphinxAtStartPar
if the majority of the neighbours show the same label as the observation, then we keep the observation

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{RepeatedEditedNearestNeighbours}}: Extends Edited Nearest neighbours in that it repeats the procedure over an over, until no further observation is removed from the dataset, or alternatively until a maximum number of iterations is reached.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{AllKNN}}: Adapts the functionality of Edited Nearest Neighbours in that, at each round, it increases the number of neighbours utilised to exclude or retain the observations.
It starts by looking at the 1 closest neighbour.
It finishes at a maximum number of neighbours to examine, determined by the user
it stops prematurely if the majority class becomes the minority

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{NeighbourhoodCleaningRule}}: The Neighbourhood Cleaning Rule works as follows:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Remove noisy observations from the majority class with ENN:

\end{enumerate}
\begin{itemize}
\item {} 
\sphinxAtStartPar
explores the 3 closest neighbours\textbackslash{}n

\item {} 
\sphinxAtStartPar
uses majority vote of neighbours to retain observations

\end{itemize}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Remove observations from the majority class if:,

\end{enumerate}
\begin{itemize}
\item {} 
\sphinxAtStartPar
they are 1 of the 3 closest neighbours to a minority sample, and,

\item {} 
\sphinxAtStartPar
most / all of those 3 closest neighbours are not minority, and,

\item {} 
\sphinxAtStartPar
the majority class has at least half as many observations as those in the minority (this can be regulated)

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{NearMiss}}: This procedures aims to select samples that are somewhat similar to the minority class, using 1 of three alternative procedures:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Select observations closer to the closest minority class

\item {} 
\sphinxAtStartPar
Select observations closer to the farthest minority class

\item {} 
\sphinxAtStartPar
Select observations furthest from their nearest neighbours

\end{itemize}

\end{itemize}

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
We train the models on a portion of the data that is under\sphinxhyphen{}sampled
We evaluate the model performance on another portion of the data that was not resampled, and thus contains the original class distribution.
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
In addition a verbose output of the models performance will be generated.
\end{sphinxadmonition}

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{undersampler\PYGZus{}dict} \PYG{o}{=} \PYG{p}{\PYGZob{}}

    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{random}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{RandomUnderSampler}\PYG{p}{(}
        \PYG{n}{sampling\PYGZus{}strategy}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{auto}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
        \PYG{n}{random\PYGZus{}state}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,}
        \PYG{n}{replacement}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}\PYG{p}{,}

    \PYG{c+c1}{\PYGZsh{} \PYGZsq{}cnn\PYGZsq{}: CondensedNearestNeighbour(}
    \PYG{c+c1}{\PYGZsh{}     sampling\PYGZus{}strategy=\PYGZsq{}auto\PYGZsq{},}
    \PYG{c+c1}{\PYGZsh{}     random\PYGZus{}state=0,}
    \PYG{c+c1}{\PYGZsh{}     n\PYGZus{}neighbors=1,}
    \PYG{c+c1}{\PYGZsh{}     n\PYGZus{}jobs=4),}

    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{tomek}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{TomekLinks}\PYG{p}{(}
        \PYG{n}{sampling\PYGZus{}strategy}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{auto}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
        \PYG{n}{n\PYGZus{}jobs}\PYG{o}{=}\PYG{l+m+mi}{4}\PYG{p}{)}\PYG{p}{,}

    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{oss}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{OneSidedSelection}\PYG{p}{(}
        \PYG{n}{sampling\PYGZus{}strategy}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{auto}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
        \PYG{n}{random\PYGZus{}state}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,}
        \PYG{n}{n\PYGZus{}neighbors}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,}
        \PYG{n}{n\PYGZus{}jobs}\PYG{o}{=}\PYG{l+m+mi}{4}\PYG{p}{)}\PYG{p}{,}

    \PYG{c+c1}{\PYGZsh{} \PYGZsq{}enn\PYGZsq{}: EditedNearestNeighbours(}
    \PYG{c+c1}{\PYGZsh{}     sampling\PYGZus{}strategy=\PYGZsq{}auto\PYGZsq{},}
    \PYG{c+c1}{\PYGZsh{}     n\PYGZus{}neighbors=3,}
    \PYG{c+c1}{\PYGZsh{}     kind\PYGZus{}sel=\PYGZsq{}all\PYGZsq{},}
    \PYG{c+c1}{\PYGZsh{}     n\PYGZus{}jobs=4),}

    \PYG{c+c1}{\PYGZsh{} \PYGZsq{}renn\PYGZsq{}: RepeatedEditedNearestNeighbours(}
    \PYG{c+c1}{\PYGZsh{}     sampling\PYGZus{}strategy=\PYGZsq{}auto\PYGZsq{},}
    \PYG{c+c1}{\PYGZsh{}     n\PYGZus{}neighbors=3,}
    \PYG{c+c1}{\PYGZsh{}     kind\PYGZus{}sel=\PYGZsq{}all\PYGZsq{},}
    \PYG{c+c1}{\PYGZsh{}     n\PYGZus{}jobs=4,}
    \PYG{c+c1}{\PYGZsh{}     max\PYGZus{}iter=100),}

    \PYG{c+c1}{\PYGZsh{} \PYGZsq{}allknn\PYGZsq{}: AllKNN(}
    \PYG{c+c1}{\PYGZsh{}     sampling\PYGZus{}strategy=\PYGZsq{}auto\PYGZsq{},}
    \PYG{c+c1}{\PYGZsh{}     n\PYGZus{}neighbors=3,}
    \PYG{c+c1}{\PYGZsh{}     kind\PYGZus{}sel=\PYGZsq{}all\PYGZsq{},}
    \PYG{c+c1}{\PYGZsh{}     n\PYGZus{}jobs=4),}

    \PYG{c+c1}{\PYGZsh{} \PYGZsq{}ncr\PYGZsq{}: NeighbourhoodCleaningRule(}
    \PYG{c+c1}{\PYGZsh{}     sampling\PYGZus{}strategy=\PYGZsq{}auto\PYGZsq{},}
    \PYG{c+c1}{\PYGZsh{}     n\PYGZus{}neighbors=3,}
    \PYG{c+c1}{\PYGZsh{}     kind\PYGZus{}sel=\PYGZsq{}all\PYGZsq{},}
    \PYG{c+c1}{\PYGZsh{}     n\PYGZus{}jobs=4,}
    \PYG{c+c1}{\PYGZsh{}     threshold\PYGZus{}cleaning=0.5),}

    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{nm1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{NearMiss}\PYG{p}{(}
        \PYG{n}{sampling\PYGZus{}strategy}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{auto}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
        \PYG{n}{version}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,}
        \PYG{n}{n\PYGZus{}neighbors}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{,}
        \PYG{n}{n\PYGZus{}jobs}\PYG{o}{=}\PYG{l+m+mi}{4}\PYG{p}{)}\PYG{p}{,}

    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{nm2}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{NearMiss}\PYG{p}{(}
        \PYG{n}{sampling\PYGZus{}strategy}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{auto}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
        \PYG{n}{version}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{,}
        \PYG{n}{n\PYGZus{}neighbors}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{,}
        \PYG{n}{n\PYGZus{}jobs}\PYG{o}{=}\PYG{l+m+mi}{4}\PYG{p}{)}\PYG{p}{,}
\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} train a model on the original data without under\PYGZhy{}sampling}
\PYG{c+c1}{\PYGZsh{} and determine model performance}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{No UnderSampling}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{run\PYGZus{}exps}\PYG{p}{(}\PYG{n}{X\PYGZus{}train}\PYG{p}{,} \PYG{n}{X\PYGZus{}test}\PYG{p}{,} \PYG{n}{y\PYGZus{}train}\PYG{p}{,} \PYG{n}{y\PYGZus{}test}\PYG{p}{)}

\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{UnderSampling Methods}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} now, we test the different under\PYGZhy{}samplers, 1 at a time}
\PYG{k}{for} \PYG{n}{undersampler} \PYG{o+ow}{in} \PYG{n}{undersampler\PYGZus{}dict}\PYG{o}{.}\PYG{n}{keys}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
    
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{undersampler}\PYG{p}{)}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
    
    \PYG{c+c1}{\PYGZsh{} resample the train set only}
    \PYG{n}{X\PYGZus{}resampled}\PYG{p}{,} \PYG{n}{y\PYGZus{}resampled} \PYG{o}{=} \PYG{n}{undersampler\PYGZus{}dict}\PYG{p}{[}\PYG{n}{undersampler}\PYG{p}{]}\PYG{o}{.}\PYG{n}{fit\PYGZus{}resample}\PYG{p}{(}\PYG{n}{X\PYGZus{}train}\PYG{o}{.}\PYG{n}{copy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{y\PYGZus{}train}\PYG{o}{.}\PYG{n}{copy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
    
    \PYG{c+c1}{\PYGZsh{} train model and evaluate performance}
    
    \PYG{c+c1}{\PYGZsh{} Note the performance returned is using the}
    \PYG{c+c1}{\PYGZsh{} test set, which was not under\PYGZhy{}sampled}
    
    \PYG{n}{run\PYGZus{}exps}\PYG{p}{(}\PYG{n}{X\PYGZus{}resampled}\PYG{p}{,} \PYG{n}{X\PYGZus{}test}\PYG{p}{,} \PYG{n}{y\PYGZus{}resampled}\PYG{p}{,} \PYG{n}{y\PYGZus{}test}\PYG{p}{)}
    
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{p}{)}
    
\PYG{n+nb}{print}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
No UnderSampling
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Model_evaluation_20_1}.png}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Algorithm with the highest accuracy: [\PYGZsq{}Naive Bayes\PYGZsq{}, 0.6830757130155256]
Algorithm with the highest macro recall:
        [\PYGZsq{}Naive Bayes\PYGZsq{}, 0.6830757130155256]
Algorithm with the highest macro precision:
        [\PYGZsq{}Logistic Regression \PYGZsq{}, 0.7267806132960342]
Algorithm with the highest AUC:
        [\PYGZsq{}Naive Bayes\PYGZsq{}, 0.6830757130155256]
model                      index             precision  recall    f1\PYGZhy{}score  support        auc     
Decision Tree Classifier   Heart Disease     0.755842   0.243913  0.368810  58484.000000   0.582561    0
                           No Heart Disease  0.549223   0.921209  0.688164  58484.000000   0.582561    0
                           accuracy          0.582561   0.582561  0.582561  0.582561       0.582561    0
                           macro avg         0.652532   0.582561  0.528487  116968.000000  0.582561    0
                           weighted avg      0.652532   0.582561  0.528487  116968.000000  0.582561    0
Gradient Boost Classifier  Heart Disease     0.930328   0.089272  0.162912  58484.000000   0.541293    0
                           No Heart Disease  0.521687   0.993314  0.684091  58484.000000   0.541293    0
                           accuracy          0.541293   0.541293  0.541293  0.541293       0.541293    0
                           macro avg         0.726008   0.541293  0.423501  116968.000000  0.541293    0
                           weighted avg      0.726008   0.541293  0.423501  116968.000000  0.541293    0
Logistic Regression        Heart Disease     0.925999   0.112544  0.200695  58484.000000   0.551775    0
                           No Heart Disease  0.527562   0.991006  0.688566  58484.000000   0.551775    0
                           accuracy          0.551775   0.551775  0.551775  0.551775       0.551775    0
                           macro avg         0.726781   0.551775  0.444631  116968.000000  0.551775    0
                           weighted avg      0.726781   0.551775  0.444631  116968.000000  0.551775    0
Naive Bayes                Heart Disease     0.786498   0.502582  0.613274  58484.000000   0.683076    0
                           No Heart Disease  0.634517   0.863570  0.731532  58484.000000   0.683076    0
                           accuracy          0.683076   0.683076  0.683076  0.683076       0.683076    0
                           macro avg         0.710507   0.683076  0.672403  116968.000000  0.683076    0
                           weighted avg      0.710507   0.683076  0.672403  116968.000000  0.683076    0
Random Forest Classifier   Heart Disease     0.867007   0.101771  0.182160  58484.000000   0.543080    0
                           No Heart Disease  0.522883   0.984389  0.682983  58484.000000   0.543080    0
                           accuracy          0.543080   0.543080  0.543080  0.543080       0.543080    0
                           macro avg         0.694945   0.543080  0.432572  116968.000000  0.543080    0
                           weighted avg      0.694945   0.543080  0.432572  116968.000000  0.543080    0
Name: dummy, dtype: int64
UnderSampling Methods
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}

random
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Model_evaluation_20_3}.png}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Algorithm with the highest accuracy: [\PYGZsq{}Gradient Boost Classifier\PYGZsq{}, 0.7631488954243896]
Algorithm with the highest macro recall:
        [\PYGZsq{}Gradient Boost Classifier\PYGZsq{}, 0.7631488954243896]
Algorithm with the highest macro precision:
        [\PYGZsq{}Gradient Boost Classifier\PYGZsq{}, 0.764231781066334]
Algorithm with the highest AUC:
        [\PYGZsq{}Gradient Boost Classifier\PYGZsq{}, 0.7631488954243896]
model                      index             precision  recall    f1\PYGZhy{}score  support        auc     
Decision Tree Classifier   Heart Disease     0.675041   0.670782  0.672905  58484.000000   0.673936    0
                           No Heart Disease  0.672846   0.677091  0.674962  58484.000000   0.673936    0
                           accuracy          0.673936   0.673936  0.673936  0.673936       0.673936    0
                           macro avg         0.673943   0.673936  0.673933  116968.000000  0.673936    0
                           weighted avg      0.673943   0.673936  0.673933  116968.000000  0.673936    0
Gradient Boost Classifier  Heart Disease     0.747316   0.795158  0.770495  58484.000000   0.763149    0
                           No Heart Disease  0.781147   0.731140  0.755317  58484.000000   0.763149    0
                           accuracy          0.763149   0.763149  0.763149  0.763149       0.763149    0
                           macro avg         0.764232   0.763149  0.762906  116968.000000  0.763149    0
                           weighted avg      0.764232   0.763149  0.762906  116968.000000  0.763149    0
Logistic Regression        Heart Disease     0.755569   0.775956  0.765627  58484.000000   0.762465    0
                           No Heart Disease  0.769743   0.748974  0.759217  58484.000000   0.762465    0
                           accuracy          0.762465   0.762465  0.762465  0.762465       0.762465    0
                           macro avg         0.762656   0.762465  0.762422  116968.000000  0.762465    0
                           weighted avg      0.762656   0.762465  0.762422  116968.000000  0.762465    0
Naive Bayes                Heart Disease     0.753837   0.641543  0.693172  58484.000000   0.716025    0
                           No Heart Disease  0.688017   0.790507  0.735710  58484.000000   0.716025    0
                           accuracy          0.716025   0.716025  0.716025  0.716025       0.716025    0
                           macro avg         0.720927   0.716025  0.714441  116968.000000  0.716025    0
                           weighted avg      0.720927   0.716025  0.714441  116968.000000  0.716025    0
Random Forest Classifier   Heart Disease     0.733705   0.782419  0.757280  58484.000000   0.749222    0
                           No Heart Disease  0.766946   0.716025  0.740611  58484.000000   0.749222    0
                           accuracy          0.749222   0.749222  0.749222  0.749222       0.749222    0
                           macro avg         0.750325   0.749222  0.748945  116968.000000  0.749222    0
                           weighted avg      0.750325   0.749222  0.748945  116968.000000  0.749222    0
Name: dummy, dtype: int64

tomek
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Model_evaluation_20_5}.png}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Algorithm with the highest accuracy: [\PYGZsq{}Naive Bayes\PYGZsq{}, 0.6872905410026674]
Algorithm with the highest macro recall:
        [\PYGZsq{}Naive Bayes\PYGZsq{}, 0.6872905410026674]
Algorithm with the highest macro precision:
        [\PYGZsq{}Gradient Boost Classifier\PYGZsq{}, 0.727595334051979]
Algorithm with the highest AUC:
        [\PYGZsq{}Naive Bayes\PYGZsq{}, 0.6872905410026673]
model                      index             precision  recall    f1\PYGZhy{}score  support        auc     
Decision Tree Classifier   Heart Disease     0.759531   0.271168  0.399652  58484.000000   0.592658    0
                           No Heart Disease  0.556396   0.914147  0.691755  58484.000000   0.592658    0
                           accuracy          0.592658   0.592658  0.592658  0.592658       0.592658    0
                           macro avg         0.657963   0.592658  0.545704  116968.000000  0.592658    0
                           weighted avg      0.657963   0.592658  0.545704  116968.000000  0.592658    0
Gradient Boost Classifier  Heart Disease     0.926230   0.117861  0.209113  58484.000000   0.554237    0
                           No Heart Disease  0.528961   0.990613  0.689661  58484.000000   0.554237    0
                           accuracy          0.554237   0.554237  0.554237  0.554237       0.554237    0
                           macro avg         0.727595   0.554237  0.449387  116968.000000  0.554237    0
                           weighted avg      0.727595   0.554237  0.449387  116968.000000  0.554237    0
Logistic Regression        Heart Disease     0.919224   0.137764  0.239617  58484.000000   0.562829    0
                           No Heart Disease  0.533959   0.987894  0.693227  58484.000000   0.562829    0
                           accuracy          0.562829   0.562829  0.562829  0.562829       0.562829    0
                           macro avg         0.726592   0.562829  0.466422  116968.000000  0.562829    0
                           weighted avg      0.726592   0.562829  0.466422  116968.000000  0.562829    0
Naive Bayes                Heart Disease     0.784928   0.515953  0.622634  58484.000000   0.687291    0
                           No Heart Disease  0.639491   0.858628  0.733032  58484.000000   0.687291    0
                           accuracy          0.687291   0.687291  0.687291  0.687291       0.687291    0
                           macro avg         0.712209   0.687291  0.677833  116968.000000  0.687291    0
                           weighted avg      0.712209   0.687291  0.677833  116968.000000  0.687291    0
Random Forest Classifier   Heart Disease     0.864348   0.140654  0.241938  58484.000000   0.559290    0
                           No Heart Disease  0.532271   0.977926  0.689342  58484.000000   0.559290    0
                           accuracy          0.559290   0.559290  0.559290  0.559290       0.559290    0
                           macro avg         0.698309   0.559290  0.465640  116968.000000  0.559290    0
                           weighted avg      0.698309   0.559290  0.465640  116968.000000  0.559290    0
Name: dummy, dtype: int64

oss
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Model_evaluation_20_7}.png}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Algorithm with the highest accuracy: [\PYGZsq{}Naive Bayes\PYGZsq{}, 0.6873418370836468]
Algorithm with the highest macro recall:
        [\PYGZsq{}Naive Bayes\PYGZsq{}, 0.6873418370836468]
Algorithm with the highest macro precision:
        [\PYGZsq{}Gradient Boost Classifier\PYGZsq{}, 0.7273676129597939]
Algorithm with the highest AUC:
        [\PYGZsq{}Naive Bayes\PYGZsq{}, 0.6873418370836468]
model                      index             precision  recall    f1\PYGZhy{}score  support        auc     
Decision Tree Classifier   Heart Disease     0.757849   0.268689  0.396723  58484.000000   0.591418    0
                           No Heart Disease  0.555558   0.914147  0.691107  58484.000000   0.591418    0
                           accuracy          0.591418   0.591418  0.591418  0.591418       0.591418    0
                           macro avg         0.656703   0.591418  0.543915  116968.000000  0.591418    0
                           weighted avg      0.656703   0.591418  0.543915  116968.000000  0.591418    0
Gradient Boost Classifier  Heart Disease     0.926036   0.116887  0.207573  58484.000000   0.553775    0
                           No Heart Disease  0.528699   0.990664  0.689451  58484.000000   0.553775    0
                           accuracy          0.553775   0.553775  0.553775  0.553775       0.553775    0
                           macro avg         0.727368   0.553775  0.448512  116968.000000  0.553775    0
                           weighted avg      0.727368   0.553775  0.448512  116968.000000  0.553775    0
Logistic Regression        Heart Disease     0.919752   0.137183  0.238755  58484.000000   0.562607    0
                           No Heart Disease  0.533826   0.988031  0.693149  58484.000000   0.562607    0
                           accuracy          0.562607   0.562607  0.562607  0.562607       0.562607    0
                           macro avg         0.726789   0.562607  0.465952  116968.000000  0.562607    0
                           weighted avg      0.726789   0.562607  0.465952  116968.000000  0.562607    0
Naive Bayes                Heart Disease     0.784917   0.516107  0.622742  58484.000000   0.687342    0
                           No Heart Disease  0.639550   0.858577  0.733053  58484.000000   0.687342    0
                           accuracy          0.687342   0.687342  0.687342  0.687342       0.687342    0
                           macro avg         0.712234   0.687342  0.677897  116968.000000  0.687342    0
                           weighted avg      0.712234   0.687342  0.677897  116968.000000  0.687342    0
Random Forest Classifier   Heart Disease     0.865964   0.140517  0.241798  58484.000000   0.559384    0
                           No Heart Disease  0.532314   0.978250  0.689459  58484.000000   0.559384    0
                           accuracy          0.559384   0.559384  0.559384  0.559384       0.559384    0
                           macro avg         0.699139   0.559384  0.465629  116968.000000  0.559384    0
                           weighted avg      0.699139   0.559384  0.465629  116968.000000  0.559384    0
Name: dummy, dtype: int64

nm1
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Model_evaluation_20_9}.png}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Algorithm with the highest accuracy: [\PYGZsq{}Logistic Regression \PYGZsq{}, 0.5924697353122221]
Algorithm with the highest macro recall:
        [\PYGZsq{}Logistic Regression \PYGZsq{}, 0.5924697353122221]
Algorithm with the highest macro precision:
        [\PYGZsq{}Logistic Regression \PYGZsq{}, 0.6157181896300579]
Algorithm with the highest AUC:
        [\PYGZsq{}Logistic Regression \PYGZsq{}, 0.5924697353122221]
model                      index             precision  recall    f1\PYGZhy{}score  support        auc     
Decision Tree Classifier   Heart Disease     0.521012   0.867656  0.651069  58484.000000   0.534992    0
                           No Heart Disease  0.604557   0.202329  0.303189  58484.000000   0.534992    0
                           accuracy          0.534992   0.534992  0.534992  0.534992       0.534992    0
                           macro avg         0.562785   0.534992  0.477129  116968.000000  0.534992    0
                           weighted avg      0.562785   0.534992  0.477129  116968.000000  0.534992    0
Gradient Boost Classifier  Heart Disease     0.547404   0.847360  0.665128  58484.000000   0.573379    0
                           No Heart Disease  0.662329   0.299398  0.412383  58484.000000   0.573379    0
                           accuracy          0.573379   0.573379  0.573379  0.573379       0.573379    0
                           macro avg         0.604866   0.573379  0.538755  116968.000000  0.573379    0
                           weighted avg      0.604866   0.573379  0.538755  116968.000000  0.573379    0
Logistic Regression        Heart Disease     0.563850   0.816582  0.667081  58484.000000   0.592470    0
                           No Heart Disease  0.667586   0.368357  0.474756  58484.000000   0.592470    0
                           accuracy          0.592470   0.592470  0.592470  0.592470       0.592470    0
                           macro avg         0.615718   0.592470  0.570918  116968.000000  0.592470    0
                           weighted avg      0.615718   0.592470  0.570918  116968.000000  0.592470    0
Naive Bayes                Heart Disease     0.555351   0.716504  0.625718  58484.000000   0.571413    0
                           No Heart Disease  0.600607   0.426322  0.498675  58484.000000   0.571413    0
                           accuracy          0.571413   0.571413  0.571413  0.571413       0.571413    0
                           macro avg         0.577979   0.571413  0.562196  116968.000000  0.571413    0
                           weighted avg      0.577979   0.571413  0.562196  116968.000000  0.571413    0
Random Forest Classifier   Heart Disease     0.522977   0.875863  0.654909  58484.000000   0.538481    0
                           No Heart Disease  0.618317   0.201098  0.303490  58484.000000   0.538481    0
                           accuracy          0.538481   0.538481  0.538481  0.538481       0.538481    0
                           macro avg         0.570647   0.538481  0.479199  116968.000000  0.538481    0
                           weighted avg      0.570647   0.538481  0.479199  116968.000000  0.538481    0
Name: dummy, dtype: int64

nm2
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gt}{\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}}
\PYG{n+ne}{MemoryError}\PYG{g+gWhitespace}{                               }Traceback (most recent call last)
\PYG{n}{Cell} \PYG{n}{In} \PYG{p}{[}\PYG{l+m+mi}{11}\PYG{p}{]}\PYG{p}{,} \PYG{n}{line} \PYG{l+m+mi}{18}
\PYG{g+gWhitespace}{     }\PYG{l+m+mi}{15} \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{g+gWhitespace}{     }\PYG{l+m+mi}{17} \PYG{c+c1}{\PYGZsh{} resample the train set only}
\PYG{n+ne}{\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZgt{} }\PYG{l+m+mi}{18} \PYG{n}{X\PYGZus{}resampled}\PYG{p}{,} \PYG{n}{y\PYGZus{}resampled} \PYG{o}{=} \PYG{n}{undersampler\PYGZus{}dict}\PYG{p}{[}\PYG{n}{undersampler}\PYG{p}{]}\PYG{o}{.}\PYG{n}{fit\PYGZus{}resample}\PYG{p}{(}\PYG{n}{X\PYGZus{}train}\PYG{o}{.}\PYG{n}{copy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{y\PYGZus{}train}\PYG{o}{.}\PYG{n}{copy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\PYG{g+gWhitespace}{     }\PYG{l+m+mi}{20} \PYG{c+c1}{\PYGZsh{} train model and evaluate performance}
\PYG{g+gWhitespace}{     }\PYG{l+m+mi}{21} 
\PYG{g+gWhitespace}{     }\PYG{l+m+mi}{22} \PYG{c+c1}{\PYGZsh{} Note the performance returned is using the}
\PYG{g+gWhitespace}{     }\PYG{l+m+mi}{23} \PYG{c+c1}{\PYGZsh{} test set, which was not under\PYGZhy{}sampled}
\PYG{g+gWhitespace}{     }\PYG{l+m+mi}{25} \PYG{n}{run\PYGZus{}exps}\PYG{p}{(}\PYG{n}{X\PYGZus{}resampled}\PYG{p}{,} \PYG{n}{X\PYGZus{}test}\PYG{p}{,} \PYG{n}{y\PYGZus{}resampled}\PYG{p}{,} \PYG{n}{y\PYGZus{}test}\PYG{p}{)}

\PYG{n+nn}{File \PYGZti{}/optum/repos/HeartDiseaseAreidy1/venv/lib/python3.8/site\PYGZhy{}packages/imblearn/base.py:83,} in \PYG{n+ni}{SamplerMixin.fit\PYGZus{}resample}\PYG{n+nt}{(self, X, y)}
\PYG{g+gWhitespace}{     }\PYG{l+m+mi}{77} \PYG{n}{X}\PYG{p}{,} \PYG{n}{y}\PYG{p}{,} \PYG{n}{binarize\PYGZus{}y} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{\PYGZus{}check\PYGZus{}X\PYGZus{}y}\PYG{p}{(}\PYG{n}{X}\PYG{p}{,} \PYG{n}{y}\PYG{p}{)}
\PYG{g+gWhitespace}{     }\PYG{l+m+mi}{79} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{sampling\PYGZus{}strategy\PYGZus{}} \PYG{o}{=} \PYG{n}{check\PYGZus{}sampling\PYGZus{}strategy}\PYG{p}{(}
\PYG{g+gWhitespace}{     }\PYG{l+m+mi}{80}     \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{sampling\PYGZus{}strategy}\PYG{p}{,} \PYG{n}{y}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{\PYGZus{}sampling\PYGZus{}type}
\PYG{g+gWhitespace}{     }\PYG{l+m+mi}{81} \PYG{p}{)}
\PYG{n+ne}{\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZgt{} }\PYG{l+m+mi}{83} \PYG{n}{output} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{\PYGZus{}fit\PYGZus{}resample}\PYG{p}{(}\PYG{n}{X}\PYG{p}{,} \PYG{n}{y}\PYG{p}{)}
\PYG{g+gWhitespace}{     }\PYG{l+m+mi}{85} \PYG{n}{y\PYGZus{}} \PYG{o}{=} \PYG{p}{(}
\PYG{g+gWhitespace}{     }\PYG{l+m+mi}{86}     \PYG{n}{label\PYGZus{}binarize}\PYG{p}{(}\PYG{n}{output}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{classes}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{unique}\PYG{p}{(}\PYG{n}{y}\PYG{p}{)}\PYG{p}{)} \PYG{k}{if} \PYG{n}{binarize\PYGZus{}y} \PYG{k}{else} \PYG{n}{output}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}
\PYG{g+gWhitespace}{     }\PYG{l+m+mi}{87} \PYG{p}{)}
\PYG{g+gWhitespace}{     }\PYG{l+m+mi}{89} \PYG{n}{X\PYGZus{}}\PYG{p}{,} \PYG{n}{y\PYGZus{}} \PYG{o}{=} \PYG{n}{arrays\PYGZus{}transformer}\PYG{o}{.}\PYG{n}{transform}\PYG{p}{(}\PYG{n}{output}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{n}{y\PYGZus{}}\PYG{p}{)}

\PYG{n+nn}{File \PYGZti{}/optum/repos/HeartDiseaseAreidy1/venv/lib/python3.8/site\PYGZhy{}packages/imblearn/under\PYGZus{}sampling/\PYGZus{}prototype\PYGZus{}selection/\PYGZus{}nearmiss.py:233,} in \PYG{n+ni}{NearMiss.\PYGZus{}fit\PYGZus{}resample}\PYG{n+nt}{(self, X, y)}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{224}     \PYG{n}{index\PYGZus{}target\PYGZus{}class} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{\PYGZus{}selection\PYGZus{}dist\PYGZus{}based}\PYG{p}{(}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{225}         \PYG{n}{X}\PYG{p}{,}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{226}         \PYG{n}{y}\PYG{p}{,}
   \PYG{p}{(}\PYG{o}{.}\PYG{o}{.}\PYG{o}{.}\PYG{p}{)}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{230}         \PYG{n}{sel\PYGZus{}strategy}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{nearest}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{231}     \PYG{p}{)}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{232} \PYG{k}{elif} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{version} \PYG{o}{==} \PYG{l+m+mi}{2}\PYG{p}{:}
\PYG{n+ne}{\PYGZhy{}\PYGZhy{}\PYGZgt{} }\PYG{l+m+mi}{233}     \PYG{n}{dist\PYGZus{}vec}\PYG{p}{,} \PYG{n}{idx\PYGZus{}vec} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{nn\PYGZus{}}\PYG{o}{.}\PYG{n}{kneighbors}\PYG{p}{(}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{234}         \PYG{n}{X\PYGZus{}class}\PYG{p}{,} \PYG{n}{n\PYGZus{}neighbors}\PYG{o}{=}\PYG{n}{target\PYGZus{}stats}\PYG{p}{[}\PYG{n}{class\PYGZus{}minority}\PYG{p}{]}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{235}     \PYG{p}{)}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{236}     \PYG{n}{index\PYGZus{}target\PYGZus{}class} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{\PYGZus{}selection\PYGZus{}dist\PYGZus{}based}\PYG{p}{(}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{237}         \PYG{n}{X}\PYG{p}{,}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{238}         \PYG{n}{y}\PYG{p}{,}
   \PYG{p}{(}\PYG{o}{.}\PYG{o}{.}\PYG{o}{.}\PYG{p}{)}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{242}         \PYG{n}{sel\PYGZus{}strategy}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{nearest}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{243}     \PYG{p}{)}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{244} \PYG{k}{elif} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{version} \PYG{o}{==} \PYG{l+m+mi}{3}\PYG{p}{:}

\PYG{n+nn}{File \PYGZti{}/optum/repos/HeartDiseaseAreidy1/venv/lib/python3.8/site\PYGZhy{}packages/sklearn/neighbors/\PYGZus{}base.py:763,} in \PYG{n+ni}{KNeighborsMixin.kneighbors}\PYG{n+nt}{(self, X, n\PYGZus{}neighbors, return\PYGZus{}distance)}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{756} \PYG{n}{use\PYGZus{}pairwise\PYGZus{}distances\PYGZus{}reductions} \PYG{o}{=} \PYG{p}{(}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{757}     \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{\PYGZus{}fit\PYGZus{}method} \PYG{o}{==} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{brute}\PYG{l+s+s2}{\PYGZdq{}}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{758}     \PYG{o+ow}{and} \PYG{n}{PairwiseDistancesArgKmin}\PYG{o}{.}\PYG{n}{is\PYGZus{}usable\PYGZus{}for}\PYG{p}{(}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{759}         \PYG{n}{X} \PYG{k}{if} \PYG{n}{X} \PYG{o+ow}{is} \PYG{o+ow}{not} \PYG{k+kc}{None} \PYG{k}{else} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{\PYGZus{}fit\PYGZus{}X}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{\PYGZus{}fit\PYGZus{}X}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{effective\PYGZus{}metric\PYGZus{}}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{760}     \PYG{p}{)}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{761} \PYG{p}{)}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{762} \PYG{k}{if} \PYG{n}{use\PYGZus{}pairwise\PYGZus{}distances\PYGZus{}reductions}\PYG{p}{:}
\PYG{n+ne}{\PYGZhy{}\PYGZhy{}\PYGZgt{} }\PYG{l+m+mi}{763}     \PYG{n}{results} \PYG{o}{=} \PYG{n}{PairwiseDistancesArgKmin}\PYG{o}{.}\PYG{n}{compute}\PYG{p}{(}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{764}         \PYG{n}{X}\PYG{o}{=}\PYG{n}{X}\PYG{p}{,}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{765}         \PYG{n}{Y}\PYG{o}{=}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{\PYGZus{}fit\PYGZus{}X}\PYG{p}{,}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{766}         \PYG{n}{k}\PYG{o}{=}\PYG{n}{n\PYGZus{}neighbors}\PYG{p}{,}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{767}         \PYG{n}{metric}\PYG{o}{=}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{effective\PYGZus{}metric\PYGZus{}}\PYG{p}{,}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{768}         \PYG{n}{metric\PYGZus{}kwargs}\PYG{o}{=}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{effective\PYGZus{}metric\PYGZus{}params\PYGZus{}}\PYG{p}{,}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{769}         \PYG{n}{strategy}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{auto}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{770}         \PYG{n}{return\PYGZus{}distance}\PYG{o}{=}\PYG{n}{return\PYGZus{}distance}\PYG{p}{,}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{771}     \PYG{p}{)}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{773} \PYG{k}{elif} \PYG{p}{(}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{774}     \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{\PYGZus{}fit\PYGZus{}method} \PYG{o}{==} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{brute}\PYG{l+s+s2}{\PYGZdq{}} \PYG{o+ow}{and} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{metric} \PYG{o}{==} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{precomputed}\PYG{l+s+s2}{\PYGZdq{}} \PYG{o+ow}{and} \PYG{n}{issparse}\PYG{p}{(}\PYG{n}{X}\PYG{p}{)}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{775} \PYG{p}{)}\PYG{p}{:}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{776}     \PYG{n}{results} \PYG{o}{=} \PYG{n}{\PYGZus{}kneighbors\PYGZus{}from\PYGZus{}graph}\PYG{p}{(}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{777}         \PYG{n}{X}\PYG{p}{,} \PYG{n}{n\PYGZus{}neighbors}\PYG{o}{=}\PYG{n}{n\PYGZus{}neighbors}\PYG{p}{,} \PYG{n}{return\PYGZus{}distance}\PYG{o}{=}\PYG{n}{return\PYGZus{}distance}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{778}     \PYG{p}{)}

\PYG{n+nn}{File sklearn/metrics/\PYGZus{}pairwise\PYGZus{}distances\PYGZus{}reduction.pyx:679,} in \PYG{n+ni}{sklearn.metrics.\PYGZus{}pairwise\PYGZus{}distances\PYGZus{}reduction.PairwiseDistancesArgKmin.compute}\PYG{n+nt}{()}

\PYG{n+nn}{File sklearn/metrics/\PYGZus{}pairwise\PYGZus{}distances\PYGZus{}reduction.pyx:1060,} in \PYG{n+ni}{sklearn.metrics.\PYGZus{}pairwise\PYGZus{}distances\PYGZus{}reduction.FastEuclideanPairwiseDistancesArgKmin.\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{n+nt}{()}

\PYG{n+nn}{File sklearn/metrics/\PYGZus{}pairwise\PYGZus{}distances\PYGZus{}reduction.pyx:737,} in \PYG{n+ni}{sklearn.metrics.\PYGZus{}pairwise\PYGZus{}distances\PYGZus{}reduction.PairwiseDistancesArgKmin.\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{n+nt}{()}

\PYG{n+nn}{File \PYGZti{}/optum/repos/HeartDiseaseAreidy1/venv/lib/python3.8/site\PYGZhy{}packages/numpy/core/numeric.py:343,} in \PYG{n+ni}{full}\PYG{n+nt}{(shape, fill\PYGZus{}value, dtype, order, like)}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{341}     \PYG{n}{fill\PYGZus{}value} \PYG{o}{=} \PYG{n}{asarray}\PYG{p}{(}\PYG{n}{fill\PYGZus{}value}\PYG{p}{)}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{342}     \PYG{n}{dtype} \PYG{o}{=} \PYG{n}{fill\PYGZus{}value}\PYG{o}{.}\PYG{n}{dtype}
\PYG{n+ne}{\PYGZhy{}\PYGZhy{}\PYGZgt{} }\PYG{l+m+mi}{343} \PYG{n}{a} \PYG{o}{=} \PYG{n}{empty}\PYG{p}{(}\PYG{n}{shape}\PYG{p}{,} \PYG{n}{dtype}\PYG{p}{,} \PYG{n}{order}\PYG{p}{)}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{344} \PYG{n}{multiarray}\PYG{o}{.}\PYG{n}{copyto}\PYG{p}{(}\PYG{n}{a}\PYG{p}{,} \PYG{n}{fill\PYGZus{}value}\PYG{p}{,} \PYG{n}{casting}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{unsafe}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{345} \PYG{k}{return} \PYG{n}{a}

\PYG{n+ne}{MemoryError}: Unable to allocate 38.2 GiB for an array with shape (233938, 21898) and data type int64
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
As we can see in the verbose model output,our best setting occurred with the \sphinxcode{\sphinxupquote{Random}} undersampling strategy. Across most tracked metrics, we see a significant improvement in model performance compared with no undersupplying techniques.

\sphinxAtStartPar
\sphinxstylestrong{Base} (no under sampling techniques) \sphinxstylestrong{winners} for tracked metrics:


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|T|}
\hline
\sphinxstyletheadfamily 
\sphinxAtStartPar
Metric
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Metric
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Value
\\
\hline
\sphinxAtStartPar
Accuracy
&
\sphinxAtStartPar
Naive Bayes
&
\sphinxAtStartPar
0.684
\\
\hline
\sphinxAtStartPar
Macro Recall
&
\sphinxAtStartPar
Naive Bayes
&
\sphinxAtStartPar
0.684
\\
\hline
\sphinxAtStartPar
Macro precision
&
\sphinxAtStartPar
Gradient Boost Classifier
&
\sphinxAtStartPar
0.726
\\
\hline
\sphinxAtStartPar
AUC
&
\sphinxAtStartPar
Naive Bayes
&
\sphinxAtStartPar
0.684
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

\sphinxAtStartPar
\sphinxstylestrong{Random under sampling technique winners} for tracked metrics:


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|T|}
\hline
\sphinxstyletheadfamily 
\sphinxAtStartPar
Metric
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Metric
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Value
\\
\hline
\sphinxAtStartPar
Accuracy
&
\sphinxAtStartPar
Gradient Boost Classifier
&
\sphinxAtStartPar
0.764
\\
\hline
\sphinxAtStartPar
Macro Recall
&
\sphinxAtStartPar
Gradient Boost Classifier
&
\sphinxAtStartPar
0.763
\\
\hline
\sphinxAtStartPar
Macro precision
&
\sphinxAtStartPar
Gradient Boost Classifier
&
\sphinxAtStartPar
0.765
\\
\hline
\sphinxAtStartPar
AUC
&
\sphinxAtStartPar
Gradient Boost Classifier
&
\sphinxAtStartPar
0.763
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}


\section{Oversampling}
\label{\detokenize{Model_evaluation:oversampling}}
\sphinxAtStartPar
The following undersampling methods were choosen:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{Random Oversampling:}} Random over\sphinxhyphen{}sampling consists in extracting at random samples from the minority class, until they reach a certain proportion compared to the majority class, typically 50:50, or in other words, a balancing ratio of 1.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{SMOTE}}: Creates new samples by interpolation of samples of the minority class and any of its k nearest neighbours (also from the minority class). K is typically 5.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{ADASYN}}: Creates new samples by interpolation of samples of the minority class and its closest neighbours. It creates more samples from samples that are harder to classify.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{Borderline SMOTE}}: Creates new samples by interpolation between samples of the minority class and their closest neighbours.
\begin{itemize}
\item {} 
\sphinxAtStartPar
It does not use all observations from the minority class as templates, unllike SMOTE.

\item {} 
\sphinxAtStartPar
It selects those observations (from the minority) for which, most of their neighbours belong to a different class (DANGER group)
\begin{itemize}
\item {} 
\sphinxAtStartPar
Variant 1 creates new examples, as SMOTE, between samples in the Danger group and their closest neighbours from the minority

\item {} 
\sphinxAtStartPar
Variant 2 creates new examples between samples in the Danger group and neighbours from minority and majority class

\end{itemize}

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{SVM SMOTE}}: Creates new samples by interpolation of samples of the support vectors from minority class and its closest neighbours.

\end{itemize}

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
We train the models on a portion of the data that is over\sphinxhyphen{}sampled
We evaluate the model performance on another portion of the data that was not resampled, and thus contains the original class distribution.
\end{sphinxadmonition}

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{oversampler\PYGZus{}dict} \PYG{o}{=} \PYG{p}{\PYGZob{}}

    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{random}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{RandomOverSampler}\PYG{p}{(}
        \PYG{n}{sampling\PYGZus{}strategy}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{auto}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
        \PYG{n}{random\PYGZus{}state}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}\PYG{p}{,}

    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{smote}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{SMOTE}\PYG{p}{(}
        \PYG{n}{sampling\PYGZus{}strategy}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{auto}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}  \PYG{c+c1}{\PYGZsh{} samples only the minority class}
        \PYG{n}{random\PYGZus{}state}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,}  \PYG{c+c1}{\PYGZsh{} for reproducibility}
        \PYG{n}{k\PYGZus{}neighbors}\PYG{o}{=}\PYG{l+m+mi}{5}\PYG{p}{,}
        \PYG{n}{n\PYGZus{}jobs}\PYG{o}{=}\PYG{l+m+mi}{4}\PYG{p}{)}\PYG{p}{,}

    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{adasyn}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{ADASYN}\PYG{p}{(}
        \PYG{n}{sampling\PYGZus{}strategy}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{auto}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}  \PYG{c+c1}{\PYGZsh{} samples only the minority class}
        \PYG{n}{random\PYGZus{}state}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,}  \PYG{c+c1}{\PYGZsh{} for reproducibility}
        \PYG{n}{n\PYGZus{}neighbors}\PYG{o}{=}\PYG{l+m+mi}{5}\PYG{p}{,}
        \PYG{n}{n\PYGZus{}jobs}\PYG{o}{=}\PYG{l+m+mi}{4}\PYG{p}{)}\PYG{p}{,}

    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{border1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{BorderlineSMOTE}\PYG{p}{(}
        \PYG{n}{sampling\PYGZus{}strategy}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{auto}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}  \PYG{c+c1}{\PYGZsh{} samples only the minority class}
        \PYG{n}{random\PYGZus{}state}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,}  \PYG{c+c1}{\PYGZsh{} for reproducibility}
        \PYG{n}{k\PYGZus{}neighbors}\PYG{o}{=}\PYG{l+m+mi}{5}\PYG{p}{,}
        \PYG{n}{m\PYGZus{}neighbors}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{,}
        \PYG{n}{kind}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{borderline\PYGZhy{}1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
        \PYG{n}{n\PYGZus{}jobs}\PYG{o}{=}\PYG{l+m+mi}{4}\PYG{p}{)}\PYG{p}{,}

    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{border2}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{BorderlineSMOTE}\PYG{p}{(}
        \PYG{n}{sampling\PYGZus{}strategy}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{auto}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}  \PYG{c+c1}{\PYGZsh{} samples only the minority class}
        \PYG{n}{random\PYGZus{}state}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,}  \PYG{c+c1}{\PYGZsh{} for reproducibility}
        \PYG{n}{k\PYGZus{}neighbors}\PYG{o}{=}\PYG{l+m+mi}{5}\PYG{p}{,}
        \PYG{n}{m\PYGZus{}neighbors}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{,}
        \PYG{n}{kind}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{borderline\PYGZhy{}2}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
        \PYG{n}{n\PYGZus{}jobs}\PYG{o}{=}\PYG{l+m+mi}{4}\PYG{p}{)}\PYG{p}{,}

    \PYG{c+c1}{\PYGZsh{} \PYGZsq{}svm\PYGZsq{}: SVMSMOTE(}
    \PYG{c+c1}{\PYGZsh{}     sampling\PYGZus{}strategy=\PYGZsq{}auto\PYGZsq{},  \PYGZsh{} samples only the minority class}
    \PYG{c+c1}{\PYGZsh{}     random\PYGZus{}state=0,  \PYGZsh{} for reproducibility}
    \PYG{c+c1}{\PYGZsh{}     k\PYGZus{}neighbors=5,}
    \PYG{c+c1}{\PYGZsh{}     m\PYGZus{}neighbors=10,}
    \PYG{c+c1}{\PYGZsh{}     n\PYGZus{}jobs=4,}
    \PYG{c+c1}{\PYGZsh{}     svm\PYGZus{}estimator=SVC(kernel=\PYGZsq{}linear\PYGZsq{})),}
\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} train a model on the original data without under\PYGZhy{}sampling}
\PYG{c+c1}{\PYGZsh{} and determine model performance}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{No OverSampling}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{run\PYGZus{}exps}\PYG{p}{(}\PYG{n}{X\PYGZus{}train}\PYG{p}{,} \PYG{n}{X\PYGZus{}test}\PYG{p}{,} \PYG{n}{y\PYGZus{}train}\PYG{p}{,} \PYG{n}{y\PYGZus{}test}\PYG{p}{)}

\PYG{n+nb}{print}\PYG{p}{(}\PYG{p}{)}

\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{OverSampling Methods}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} now, we test the different under\PYGZhy{}samplers, 1 at a time}
\PYG{k}{for} \PYG{n}{oversampler} \PYG{o+ow}{in} \PYG{n}{oversampler\PYGZus{}dict}\PYG{o}{.}\PYG{n}{keys}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
    
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{oversampler}\PYG{p}{)}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
    
    \PYG{c+c1}{\PYGZsh{} resample the train set only}
    \PYG{n}{X\PYGZus{}resampled}\PYG{p}{,} \PYG{n}{y\PYGZus{}resampled} \PYG{o}{=} \PYG{n}{oversampler\PYGZus{}dict}\PYG{p}{[}\PYG{n}{oversampler}\PYG{p}{]}\PYG{o}{.}\PYG{n}{fit\PYGZus{}resample}\PYG{p}{(}\PYG{n}{X\PYGZus{}train}\PYG{o}{.}\PYG{n}{copy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{y\PYGZus{}train}\PYG{o}{.}\PYG{n}{copy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
    
    \PYG{c+c1}{\PYGZsh{} train model and evaluate performance}
    
    \PYG{c+c1}{\PYGZsh{} Note the performance returned is using the}
    \PYG{c+c1}{\PYGZsh{} test set, which was not under\PYGZhy{}sampled}
    
    \PYG{n}{run\PYGZus{}exps}\PYG{p}{(}\PYG{n}{X\PYGZus{}resampled}\PYG{p}{,} \PYG{n}{X\PYGZus{}test}\PYG{p}{,} \PYG{n}{y\PYGZus{}resampled}\PYG{p}{,} \PYG{n}{y\PYGZus{}test}\PYG{p}{)}
    
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{p}{)}
    
\PYG{n+nb}{print}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
No OverSampling
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Model_evaluation_24_1}.png}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Algorithm with the highest accuracy: [\PYGZsq{}Naive Bayes\PYGZsq{}, 0.6834689829697012]
Algorithm with the highest macro recall:
        [\PYGZsq{}Naive Bayes\PYGZsq{}, 0.6834689829697012]
Algorithm with the highest macro precision:
        [\PYGZsq{}Logistic Regression \PYGZsq{}, 0.7264522097814119]
Algorithm with the highest AUC:
        [\PYGZsq{}Naive Bayes\PYGZsq{}, 0.683468982969701]
model                      index             precision  recall    f1\PYGZhy{}score  support        auc     
Decision Tree Classifier   Heart Disease     0.759235   0.247760  0.373603  58484.000000   0.584596    0
                           No Heart Disease  0.550545   0.921432  0.689263  58484.000000   0.584596    0
                           accuracy          0.584596   0.584596  0.584596  0.584596       0.584596    0
                           macro avg         0.654890   0.584596  0.531433  116968.000000  0.584596    0
                           weighted avg      0.654890   0.584596  0.531433  116968.000000  0.584596    0
Gradient Boost Classifier  Heart Disease     0.929941   0.088742  0.162023  58484.000000   0.541028    0
                           No Heart Disease  0.521542   0.993314  0.683966  58484.000000   0.541028    0
                           accuracy          0.541028   0.541028  0.541028  0.541028       0.541028    0
                           macro avg         0.725741   0.541028  0.422995  116968.000000  0.541028    0
                           weighted avg      0.725741   0.541028  0.422995  116968.000000  0.541028    0
Logistic Regression        Heart Disease     0.925548   0.111808  0.199515  58484.000000   0.551407    0
                           No Heart Disease  0.527356   0.991006  0.688390  58484.000000   0.551407    0
                           accuracy          0.551407   0.551407  0.551407  0.551407       0.551407    0
                           macro avg         0.726452   0.551407  0.443953  116968.000000  0.551407    0
                           weighted avg      0.726452   0.551407  0.443953  116968.000000  0.551407    0
Naive Bayes                Heart Disease     0.786760   0.503368  0.613939  58484.000000   0.683469    0
                           No Heart Disease  0.634884   0.863570  0.731776  58484.000000   0.683469    0
                           accuracy          0.683469   0.683469  0.683469  0.683469       0.683469    0
                           macro avg         0.710822   0.683469  0.672858  116968.000000  0.683469    0
                           weighted avg      0.710822   0.683469  0.672858  116968.000000  0.683469    0
Random Forest Classifier   Heart Disease     0.873803   0.107619  0.191636  58484.000000   0.546038    0
                           No Heart Disease  0.524530   0.984457  0.684402  58484.000000   0.546038    0
                           accuracy          0.546038   0.546038  0.546038  0.546038       0.546038    0
                           macro avg         0.699166   0.546038  0.438019  116968.000000  0.546038    0
                           weighted avg      0.699166   0.546038  0.438019  116968.000000  0.546038    0
Name: dummy, dtype: int64

OverSampling Methods
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}
random
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Model_evaluation_24_3}.png}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Algorithm with the highest accuracy: [\PYGZsq{}Gradient Boost Classifier\PYGZsq{}, 0.7643971000615553]
Algorithm with the highest macro recall:
        [\PYGZsq{}Gradient Boost Classifier\PYGZsq{}, 0.7643971000615553]
Algorithm with the highest macro precision:
        [\PYGZsq{}Gradient Boost Classifier\PYGZsq{}, 0.7655902916773867]
Algorithm with the highest AUC:
        [\PYGZsq{}Gradient Boost Classifier\PYGZsq{}, 0.7643971000615553]
model                      index             precision  recall    f1\PYGZhy{}score  support        auc     
Decision Tree Classifier   Heart Disease     0.754408   0.231892  0.354743  58484.000000   0.578201    0
                           No Heart Disease  0.546201   0.924509  0.686699  58484.000000   0.578201    0
                           accuracy          0.578201   0.578201  0.578201  0.578201       0.578201    0
                           macro avg         0.650305   0.578201  0.520721  116968.000000  0.578201    0
                           weighted avg      0.650305   0.578201  0.520721  116968.000000  0.578201    0
Gradient Boost Classifier  Heart Disease     0.747789   0.797911  0.772037  58484.000000   0.764397    0
                           No Heart Disease  0.783392   0.730884  0.756227  58484.000000   0.764397    0
                           accuracy          0.764397   0.764397  0.764397  0.764397       0.764397    0
                           macro avg         0.765590   0.764397  0.764132  116968.000000  0.764397    0
                           weighted avg      0.765590   0.764397  0.764132  116968.000000  0.764397    0
Logistic Regression        Heart Disease     0.755979   0.776623  0.766162  58484.000000   0.762969    0
                           No Heart Disease  0.770352   0.749316  0.759688  58484.000000   0.762969    0
                           accuracy          0.762969   0.762969  0.762969  0.762969       0.762969    0
                           macro avg         0.763166   0.762969  0.762925  116968.000000  0.762969    0
                           weighted avg      0.763166   0.762969  0.762925  116968.000000  0.762969    0
Naive Bayes                Heart Disease     0.754154   0.647203  0.696597  58484.000000   0.718111    0
                           No Heart Disease  0.691021   0.789019  0.736776  58484.000000   0.718111    0
                           accuracy          0.718111   0.718111  0.718111  0.718111       0.718111    0
                           macro avg         0.722588   0.718111  0.716686  116968.000000  0.718111    0
                           weighted avg      0.722588   0.718111  0.716686  116968.000000  0.718111    0
Random Forest Classifier   Heart Disease     0.836629   0.204723  0.328951  58484.000000   0.582373    0
                           No Heart Disease  0.546928   0.960023  0.696856  58484.000000   0.582373    0
                           accuracy          0.582373   0.582373  0.582373  0.582373       0.582373    0
                           macro avg         0.691779   0.582373  0.512903  116968.000000  0.582373    0
                           weighted avg      0.691779   0.582373  0.512903  116968.000000  0.582373    0
Name: dummy, dtype: int64

smote
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Model_evaluation_24_5}.png}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Algorithm with the highest accuracy: [\PYGZsq{}Logistic Regression \PYGZsq{}, 0.7382190000683948]
Algorithm with the highest macro recall:
        [\PYGZsq{}Logistic Regression \PYGZsq{}, 0.7382190000683948]
Algorithm with the highest macro precision:
        [\PYGZsq{}Logistic Regression \PYGZsq{}, 0.7385097659900086]
Algorithm with the highest AUC:
        [\PYGZsq{}Logistic Regression \PYGZsq{}, 0.7382190000683948]
model                      index             precision  recall    f1\PYGZhy{}score  support        auc     
Decision Tree Classifier   Heart Disease     0.751677   0.260601  0.387024  58484.000000   0.587255    0
                           No Heart Disease  0.552776   0.913908  0.688882  58484.000000   0.587255    0
                           accuracy          0.587255   0.587255  0.587255  0.587255       0.587255    0
                           macro avg         0.652226   0.587255  0.537953  116968.000000  0.587255    0
                           weighted avg      0.652226   0.587255  0.537953  116968.000000  0.587255    0
Gradient Boost Classifier  Heart Disease     0.815185   0.487963  0.610491  58484.000000   0.688667    0
                           No Heart Disease  0.634627   0.889371  0.740708  58484.000000   0.688667    0
                           accuracy          0.688667   0.688667  0.688667  0.688667       0.688667    0
                           macro avg         0.724906   0.688667  0.675599  116968.000000  0.688667    0
                           weighted avg      0.724906   0.688667  0.675599  116968.000000  0.688667    0
Logistic Regression        Heart Disease     0.746837   0.720761  0.733568  58484.000000   0.738219    0
                           No Heart Disease  0.730182   0.755677  0.742711  58484.000000   0.738219    0
                           accuracy          0.738219   0.738219  0.738219  0.738219       0.738219    0
                           macro avg         0.738510   0.738219  0.738139  116968.000000  0.738219    0
                           weighted avg      0.738510   0.738219  0.738139  116968.000000  0.738219    0
Naive Bayes                Heart Disease     0.660142   0.799826  0.723302  58484.000000   0.694027    0
                           No Heart Disease  0.746102   0.588229  0.657826  58484.000000   0.694027    0
                           accuracy          0.694027   0.694027  0.694027  0.694027       0.694027    0
                           macro avg         0.703122   0.694027  0.690564  116968.000000  0.694027    0
                           weighted avg      0.703122   0.694027  0.690564  116968.000000  0.694027    0
Random Forest Classifier   Heart Disease     0.825140   0.204620  0.327921  58484.000000   0.580629    0
                           No Heart Disease  0.546021   0.956638  0.695226  58484.000000   0.580629    0
                           accuracy          0.580629   0.580629  0.580629  0.580629       0.580629    0
                           macro avg         0.685580   0.580629  0.511574  116968.000000  0.580629    0
                           weighted avg      0.685580   0.580629  0.511574  116968.000000  0.580629    0
Name: dummy, dtype: int64

adasyn
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Model_evaluation_24_7}.png}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Algorithm with the highest accuracy: [\PYGZsq{}Logistic Regression \PYGZsq{}, 0.7316018056220505]
Algorithm with the highest macro recall:
        [\PYGZsq{}Logistic Regression \PYGZsq{}, 0.7316018056220505]
Algorithm with the highest macro precision:
        [\PYGZsq{}Logistic Regression \PYGZsq{}, 0.7318664957489149]
Algorithm with the highest AUC:
        [\PYGZsq{}Logistic Regression \PYGZsq{}, 0.7316018056220505]
model                      index             precision  recall    f1\PYGZhy{}score  support        auc     
Decision Tree Classifier   Heart Disease     0.749184   0.258943  0.384864  58484.000000   0.586126    0
                           No Heart Disease  0.552060   0.913310  0.688156  58484.000000   0.586126    0
                           accuracy          0.586126   0.586126  0.586126  0.586126       0.586126    0
                           macro avg         0.650622   0.586126  0.536510  116968.000000  0.586126    0
                           weighted avg      0.650622   0.586126  0.536510  116968.000000  0.586126    0
Gradient Boost Classifier  Heart Disease     0.814696   0.470898  0.596827  58484.000000   0.681896    0
                           No Heart Disease  0.627916   0.892894  0.737321  58484.000000   0.681896    0
                           accuracy          0.681896   0.681896  0.681896  0.681896       0.681896    0
                           macro avg         0.721306   0.681896  0.667074  116968.000000  0.681896    0
                           weighted avg      0.721306   0.681896  0.667074  116968.000000  0.681896    0
Logistic Regression        Heart Disease     0.739701   0.714708  0.726990  58484.000000   0.731602    0
                           No Heart Disease  0.724032   0.748495  0.736061  58484.000000   0.731602    0
                           accuracy          0.731602   0.731602  0.731602  0.731602       0.731602    0
                           macro avg         0.731866   0.731602  0.731525  116968.000000  0.731602    0
                           weighted avg      0.731866   0.731602  0.731525  116968.000000  0.731602    0
Naive Bayes                Heart Disease     0.644858   0.810119  0.718103  58484.000000   0.681981    0
                           No Heart Disease  0.744689   0.553844  0.635242  58484.000000   0.681981    0
                           accuracy          0.681981   0.681981  0.681981  0.681981       0.681981    0
                           macro avg         0.694774   0.681981  0.676673  116968.000000  0.681981    0
                           weighted avg      0.694774   0.681981  0.676673  116968.000000  0.681981    0
Random Forest Classifier   Heart Disease     0.825471   0.199268  0.321038  58484.000000   0.578568    0
                           No Heart Disease  0.544677   0.957869  0.694460  58484.000000   0.578568    0
                           accuracy          0.578568   0.578568  0.578568  0.578568       0.578568    0
                           macro avg         0.685074   0.578568  0.507749  116968.000000  0.578568    0
                           weighted avg      0.685074   0.578568  0.507749  116968.000000  0.578568    0
Name: dummy, dtype: int64

border1
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Model_evaluation_24_9}.png}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Algorithm with the highest accuracy: [\PYGZsq{}Logistic Regression \PYGZsq{}, 0.7352523767184187]
Algorithm with the highest macro recall:
        [\PYGZsq{}Logistic Regression \PYGZsq{}, 0.7352523767184187]
Algorithm with the highest macro precision:
        [\PYGZsq{}Logistic Regression \PYGZsq{}, 0.7369188698917095]
Algorithm with the highest AUC:
        [\PYGZsq{}Logistic Regression \PYGZsq{}, 0.7352523767184187]
model                      index             precision  recall    f1\PYGZhy{}score  support        auc     
Decision Tree Classifier   Heart Disease     0.752669   0.261576  0.388230  58484.000000   0.587810    0
                           No Heart Disease  0.553139   0.914045  0.689203  58484.000000   0.587810    0
                           accuracy          0.587810   0.587810  0.587810  0.587810       0.587810    0
                           macro avg         0.652904   0.587810  0.538716  116968.000000  0.587810    0
                           weighted avg      0.652904   0.587810  0.538716  116968.000000  0.587810    0
Gradient Boost Classifier  Heart Disease     0.811691   0.523288  0.636337  58484.000000   0.700944    0
                           No Heart Disease  0.648264   0.878599  0.746058  58484.000000   0.700944    0
                           accuracy          0.700944   0.700944  0.700944  0.700944       0.700944    0
                           macro avg         0.729978   0.700944  0.691198  116968.000000  0.700944    0
                           weighted avg      0.729978   0.700944  0.691198  116968.000000  0.700944    0
Logistic Regression        Heart Disease     0.756789   0.693318  0.723664  58484.000000   0.735252    0
                           No Heart Disease  0.717049   0.777187  0.745908  58484.000000   0.735252    0
                           accuracy          0.735252   0.735252  0.735252  0.735252       0.735252    0
                           macro avg         0.736919   0.735252  0.734786  116968.000000  0.735252    0
                           weighted avg      0.736919   0.735252  0.734786  116968.000000  0.735252    0
Naive Bayes                Heart Disease     0.678349   0.782009  0.726500  58484.000000   0.705603    0
                           No Heart Disease  0.742689   0.629198  0.681249  58484.000000   0.705603    0
                           accuracy          0.705603   0.705603  0.705603  0.705603       0.705603    0
                           macro avg         0.710519   0.705603  0.703875  116968.000000  0.705603    0
                           weighted avg      0.710519   0.705603  0.703875  116968.000000  0.705603    0
Random Forest Classifier   Heart Disease     0.831626   0.204039  0.327681  58484.000000   0.581364    0
                           No Heart Disease  0.546371   0.958690  0.696052  58484.000000   0.581364    0
                           accuracy          0.581364   0.581364  0.581364  0.581364       0.581364    0
                           macro avg         0.688998   0.581364  0.511866  116968.000000  0.581364    0
                           weighted avg      0.688998   0.581364  0.511866  116968.000000  0.581364    0
Name: dummy, dtype: int64

border2
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Model_evaluation_24_11}.png}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Algorithm with the highest accuracy: [\PYGZsq{}Logistic Regression \PYGZsq{}, 0.7136481772792559]
Algorithm with the highest macro recall:
        [\PYGZsq{}Logistic Regression \PYGZsq{}, 0.7136481772792558]
Algorithm with the highest macro precision:
        [\PYGZsq{}Logistic Regression \PYGZsq{}, 0.7225457457487992]
Algorithm with the highest AUC:
        [\PYGZsq{}Logistic Regression \PYGZsq{}, 0.7136481772792559]
model                      index             precision  recall    f1\PYGZhy{}score  support        auc     
Decision Tree Classifier   Heart Disease     0.743002   0.250085  0.374215  58484.000000   0.581792    0
                           No Heart Disease  0.549171   0.913498  0.685960  58484.000000   0.581792    0
                           accuracy          0.581792   0.581792  0.581792  0.581792       0.581792    0
                           macro avg         0.646087   0.581792  0.530088  116968.000000  0.581792    0
                           weighted avg      0.646087   0.581792  0.530088  116968.000000  0.581792    0
Gradient Boost Classifier  Heart Disease     0.835545   0.409086  0.549255  58484.000000   0.664284    0
                           No Heart Disease  0.608769   0.919482  0.732540  58484.000000   0.664284    0
                           accuracy          0.664284   0.664284  0.664284  0.664284       0.664284    0
                           macro avg         0.722157   0.664284  0.640897  116968.000000  0.664284    0
                           weighted avg      0.722157   0.664284  0.640897  116968.000000  0.664284    0
Logistic Regression        Heart Disease     0.767044   0.613672  0.681840  58484.000000   0.713648    0
                           No Heart Disease  0.678047   0.813624  0.739674  58484.000000   0.713648    0
                           accuracy          0.713648   0.713648  0.713648  0.713648       0.713648    0
                           macro avg         0.722546   0.713648  0.710757  116968.000000  0.713648    0
                           weighted avg      0.722546   0.713648  0.710757  116968.000000  0.713648    0
Naive Bayes                Heart Disease     0.617076   0.788934  0.692502  58484.000000   0.649682    0
                           No Heart Disease  0.707460   0.510430  0.593008  58484.000000   0.649682    0
                           accuracy          0.649682   0.649682  0.649682  0.649682       0.649682    0
                           macro avg         0.662268   0.649682  0.642755  116968.000000  0.649682    0
                           weighted avg      0.662268   0.649682  0.642755  116968.000000  0.649682    0
Random Forest Classifier   Heart Disease     0.830228   0.188975  0.307872  58484.000000   0.575166    0
                           No Heart Disease  0.542410   0.961357  0.693524  58484.000000   0.575166    0
                           accuracy          0.575166   0.575166  0.575166  0.575166       0.575166    0
                           macro avg         0.686319   0.575166  0.500698  116968.000000  0.575166    0
                           weighted avg      0.686319   0.575166  0.500698  116968.000000  0.575166    0
Name: dummy, dtype: int64
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
As we can see in the verbose model output,our best setting occurred with the \sphinxcode{\sphinxupquote{Random}} over sampling strategy. Across most tracked metrics, we see a significant improvement in model performance compared with no oversampling techniques.

\sphinxAtStartPar
\sphinxstylestrong{Base} (no under sampling techniques) \sphinxstylestrong{winners} for tracked metrics:


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|T|}
\hline
\sphinxstyletheadfamily 
\sphinxAtStartPar
Metric
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Metric
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Value
\\
\hline
\sphinxAtStartPar
Accuracy
&
\sphinxAtStartPar
Naive Bayes
&
\sphinxAtStartPar
0.684
\\
\hline
\sphinxAtStartPar
Macro Recall
&
\sphinxAtStartPar
Naive Bayes
&
\sphinxAtStartPar
0.684
\\
\hline
\sphinxAtStartPar
Macro precision
&
\sphinxAtStartPar
Gradient Boost Classifier
&
\sphinxAtStartPar
0.726
\\
\hline
\sphinxAtStartPar
AUC
&
\sphinxAtStartPar
Naive Bayes
&
\sphinxAtStartPar
0.684
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

\sphinxAtStartPar
\sphinxstylestrong{Random over sampling technique winners} for tracked metrics:


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|T|}
\hline
\sphinxstyletheadfamily 
\sphinxAtStartPar
Metric
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Metric
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Value
\\
\hline
\sphinxAtStartPar
Accuracy
&
\sphinxAtStartPar
Gradient Boost Classifier
&
\sphinxAtStartPar
0.764
\\
\hline
\sphinxAtStartPar
Macro Recall
&
\sphinxAtStartPar
Gradient Boost Classifier
&
\sphinxAtStartPar
0.764
\\
\hline
\sphinxAtStartPar
Macro precision
&
\sphinxAtStartPar
Gradient Boost Classifier
&
\sphinxAtStartPar
0.765
\\
\hline
\sphinxAtStartPar
AUC
&
\sphinxAtStartPar
Gradient Boost Classifier
&
\sphinxAtStartPar
0.764
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

\sphinxAtStartPar
From now on, we choose the Gradient Boost Classifier with random oversampling as our base model. We will now try to improve the performance of the model by using the feature selection. Although random undersampling achieves similar accuracy as random oversampling, we achieve marginally better auc, precision and recall scores.


\section{Feature Selection}
\label{\detokenize{Model_evaluation:feature-selection}}
\sphinxAtStartPar
The idea of feature selection and extraction is to avoid the curse of
dimensionality. This refers to the fact that as we move to higher
dimension input feature spaces the volume of the space grows rapidly
and we end up with very few instances per unit volume, i.e. we have
very sparse sampling of the space of possible instances making
modelling difficult.

\sphinxAtStartPar
Feature Selection: It is clear from what we have seen that a good feature engineering
idea might be to choose a subset of the features available to reduce
the dimension of the feature space. This act is called feature
selection. One way of doing this is to try out different permutations of
features increasing the numbers of features involved as you proceed
and calculate machine learning performance. This is rarely practical
though. More efficient approaches include wrapper, filter and
embedded methods.

\sphinxAtStartPar
We decide to explore the following methods:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Perform PCA anaylsis and idenitify the variables that most contribute to the

\item {} 
\sphinxAtStartPar
A simple filter method:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Identify input features having high correlation with target variable.

\item {} 
\sphinxAtStartPar
Identify input features that have a low correlation with other independent variables

\item {} 
\sphinxAtStartPar
Find the information gain or mutual information of the independent variable with respect to a target variable

\end{itemize}

\end{itemize}


\subsection{PCA}
\label{\detokenize{Model_evaluation:pca}}
\sphinxAtStartPar
PCA is mathematically defined as an orthogonal linear transformation that transforms the data to a new coordinate system such that the greatest variance by some projection of the data comes to lie on the first coordinate (called the first principal component), the second greatest variance on the second coordinate, and so on.

\sphinxAtStartPar
We decide to get the top 5 features that contribute most to the first principal component and the top 5 features that contribute most to the second principal compoenent.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{pca\PYGZus{}most\PYGZus{}important\PYGZus{}features}\PYG{p}{(}\PYG{n}{df}\PYG{p}{:} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{)} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZgt{}} \PYG{n}{List}\PYG{p}{[}\PYG{n+nb}{str}\PYG{p}{]}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    Retrieve the top 10 features that contribute most }
\PYG{l+s+sd}{        variation to the top 2 principal components}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{n}{model} \PYG{o}{=} \PYG{n}{PCA}\PYG{p}{(}\PYG{n}{n\PYGZus{}components}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{df}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} number of components}
    \PYG{n}{n\PYGZus{}pcs}\PYG{p}{:} \PYG{n+nb}{int} \PYG{o}{=} \PYG{n}{model}\PYG{o}{.}\PYG{n}{components\PYGZus{}}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}

    \PYG{n}{most\PYGZus{}important\PYGZus{}features\PYGZus{}indicies}\PYG{p}{:} \PYG{n}{List}\PYG{p}{[}\PYG{n+nb}{int}\PYG{p}{]} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}
    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{n\PYGZus{}pcs}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{top\PYGZus{}5} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{argpartition}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{abs}\PYG{p}{(}\PYG{n}{model}\PYG{o}{.}\PYG{n}{components\PYGZus{}}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{5}\PYG{p}{)}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{5}\PYG{p}{:}\PYG{p}{]}\PYG{o}{.}\PYG{n}{tolist}\PYG{p}{(}\PYG{p}{)}
        \PYG{n}{most\PYGZus{}important\PYGZus{}features\PYGZus{}indicies}\PYG{o}{.}\PYG{n}{extend}\PYG{p}{(}\PYG{n}{top\PYGZus{}5}\PYG{p}{)}
        
    \PYG{n}{most\PYGZus{}important\PYGZus{}features\PYGZus{}indicies} \PYG{o}{=} \PYG{n+nb}{list}\PYG{p}{(}\PYG{n+nb}{set}\PYG{p}{(}\PYG{n}{most\PYGZus{}important\PYGZus{}features\PYGZus{}indicies}\PYG{p}{)}\PYG{p}{)}
    \PYG{n}{initial\PYGZus{}feature\PYGZus{}names} \PYG{o}{=} \PYG{n}{df}\PYG{o}{.}\PYG{n}{columns}
    \PYG{n}{most\PYGZus{}important\PYGZus{}names}  \PYG{o}{=} \PYG{p}{[}\PYG{n}{initial\PYGZus{}feature\PYGZus{}names}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]} \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n}{most\PYGZus{}important\PYGZus{}features\PYGZus{}indicies}\PYG{p}{]}
    
    \PYG{k}{return} \PYG{n}{most\PYGZus{}important\PYGZus{}names}
    
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{pca\PYGZus{}features} \PYG{o}{=} \PYG{n}{pca\PYGZus{}most\PYGZus{}important\PYGZus{}features}\PYG{p}{(}\PYG{n}{heart\PYGZus{}disease\PYGZus{}dataset\PYGZus{}standardized}\PYG{p}{)}
\PYG{n}{pca\PYGZus{}features}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
[\PYGZsq{}BMI\PYGZsq{},
 \PYGZsq{}PhysicalHealth\PYGZsq{},
 \PYGZsq{}MentalHealth\PYGZsq{},
 \PYGZsq{}AgeCategory\PYGZsq{},
 \PYGZsq{}GenHealth\PYGZsq{},
 \PYGZsq{}SleepTime\PYGZsq{},
 \PYGZsq{}BMI\PYGZus{}Bin\PYGZsq{},
 \PYGZsq{}LOG\PYGZus{}BMI\PYGZsq{}]
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{X\PYGZus{}resampled}\PYG{p}{,} \PYG{n}{y\PYGZus{}resampled} \PYG{o}{=} \PYG{n}{oversampler\PYGZus{}dict}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{random}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{fit\PYGZus{}resample}\PYG{p}{(}\PYG{n}{X\PYGZus{}train}\PYG{o}{.}\PYG{n}{copy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{y\PYGZus{}train}\PYG{o}{.}\PYG{n}{copy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{run\PYGZus{}exps}\PYG{p}{(}\PYG{n}{X\PYGZus{}resampled}\PYG{p}{[}\PYG{n}{pca\PYGZus{}features}\PYG{p}{]}\PYG{p}{,} \PYG{n}{X\PYGZus{}test}\PYG{p}{[}\PYG{n}{pca\PYGZus{}features}\PYG{p}{]}\PYG{p}{,} \PYG{n}{y\PYGZus{}resampled}\PYG{p}{,} \PYG{n}{y\PYGZus{}test}\PYG{p}{)}
    
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{Model_evaluation_31_0}.png}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Algorithm with the highest accuracy: [\PYGZsq{}Gradient Boost Classifier\PYGZsq{}, 0.7457253265850489]
Algorithm with the highest macro recall:
        [\PYGZsq{}Gradient Boost Classifier\PYGZsq{}, 0.7457253265850489]
Algorithm with the highest macro precision:
        [\PYGZsq{}Gradient Boost Classifier\PYGZsq{}, 0.7491858337247853]
Algorithm with the highest AUC:
        [\PYGZsq{}Gradient Boost Classifier\PYGZsq{}, 0.7457253265850489]
model                      index             precision  recall    f1\PYGZhy{}score  support        auc     
Decision Tree Classifier   Heart Disease     0.691025   0.260003  0.377840  58484.000000   0.571874    0
                           No Heart Disease  0.544265   0.883746  0.673653  58484.000000   0.571874    0
                           accuracy          0.571874   0.571874  0.571874  0.571874       0.571874    0
                           macro avg         0.617645   0.571874  0.525747  116968.000000  0.571874    0
                           weighted avg      0.617645   0.571874  0.525747  116968.000000  0.571874    0
Gradient Boost Classifier  Heart Disease     0.719821   0.804647  0.759874  58484.000000   0.745725    0
                           No Heart Disease  0.778551   0.686803  0.729805  58484.000000   0.745725    0
                           accuracy          0.745725   0.745725  0.745725  0.745725       0.745725    0
                           macro avg         0.749186   0.745725  0.744839  116968.000000  0.745725    0
                           weighted avg      0.749186   0.745725  0.744839  116968.000000  0.745725    0
Logistic Regression        Heart Disease     0.725791   0.780607  0.752202  58484.000000   0.742844    0
                           No Heart Disease  0.762683   0.705082  0.732752  58484.000000   0.742844    0
                           accuracy          0.742844   0.742844  0.742844  0.742844       0.742844    0
                           macro avg         0.744237   0.742844  0.742477  116968.000000  0.742844    0
                           weighted avg      0.744237   0.742844  0.742477  116968.000000  0.742844    0
Naive Bayes                Heart Disease     0.737251   0.614014  0.670013  58484.000000   0.697593    0
                           No Heart Disease  0.669294   0.781171  0.720918  58484.000000   0.697593    0
                           accuracy          0.697593   0.697593  0.697593  0.697593       0.697593    0
                           macro avg         0.703272   0.697593  0.695465  116968.000000  0.697593    0
                           weighted avg      0.703272   0.697593  0.695465  116968.000000  0.697593    0
Random Forest Classifier   Heart Disease     0.710323   0.240373  0.359195  58484.000000   0.571173    0
                           No Heart Disease  0.542834   0.901973  0.677768  58484.000000   0.571173    0
                           accuracy          0.571173   0.571173  0.571173  0.571173       0.571173    0
                           macro avg         0.626579   0.571173  0.518481  116968.000000  0.571173    0
                           weighted avg      0.626579   0.571173  0.518481  116968.000000  0.571173    0
Name: dummy, dtype: int64
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
               index  precision    recall  f1\PYGZhy{}score        support  \PYGZbs{}
0   No Heart Disease   0.762683  0.705082  0.732752   58484.000000   
1      Heart Disease   0.725791  0.780607  0.752202   58484.000000   
2           accuracy   0.742844  0.742844  0.742844       0.742844   
3          macro avg   0.744237  0.742844  0.742477  116968.000000   
4       weighted avg   0.744237  0.742844  0.742477  116968.000000   
5   No Heart Disease   0.778551  0.686803  0.729805   58484.000000   
6      Heart Disease   0.719821  0.804647  0.759874   58484.000000   
7           accuracy   0.745725  0.745725  0.745725       0.745725   
8          macro avg   0.749186  0.745725  0.744839  116968.000000   
9       weighted avg   0.749186  0.745725  0.744839  116968.000000   
10  No Heart Disease   0.544265  0.883746  0.673653   58484.000000   
11     Heart Disease   0.691025  0.260003  0.377840   58484.000000   
12          accuracy   0.571874  0.571874  0.571874       0.571874   
13         macro avg   0.617645  0.571874  0.525747  116968.000000   
14      weighted avg   0.617645  0.571874  0.525747  116968.000000   
15  No Heart Disease   0.542834  0.901973  0.677768   58484.000000   
16     Heart Disease   0.710323  0.240373  0.359195   58484.000000   
17          accuracy   0.571173  0.571173  0.571173       0.571173   
18         macro avg   0.626579  0.571173  0.518481  116968.000000   
19      weighted avg   0.626579  0.571173  0.518481  116968.000000   
20  No Heart Disease   0.669294  0.781171  0.720918   58484.000000   
21     Heart Disease   0.737251  0.614014  0.670013   58484.000000   
22          accuracy   0.697593  0.697593  0.697593       0.697593   
23         macro avg   0.703272  0.697593  0.695465  116968.000000   
24      weighted avg   0.703272  0.697593  0.695465  116968.000000   

                        model       auc dummy  
0        Logistic Regression   0.742844  None  
1        Logistic Regression   0.742844  None  
2        Logistic Regression   0.742844  None  
3        Logistic Regression   0.742844  None  
4        Logistic Regression   0.742844  None  
5   Gradient Boost Classifier  0.745725  None  
6   Gradient Boost Classifier  0.745725  None  
7   Gradient Boost Classifier  0.745725  None  
8   Gradient Boost Classifier  0.745725  None  
9   Gradient Boost Classifier  0.745725  None  
10   Decision Tree Classifier  0.571874  None  
11   Decision Tree Classifier  0.571874  None  
12   Decision Tree Classifier  0.571874  None  
13   Decision Tree Classifier  0.571874  None  
14   Decision Tree Classifier  0.571874  None  
15   Random Forest Classifier  0.571173  None  
16   Random Forest Classifier  0.571173  None  
17   Random Forest Classifier  0.571173  None  
18   Random Forest Classifier  0.571173  None  
19   Random Forest Classifier  0.571173  None  
20                Naive Bayes  0.697593  None  
21                Naive Bayes  0.697593  None  
22                Naive Bayes  0.697593  None  
23                Naive Bayes  0.697593  None  
24                Naive Bayes  0.697593  None  
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
We note a marginal decrease in model performance when using the top 5 features from the 2 principal components. This is likely due to the fact that the top 5 features from the 2 principal components are not the most important features in the dataset.


\subsection{Filter Methods}
\label{\detokenize{Model_evaluation:filter-methods}}
\sphinxAtStartPar
A simple filter method:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{Identify input features having high correlation with target variable}}: We want to keep features with only a high correlation with the target variable. This implies that the input feature has a high influence in predicting the target variable. We set the threshold to the absolute value of 0.2. We keep input features only if the correlation of the input feature with the target variable is greater than 0.2. Our analysis reveled most variables have little if all correlation to our target variable

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{Find the information gain or mutual information of the independent variable with respect to a target variable}}

\end{itemize}

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{idenity\PYGZus{}high\PYGZus{}corr\PYGZus{}features}\PYG{p}{(}\PYG{n}{df}\PYG{p}{:} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{)} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZgt{}} \PYG{n}{List}\PYG{p}{[}\PYG{n+nb}{str}\PYG{p}{]}\PYG{p}{:}
    \PYG{n}{importances} \PYG{o}{=} \PYG{n}{df}\PYG{o}{.}\PYG{n}{drop}\PYG{p}{(}
        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{HeartDisease}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{.}\PYG{n}{apply}\PYG{p}{(}
            \PYG{k}{lambda} \PYG{n}{x}\PYG{p}{:} \PYG{n}{x}\PYG{o}{.}\PYG{n}{corr}\PYG{p}{(}\PYG{n}{df}\PYG{o}{.}\PYG{n}{HeartDisease}\PYG{p}{)}\PYG{p}{)}
    
    \PYG{n}{indices} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{argsort}\PYG{p}{(}\PYG{n}{importances}\PYG{p}{)}
    \PYG{n}{important\PYGZus{}feature\PYGZus{}names} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}
    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{indices}\PYG{p}{)}\PYG{p}{)}\PYG{p}{:}
        \PYG{k}{if} \PYG{n}{np}\PYG{o}{.}\PYG{n}{abs}\PYG{p}{(}\PYG{n}{importances}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{)} \PYG{o}{\PYGZgt{}} \PYG{l+m+mf}{0.2}\PYG{p}{:}
            \PYG{n}{important\PYGZus{}feature\PYGZus{}names}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}
                \PYG{n}{df}\PYG{o}{.}\PYG{n}{columns}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{)}
    \PYG{k}{return} \PYG{n}{important\PYGZus{}feature\PYGZus{}names}
    
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{high\PYGZus{}corr\PYGZus{}features} \PYG{o}{=} \PYG{n}{idenity\PYGZus{}high\PYGZus{}corr\PYGZus{}features}\PYG{p}{(}\PYG{n}{heart\PYGZus{}disease\PYGZus{}dataset\PYGZus{}standardized}\PYG{p}{)}
\PYG{n}{high\PYGZus{}corr\PYGZus{}features}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
[\PYGZsq{}MentalHealth\PYGZsq{}, \PYGZsq{}DiffWalking\PYGZsq{}, \PYGZsq{}PhysicalActivity\PYGZsq{}]
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{mi} \PYG{o}{=} \PYG{n}{mutual\PYGZus{}info\PYGZus{}classif}\PYG{p}{(}\PYG{n}{heart\PYGZus{}disease\PYGZus{}dataset\PYGZus{}standardized}\PYG{o}{.}\PYG{n}{drop}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{HeartDisease}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{,}
                         \PYG{n}{heart\PYGZus{}disease\PYGZus{}dataset\PYGZus{}standardized}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{HeartDisease}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{mi} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{Series}\PYG{p}{(}\PYG{n}{mi}\PYG{p}{)}
\PYG{n}{mi}\PYG{o}{.}\PYG{n}{index} \PYG{o}{=} \PYG{n}{heart\PYGZus{}disease\PYGZus{}dataset\PYGZus{}standardized}\PYG{o}{.}\PYG{n}{drop}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{HeartDisease}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{.}\PYG{n}{columns}
\PYG{n}{mi}\PYG{o}{.}\PYG{n}{sort\PYGZus{}values}\PYG{p}{(}\PYG{n}{ascending}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}\PYG{o}{.}\PYG{n}{plot}\PYG{o}{.}\PYG{n}{bar}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{10}\PYG{p}{,} \PYG{l+m+mi}{4}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}matplotlib.axes.\PYGZus{}subplots.AxesSubplot at 0x7f932dce9090\PYGZgt{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Model_evaluation_36_1}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{top\PYGZus{}10\PYGZus{}mi} \PYG{o}{=} \PYG{n}{mi}\PYG{o}{.}\PYG{n}{sort\PYGZus{}values}\PYG{p}{(}\PYG{n}{ascending}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}\PYG{p}{[}\PYG{p}{:}\PYG{l+m+mi}{10}\PYG{p}{]}\PYG{o}{.}\PYG{n}{index}\PYG{o}{.}\PYG{n}{tolist}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{top\PYGZus{}10\PYGZus{}mi}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
[\PYGZsq{}PhysicalActivity\PYGZsq{},
 \PYGZsq{}GenHealth\PYGZsq{},
 \PYGZsq{}White\PYGZsq{},
 \PYGZsq{}AgeCategory\PYGZsq{},
 \PYGZsq{}Race\PYGZus{}freq\PYGZsq{},
 \PYGZsq{}PhysicalHealth\PYGZsq{},
 \PYGZsq{}Female\PYGZsq{},
 \PYGZsq{}BMI\PYGZus{}Bin\PYGZsq{},
 \PYGZsq{}Male\PYGZsq{},
 \PYGZsq{}DiffWalking\PYGZsq{}]
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
We choose to pick the top 10 features, ranked by mutual information, along with the high correlation features to use in our model.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{filter\PYGZus{}features} \PYG{o}{=} \PYG{n+nb}{list}\PYG{p}{(}\PYG{n+nb}{set}\PYG{p}{(}\PYG{n}{top\PYGZus{}10\PYGZus{}mi}\PYG{p}{)} \PYG{o}{|} \PYG{p}{(}\PYG{n+nb}{set}\PYG{p}{(}\PYG{n}{high\PYGZus{}corr\PYGZus{}features}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{filter\PYGZus{}features}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
[\PYGZsq{}Male\PYGZsq{},
 \PYGZsq{}GenHealth\PYGZsq{},
 \PYGZsq{}BMI\PYGZus{}Bin\PYGZsq{},
 \PYGZsq{}PhysicalActivity\PYGZsq{},
 \PYGZsq{}Race\PYGZus{}freq\PYGZsq{},
 \PYGZsq{}PhysicalHealth\PYGZsq{},
 \PYGZsq{}Female\PYGZsq{},
 \PYGZsq{}White\PYGZsq{},
 \PYGZsq{}DiffWalking\PYGZsq{},
 \PYGZsq{}AgeCategory\PYGZsq{},
 \PYGZsq{}MentalHealth\PYGZsq{}]
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{X\PYGZus{}resampled}\PYG{p}{,} \PYG{n}{y\PYGZus{}resampled} \PYG{o}{=} \PYG{n}{oversampler\PYGZus{}dict}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{random}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{fit\PYGZus{}resample}\PYG{p}{(}\PYG{n}{X\PYGZus{}train}\PYG{o}{.}\PYG{n}{copy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{y\PYGZus{}train}\PYG{o}{.}\PYG{n}{copy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{run\PYGZus{}exps}\PYG{p}{(}\PYG{n}{X\PYGZus{}resampled}\PYG{p}{[}\PYG{n}{filter\PYGZus{}features}\PYG{p}{]}\PYG{p}{,} \PYG{n}{X\PYGZus{}test}\PYG{p}{[}\PYG{n}{filter\PYGZus{}features}\PYG{p}{]}\PYG{p}{,} \PYG{n}{y\PYGZus{}resampled}\PYG{p}{,} \PYG{n}{y\PYGZus{}test}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{Model_evaluation_40_0}.png}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Algorithm with the highest accuracy: [\PYGZsq{}Gradient Boost Classifier\PYGZsq{}, 0.7571301552561385]
Algorithm with the highest macro recall:
        [\PYGZsq{}Gradient Boost Classifier\PYGZsq{}, 0.7571301552561385]
Algorithm with the highest macro precision:
        [\PYGZsq{}Gradient Boost Classifier\PYGZsq{}, 0.7590911327902984]
Algorithm with the highest AUC:
        [\PYGZsq{}Gradient Boost Classifier\PYGZsq{}, 0.7571301552561385]
model                      index             precision  recall    f1\PYGZhy{}score  support        auc     
Decision Tree Classifier   Heart Disease     0.715839   0.541310  0.616460  58484.000000   0.663216    0
                           No Heart Disease  0.631222   0.785121  0.699810  58484.000000   0.663216    0
                           accuracy          0.663216   0.663216  0.663216  0.663216       0.663216    0
                           macro avg         0.673531   0.663216  0.658135  116968.000000  0.663216    0
                           weighted avg      0.673531   0.663216  0.658135  116968.000000  0.663216    0
Gradient Boost Classifier  Heart Disease     0.736551   0.800629  0.767254  58484.000000   0.757130    0
                           No Heart Disease  0.781632   0.713631  0.746085  58484.000000   0.757130    0
                           accuracy          0.757130   0.757130  0.757130  0.757130       0.757130    0
                           macro avg         0.759091   0.757130  0.756670  116968.000000  0.757130    0
                           weighted avg      0.759091   0.757130  0.756670  116968.000000  0.757130    0
Logistic Regression        Heart Disease     0.738478   0.785223  0.761134  58484.000000   0.753574    0
                           No Heart Disease  0.770709   0.721924  0.745519  58484.000000   0.753574    0
                           accuracy          0.753574   0.753574  0.753574  0.753574       0.753574    0
                           macro avg         0.754594   0.753574  0.753327  116968.000000  0.753574    0
                           weighted avg      0.754594   0.753574  0.753327  116968.000000  0.753574    0
Naive Bayes                Heart Disease     0.750320   0.621281  0.679731  58484.000000   0.707270    0
                           No Heart Disease  0.676855   0.793260  0.730449  58484.000000   0.707270    0
                           accuracy          0.707270   0.707270  0.707270  0.707270       0.707270    0
                           macro avg         0.713588   0.707270  0.705090  116968.000000  0.707270    0
                           weighted avg      0.713588   0.707270  0.705090  116968.000000  0.707270    0
Random Forest Classifier   Heart Disease     0.724543   0.547124  0.623457  58484.000000   0.669559    0
                           No Heart Disease  0.636206   0.791994  0.705604  58484.000000   0.669559    0
                           accuracy          0.669559   0.669559  0.669559  0.669559       0.669559    0
                           macro avg         0.680375   0.669559  0.664530  116968.000000  0.669559    0
                           weighted avg      0.680375   0.669559  0.664530  116968.000000  0.669559    0
Name: dummy, dtype: int64
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
               index  precision    recall  f1\PYGZhy{}score        support  \PYGZbs{}
0   No Heart Disease   0.770709  0.721924  0.745519   58484.000000   
1      Heart Disease   0.738478  0.785223  0.761134   58484.000000   
2           accuracy   0.753574  0.753574  0.753574       0.753574   
3          macro avg   0.754594  0.753574  0.753327  116968.000000   
4       weighted avg   0.754594  0.753574  0.753327  116968.000000   
5   No Heart Disease   0.781632  0.713631  0.746085   58484.000000   
6      Heart Disease   0.736551  0.800629  0.767254   58484.000000   
7           accuracy   0.757130  0.757130  0.757130       0.757130   
8          macro avg   0.759091  0.757130  0.756670  116968.000000   
9       weighted avg   0.759091  0.757130  0.756670  116968.000000   
10  No Heart Disease   0.631222  0.785121  0.699810   58484.000000   
11     Heart Disease   0.715839  0.541310  0.616460   58484.000000   
12          accuracy   0.663216  0.663216  0.663216       0.663216   
13         macro avg   0.673531  0.663216  0.658135  116968.000000   
14      weighted avg   0.673531  0.663216  0.658135  116968.000000   
15  No Heart Disease   0.636206  0.791994  0.705604   58484.000000   
16     Heart Disease   0.724543  0.547124  0.623457   58484.000000   
17          accuracy   0.669559  0.669559  0.669559       0.669559   
18         macro avg   0.680375  0.669559  0.664530  116968.000000   
19      weighted avg   0.680375  0.669559  0.664530  116968.000000   
20  No Heart Disease   0.676855  0.793260  0.730449   58484.000000   
21     Heart Disease   0.750320  0.621281  0.679731   58484.000000   
22          accuracy   0.707270  0.707270  0.707270       0.707270   
23         macro avg   0.713588  0.707270  0.705090  116968.000000   
24      weighted avg   0.713588  0.707270  0.705090  116968.000000   

                        model       auc dummy  
0        Logistic Regression   0.753574  None  
1        Logistic Regression   0.753574  None  
2        Logistic Regression   0.753574  None  
3        Logistic Regression   0.753574  None  
4        Logistic Regression   0.753574  None  
5   Gradient Boost Classifier  0.757130  None  
6   Gradient Boost Classifier  0.757130  None  
7   Gradient Boost Classifier  0.757130  None  
8   Gradient Boost Classifier  0.757130  None  
9   Gradient Boost Classifier  0.757130  None  
10   Decision Tree Classifier  0.663216  None  
11   Decision Tree Classifier  0.663216  None  
12   Decision Tree Classifier  0.663216  None  
13   Decision Tree Classifier  0.663216  None  
14   Decision Tree Classifier  0.663216  None  
15   Random Forest Classifier  0.669559  None  
16   Random Forest Classifier  0.669559  None  
17   Random Forest Classifier  0.669559  None  
18   Random Forest Classifier  0.669559  None  
19   Random Forest Classifier  0.669559  None  
20                Naive Bayes  0.707270  None  
21                Naive Bayes  0.707270  None  
22                Naive Bayes  0.707270  None  
23                Naive Bayes  0.707270  None  
24                Naive Bayes  0.707270  None  
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Our filter method results in a marginal decrease in the evaluation metrics tracked when compared to with just the oversampling method applied. Although, we do achieve a slight increase in the all scores when compared to our PCA analysis method.


\section{Hyperparameter finetuning}
\label{\detokenize{Model_evaluation:hyperparameter-finetuning}}
\sphinxAtStartPar
Our best setting was the gradient booting classifier method with the random oversampling method applied. We will now try to improve the performance of the model by finetuning the hyperparameters of the model. We use cross validation on the trainig data to fine tune our model, focusing on the accuracy metric.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{X\PYGZus{}resampled}\PYG{p}{,} \PYG{n}{y\PYGZus{}resampled} \PYG{o}{=} \PYG{n}{oversampler\PYGZus{}dict}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{random}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{fit\PYGZus{}resample}\PYG{p}{(}\PYG{n}{X\PYGZus{}train}\PYG{o}{.}\PYG{n}{copy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{y\PYGZus{}train}\PYG{o}{.}\PYG{n}{copy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{gbc} \PYG{o}{=} \PYG{n}{GradientBoostingClassifier}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{parameters} \PYG{o}{=} \PYG{p}{\PYGZob{}}
    \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{n\PYGZus{}estimators}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:}\PYG{p}{[}\PYG{l+m+mi}{5}\PYG{p}{,}\PYG{l+m+mi}{50}\PYG{p}{,}\PYG{l+m+mi}{250}\PYG{p}{,}\PYG{l+m+mi}{500}\PYG{p}{]}\PYG{p}{,}
    \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{max\PYGZus{}depth}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{,}\PYG{l+m+mi}{5}\PYG{p}{,}\PYG{l+m+mi}{7}\PYG{p}{,}\PYG{l+m+mi}{9}\PYG{p}{]}\PYG{p}{,}
    \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{learning\PYGZus{}rate}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:}\PYG{p}{[}\PYG{l+m+mf}{0.01}\PYG{p}{,}\PYG{l+m+mf}{0.1}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{10}\PYG{p}{,}\PYG{l+m+mi}{100}\PYG{p}{]}
\PYG{p}{\PYGZcb{}}
\PYG{n}{cv} \PYG{o}{=} \PYG{n}{GridSearchCV}\PYG{p}{(}\PYG{n}{gbc}\PYG{p}{,}\PYG{n}{parameters}\PYG{p}{,}\PYG{n}{cv}\PYG{o}{=}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{n}{scoring}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{accuracy}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{cv}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{X\PYGZus{}resampled}\PYG{p}{,}\PYG{n}{y\PYGZus{}resampled}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{display}\PYG{p}{(}\PYG{n}{results}\PYG{p}{)}\PYG{p}{:}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Best parameters are: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{results}\PYG{o}{.}\PYG{n}{best\PYGZus{}params\PYGZus{}}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
    \PYG{n}{mean\PYGZus{}score} \PYG{o}{=} \PYG{n}{results}\PYG{o}{.}\PYG{n}{cv\PYGZus{}results\PYGZus{}}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{mean\PYGZus{}test\PYGZus{}score}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
    \PYG{n}{std\PYGZus{}score} \PYG{o}{=} \PYG{n}{results}\PYG{o}{.}\PYG{n}{cv\PYGZus{}results\PYGZus{}}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{std\PYGZus{}test\PYGZus{}score}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
    \PYG{n}{params} \PYG{o}{=} \PYG{n}{results}\PYG{o}{.}\PYG{n}{cv\PYGZus{}results\PYGZus{}}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{params}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
    \PYG{k}{for} \PYG{n}{mean}\PYG{p}{,}\PYG{n}{std}\PYG{p}{,}\PYG{n}{params} \PYG{o+ow}{in} \PYG{n+nb}{zip}\PYG{p}{(}\PYG{n}{mean\PYGZus{}score}\PYG{p}{,}\PYG{n}{std\PYGZus{}score}\PYG{p}{,}\PYG{n}{params}\PYG{p}{)}\PYG{p}{:}
        \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+si}{\PYGZob{}}\PYG{n+nb}{round}\PYG{p}{(}\PYG{n}{mean}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{)}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{ + or \PYGZhy{}}\PYG{l+s+si}{\PYGZob{}}\PYG{n+nb}{round}\PYG{p}{(}\PYG{n}{std}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{)}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{ for the }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{params}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{display}\PYG{p}{(}\PYG{n}{cv}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}

\section{Pickle Model}
\label{\detokenize{Model_evaluation:pickle-model}}
\sphinxAtStartPar
Pickle is the standard way of serializing objects in Python.

\sphinxAtStartPar
You can use the pickle operation to serialize your machine learning algorithms and save the serialized format to a file.

\sphinxAtStartPar
Later we will load this file to deserialize your model and use it to make new predictions in our web app.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{model}  \PYG{o}{=}  \PYG{n}{GradientBoostingClassifier}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{model}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{X\PYGZus{}resampled}\PYG{p}{,} \PYG{n}{y\PYGZus{}resampled}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
GaussianNB()
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{pickle}\PYG{o}{.}\PYG{n}{dump}\PYG{p}{(}\PYG{n}{model}\PYG{p}{,} \PYG{n+nb}{open}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{../app/model/finalized\PYGZus{}model.sav}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{wb}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxstepscope


\chapter{Conclusion}
\label{\detokenize{Conclusion:conclusion}}\label{\detokenize{Conclusion::doc}}
\sphinxAtStartPar
In this investigation, we investigate the predictability of patients who have or did not have heart disease. Engineered features included a log transformed BMI variable to offset the effect of outliers in our model, a substance abuse binary variable (whether they were a heavy smoker or drinker) and two principal components of the dataset, in an attempt to reduce the dimensionality.  In addition, we augment the existing data by adding participants general BMI classification (underweight, normal weight, overweight obese). This enables us, to examine not only the the risk factors for heart disease but also the risk factors for heart disease in the context of a patient’s BMI.  Furthermore, our method of using K\sphinxhyphen{}means clustering to segment patients into groups with similar risk factors for heart disease is a our approach to understanding the risk factors for heart disease. We propose that this method of segmenting patients into groups with similar risk factors for heart disease can be used to identify patients that are at higher risk of developing heart disease and thus, we added this feature into our final model

\sphinxAtStartPar
Our most significant results are the following:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Exploratory data analysis contradicts  patients own self\sphinxhyphen{}reported health status. The majority of patients who have heart disease report having good or very good health, and the majority of patients who do not have heart disease report having fair or poor health. This suggests that the patients themselves are not aware of their heart disease status.

\item {} 
\sphinxAtStartPar
The most significant risk factors for heart disease for the general population are smoking, alcohol drinking, obesity, and diabetes.

\item {} 
\sphinxAtStartPar
Similarity, our findings suggest that patients who are obese and have a substance abuse problem are at a significantly higher risk of developing heart disease. This is not surprising as obesity is a well known risk factor for heart disease. However, the fact that patients who are obese and have a substance abuse problem are at a significantly higher risk of developing heart disease in the context of their BMI is more interesting. While this is not surprising, it does suggest that there are other factors that impact the risk of heart disease in obese patients.

\item {} 
\sphinxAtStartPar
Out of all the undersupplying and oversampling techniques we investigated to overcome the class imbalance problem in our dataset, random oversampling seemed to be most fruitful. This is because it oversampled the minority class (patients who have heart disease) to the same number of samples as the majority class (patients who do not have heart disease). This is important because it ensures that the model is not biased towards the majority class.

\item {} 
\sphinxAtStartPar
Our final model was serialized using the  pickle library. This allowed us to create a responsive web application. More details on the web app can be found in the \DUrole{xref,myst}{app directory}.

\end{enumerate}


\section{Future work}
\label{\detokenize{Conclusion:future-work}}
\sphinxAtStartPar
As an extension to this work, and some sort of limitation to the work performed here,
different types of classifiers can be included in the analysis and more in depth sensitivity analysis can be performed on these classifiers, also an extension can be made by applying same analysis to other bioinformatics diseases’ datasets, and see the performance of these classifiers to classify and predict these diseases. In addition, we would like to investigate the use of deeper models. Similar endeavors have shown to be fruitful, albeit often decreasing the interoperability of results.  Finally, we are interested in incorporating other features about the subjects such as socio\sphinxhyphen{}economic status,  heart disease prevalence in their family (measured on some continuum), their blood pressure and  cholesterol levels, and their dietary habitats. We hope such features might uncover specific genetic components  patterns or behavioural aspects that might increase or decrease the likehood of heart disease.

\sphinxstepscope


\chapter{References}
\label{\detokenize{References:references}}\label{\detokenize{References::doc}}
\sphinxAtStartPar
@misc\{laya\_healthcare,
title=\{Worrying statistics about heart disease\}, url=\{\sphinxurl{https://www.layahealthcare.ie/pressandmedia/pressreleases/worrying-stats-highlight-heart-disease-risk-among-irish-adults-.html\#\_ftn3}\},
year=\{2020\},
journal=\{Laya Healthcare\}\}

\begin{sphinxthebibliography}{1}
\bibitem[1]{References:id3}
\sphinxAtStartPar
Worrying statistics about heart disease. 2020. URL: \sphinxurl{https://www.layahealthcare.ie/pressandmedia/pressreleases/worrying-stats-highlight-heart-disease-risk-among-irish-adults-.html\#\_ftn3}.
\bibitem[2]{References:id8}
\sphinxAtStartPar
Amit Banerjee and Rajesh N Dave. Validating clusters using the hopkins statistic. In \sphinxstyleemphasis{2004 IEEE International conference on fuzzy systems (IEEE Cat. No. 04CH37542)}, volume 1, 149–153. IEEE, 2004.
\end{sphinxthebibliography}







\renewcommand{\indexname}{Index}
\printindex
\end{document}