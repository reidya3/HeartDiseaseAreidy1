%% Generated by Sphinx.
\def\sphinxdocclass{jupyterBook}
\documentclass[letterpaper,10pt,english]{jupyterBook}
\ifdefined\pdfpxdimen
   \let\sphinxpxdimen\pdfpxdimen\else\newdimen\sphinxpxdimen
\fi \sphinxpxdimen=.75bp\relax
\ifdefined\pdfimageresolution
    \pdfimageresolution= \numexpr \dimexpr1in\relax/\sphinxpxdimen\relax
\fi
%% let collapsible pdf bookmarks panel have high depth per default
\PassOptionsToPackage{bookmarksdepth=5}{hyperref}
%% turn off hyperref patch of \index as sphinx.xdy xindy module takes care of
%% suitable \hyperpage mark-up, working around hyperref-xindy incompatibility
\PassOptionsToPackage{hyperindex=false}{hyperref}
%% memoir class requires extra handling
\makeatletter\@ifclassloaded{memoir}
{\ifdefined\memhyperindexfalse\memhyperindexfalse\fi}{}\makeatother

\PassOptionsToPackage{warn}{textcomp}

\catcode`^^^^00a0\active\protected\def^^^^00a0{\leavevmode\nobreak\ }
\usepackage{cmap}
\usepackage{fontspec}
\defaultfontfeatures[\rmfamily,\sffamily,\ttfamily]{}
\usepackage{amsmath,amssymb,amstext}
\usepackage{polyglossia}
\setmainlanguage{english}



\setmainfont{FreeSerif}[
  Extension      = .otf,
  UprightFont    = *,
  ItalicFont     = *Italic,
  BoldFont       = *Bold,
  BoldItalicFont = *BoldItalic
]
\setsansfont{FreeSans}[
  Extension      = .otf,
  UprightFont    = *,
  ItalicFont     = *Oblique,
  BoldFont       = *Bold,
  BoldItalicFont = *BoldOblique,
]
\setmonofont{FreeMono}[
  Extension      = .otf,
  UprightFont    = *,
  ItalicFont     = *Oblique,
  BoldFont       = *Bold,
  BoldItalicFont = *BoldOblique,
]



\usepackage[Bjarne]{fncychap}
\usepackage[,numfigreset=1,mathnumfig]{sphinx}

\fvset{fontsize=\small}
\usepackage{geometry}


% Include hyperref last.
\usepackage{hyperref}
% Fix anchor placement for figures with captions.
\usepackage{hypcap}% it must be loaded after hyperref.
% Set up styles of URL: it should be placed after hyperref.
\urlstyle{same}


\usepackage{sphinxmessages}



        % Start of preamble defined in sphinx-jupyterbook-latex %
         \usepackage[Latin,Greek]{ucharclasses}
        \usepackage{unicode-math}
        % fixing title of the toc
        \addto\captionsenglish{\renewcommand{\contentsname}{Contents}}
        \hypersetup{
            pdfencoding=auto,
            psdextra
        }
        % End of preamble defined in sphinx-jupyterbook-latex %
        

\title{Heart Disease Prediction Task}
\date{Nov 06, 2022}
\release{}
\author{Anthony Reidy}
\newcommand{\sphinxlogo}{\vbox{}}
\renewcommand{\releasename}{}
\makeindex
\begin{document}

\pagestyle{empty}
\sphinxmaketitle
\pagestyle{plain}
\sphinxtableofcontents
\pagestyle{normal}
\phantomsection\label{\detokenize{Introduction::doc}}


\sphinxAtStartPar
Heart disease, alternatively known as cardiovascular disease, encases various conditions that impact the heart and is the primary basis of death worldwide over the span of the past few decades. Approximately 10,000 people die in Ireland from Cardiovascular Disease each year, accounting for 36\% of deaths per annum {[}{]}. That’s despite the fact that 80\% of all heart disease is deemed preventable. This investigations aims to explore a \sphinxhref{https://www.kaggle.com/code/mushfirat/heartdisease-eda-prediction/notebook}{kaggle dataset} and build a model that can predict the likelihood of a patient having heart disease. More specifically, with this dataset, we would like to see if we can develop a good model to predict if a person has heart disease and what \sphinxstyleemphasis{factors} can be attributed to heart disease most directly. We will be tackling this question with the usage of different regression techniques and algorithms.

\begin{DUlineblock}{0em}
\item[] \sphinxstylestrong{\Large Description of Dataset}
\end{DUlineblock}

\sphinxAtStartPar
We uses an existing dataset from \sphinxhref{https://www.kaggle.com/code/mushfirat/heartdisease-eda-prediction/notebook}{Kaggle}. The dataset comprises approx 320,0000 instances and 14 attributes.

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
No personal identifiable information of the patients are recorded in the dataset.
\end{sphinxadmonition}

\sphinxAtStartPar
The table below summarizes the multiple columns used in this investigation.


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|}
\hline
\sphinxstyletheadfamily 
\sphinxAtStartPar
Name
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Description
\\
\hline
\sphinxAtStartPar
HeartDisease
&
\sphinxAtStartPar
Respondents that have ever reported having coronary heart disease (CHD) or myocardial infarction (MI).
\\
\hline
\sphinxAtStartPar
BMI
&
\sphinxAtStartPar
Body Mass Index.
\\
\hline
\sphinxAtStartPar
Smoking
&
\sphinxAtStartPar
Have you smoked at least 100 cigarettes in your entire life?
\\
\hline
\sphinxAtStartPar
AlcoholDrinking
&
\sphinxAtStartPar
Heavy drinkers (adult men having more than 14 drinks per week and adult women having more than 7 drinks per week
\\
\hline
\sphinxAtStartPar
Stroke
&
\sphinxAtStartPar
(Ever told) (you had) a stroke?
\\
\hline
\sphinxAtStartPar
PhysicalHealth
&
\sphinxAtStartPar
Now thinking about your physical health, which includes physical illness and injury, for how many days during the past 30 days was your physical health not good? (0\sphinxhyphen{}30 days).
\\
\hline
\sphinxAtStartPar
MentalHealth
&
\sphinxAtStartPar
Thinking about your mental health, for how many days during the past 30 days was your mental health not good? (0\sphinxhyphen{}30 days).
\\
\hline
\sphinxAtStartPar
DiffWalking
&
\sphinxAtStartPar
Do you have serious difficulty walking or climbing stairs?
\\
\hline
\sphinxAtStartPar
Sex
&
\sphinxAtStartPar
Are you male or female?
\\
\hline
\sphinxAtStartPar
AgeCategory
&
\sphinxAtStartPar
Fourteen\sphinxhyphen{}level age category. (then calculated the mean)
\\
\hline
\sphinxAtStartPar
Race
&
\sphinxAtStartPar
Imputed race/ethnicity value.
\\
\hline
\sphinxAtStartPar
Diabetic
&
\sphinxAtStartPar
(Ever told) (you had) diabetes?
\\
\hline
\sphinxAtStartPar
PhysicalActivity
&
\sphinxAtStartPar
Adults who reported doing physical activity or exercise during the past 30 days other than their regular job.
\\
\hline
\sphinxAtStartPar
GenHealth
&
\sphinxAtStartPar
Would you say that in general your health is…
\\
\hline
\sphinxAtStartPar
SleepTime
&
\sphinxAtStartPar
On average, how many hours of sleep do you get in a 24\sphinxhyphen{}hour period?
\\
\hline
\sphinxAtStartPar
Asthma
&
\sphinxAtStartPar
(Ever told) (you had) asthma?
\\
\hline
\sphinxAtStartPar
KidneyDisease
&
\sphinxAtStartPar
Not including kidney stones, bladder infection or incontinence, were you ever told you had kidney disease?
\\
\hline
\sphinxAtStartPar
SkinCancer
&
\sphinxAtStartPar
(Ever told) (you had) skin cancer?
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

\begin{DUlineblock}{0em}
\item[] \sphinxstylestrong{\Large Methodology}
\end{DUlineblock}

\sphinxAtStartPar
Methodology will follows a typical data science project: from understanding the dataset through exploratory data analysis, data preparation, model buildings and finally model evaluation. We seek to build a model that predicts heart disease, a binary outcome
In this investigation, we seek to use the K\sphinxhyphen{}means clustering approach to segment the patients into well\sphinxhyphen{}defined groups.
To start, we perform an initial data exploration to perform transformations \& data sanitization checks; acquire rudimentary statistics of the datasets; perform data augmentation; create exploratory visualizations. Next, we perform cluster analysis and evaluate our clusters using metrics such as Silhouette Coefficient and an Elbow curve.
These clusters represent participants that exhibit similar risk factors for heart disease and may have similar underlying determinants of health such as their age, BMI, whether the smoke or have asthma. Next, we envision the probability of developing heart disease in the patients. Finally, we conclude with the most important outcomes of our work.

\sphinxstepscope


\chapter{Initial Data Exploration}
\label{\detokenize{Initial_Data_Exploration:initial-data-exploration}}\label{\detokenize{Initial_Data_Exploration::doc}}
\sphinxAtStartPar
The purpose of our initial data exploration is to:




\section{Importing required libraries}
\label{\detokenize{Initial_Data_Exploration:importing-required-libraries}}
\sphinxAtStartPar
Data processing

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{preprocessing} \PYG{k+kn}{import} \PYG{n}{StandardScaler}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{decomposition} \PYG{k+kn}{import} \PYG{n}{PCA}
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{import} \PYG{n+nn}{copy}
\PYG{k+kn}{import} \PYG{n+nn}{random}
\PYG{k+kn}{import} \PYG{n+nn}{pickle}
\PYG{k+kn}{import} \PYG{n+nn}{json}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
Data Visualization

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}
\PYG{k+kn}{import} \PYG{n+nn}{seaborn} \PYG{k}{as} \PYG{n+nn}{sns}
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{patches} \PYG{k}{as} \PYG{n+nn}{mpatches}
\PYG{k+kn}{import} \PYG{n+nn}{plotly}\PYG{n+nn}{.}\PYG{n+nn}{graph\PYGZus{}objects} \PYG{k}{as} \PYG{n+nn}{go}
\PYG{k+kn}{import} \PYG{n+nn}{squarify} 
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{style}\PYG{o}{.}\PYG{n}{use}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ggplot}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
Code Styling

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{typing} \PYG{k+kn}{import} \PYG{n}{List}\PYG{p}{,} \PYG{n}{Dict}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
Read in dataframe and bried inspection of the data.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{heart\PYGZus{}disease}\PYG{p}{:} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{data/heart\PYGZus{}2020\PYGZus{}cleaned.csv}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}

\section{Data Cleaning}
\label{\detokenize{Initial_Data_Exploration:data-cleaning}}
\sphinxAtStartPar
Check for missing values

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Does the heart disease dataset contain any null values? }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{heart\PYGZus{}disease}\PYG{o}{.}\PYG{n}{isnull}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{any}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{any}\PYG{p}{(}\PYG{p}{)}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
Does the heart disease dataset contain any null values? False
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Next, we calculate the basic statistics of each data set. This is a trivial step, and it is designed to increase understanding of the computational problem.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Unique values in the heart disease dataset, stratified by column:}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{sep}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{k}{for} \PYG{n}{col} \PYG{o+ow}{in} \PYG{n}{heart\PYGZus{}disease}\PYG{o}{.}\PYG{n}{columns}\PYG{p}{:}
    \PYG{k}{if} \PYG{n}{heart\PYGZus{}disease}\PYG{p}{[}\PYG{n}{col}\PYG{p}{]}\PYG{o}{.}\PYG{n}{nunique}\PYG{p}{(}\PYG{p}{)} \PYG{o}{\PYGZlt{}} \PYG{l+m+mi}{20}\PYG{p}{:}
        \PYG{k}{if} \PYG{n}{heart\PYGZus{}disease}\PYG{p}{[}\PYG{n}{col}\PYG{p}{]}\PYG{o}{.}\PYG{n}{nunique}\PYG{p}{(}\PYG{p}{)} \PYG{o}{\PYGZlt{}} \PYG{l+m+mi}{3}\PYG{p}{:} 
            \PYG{n}{heart\PYGZus{}disease}\PYG{p}{[}\PYG{n}{col}\PYG{p}{]}\PYG{o}{.}\PYG{n}{value\PYGZus{}counts}\PYG{p}{(}\PYG{n}{normalize}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{kind}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{barh}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{title}\PYG{o}{=}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Unique values of }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{col}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{ column}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{p}{)}
            \PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
Unique values in the heart disease dataset, stratified by column:
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Initial_Data_Exploration_11_1}.png}

\noindent\sphinxincludegraphics{{Initial_Data_Exploration_11_2}.png}

\noindent\sphinxincludegraphics{{Initial_Data_Exploration_11_3}.png}

\noindent\sphinxincludegraphics{{Initial_Data_Exploration_11_4}.png}

\noindent\sphinxincludegraphics{{Initial_Data_Exploration_11_5}.png}

\noindent\sphinxincludegraphics{{Initial_Data_Exploration_11_6}.png}

\noindent\sphinxincludegraphics{{Initial_Data_Exploration_11_7}.png}

\noindent\sphinxincludegraphics{{Initial_Data_Exploration_11_8}.png}

\noindent\sphinxincludegraphics{{Initial_Data_Exploration_11_9}.png}

\noindent\sphinxincludegraphics{{Initial_Data_Exploration_11_10}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
From this we note the this dataset is imbalanced .i.e, data set with skewed class proportions. This is a common problem in classification problems. We will address this issue later in the notebook.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{heart\PYGZus{}disease}\PYG{o}{.}\PYG{n}{describe}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
                 BMI  PhysicalHealth   MentalHealth      SleepTime
count  319795.000000    319795.00000  319795.000000  319795.000000
mean       28.325399         3.37171       3.898366       7.097075
std         6.356100         7.95085       7.955235       1.436007
min        12.020000         0.00000       0.000000       1.000000
25\PYGZpc{}        24.030000         0.00000       0.000000       6.000000
50\PYGZpc{}        27.340000         0.00000       0.000000       7.000000
75\PYGZpc{}        31.420000         2.00000       3.000000       8.000000
max        94.850000        30.00000      30.000000      24.000000
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{pairplot}\PYG{p}{(}\PYG{n}{heart\PYGZus{}disease}\PYG{p}{[}
    \PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{HeartDisease}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{BMI}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{PhysicalHealth}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{MentalHealth}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{SleepTime}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}
    \PYG{p}{]}\PYG{p}{,} \PYG{n}{hue}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{HeartDisease}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{plot\PYGZus{}kws}\PYG{o}{=}\PYG{n+nb}{dict}\PYG{p}{(}\PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{80}\PYG{p}{,} \PYG{n}{edgecolor}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{white}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{linewidth}\PYG{o}{=}\PYG{l+m+mf}{2.5}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{Initial_Data_Exploration_14_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Surprisingly, the distribution of BMI, mental health and sleep time don’t seem to be significantly different when comparing with people who \sphinxstylestrong{do or do not} have heart disease. However, the mental health do seem to have a strong correlation with heart disease.


\chapter{Data Visualization}
\label{\detokenize{Initial_Data_Exploration:data-visualization}}
\sphinxAtStartPar
To familiarize ourself with the data, we generate plots that seek to illuminate some research questions.
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Are people from certain backgrounds more likely to have heart disease, given their ethnicity?

\item {} 
\sphinxAtStartPar
Is the percentage of females and males with heart disease similar?

\item {} 
\sphinxAtStartPar
Do peoples \sphinxstyleemphasis{reported} physical activity correlate with their physical health?

\item {} 
\sphinxAtStartPar
Which risk factors are most correlated with heart disease?

\item {} 
\sphinxAtStartPar
What is the flow of patients from  their age to substance misuse to if they have an aliment listed in the dataset to  their likelihood of contracting heart disease?

\end{enumerate}

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{create\PYGZus{}stacked\PYGZus{}bar\PYGZus{}hart}\PYG{p}{(}\PYG{n}{heart\PYGZus{}disease}\PYG{p}{:} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{)} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZgt{}} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    Args:}
\PYG{l+s+sd}{        heart\PYGZus{}disease (pd.DataFrame): the original heart disease kaggle dataset}

\PYG{l+s+sd}{    Returns:}
\PYG{l+s+sd}{        plt.figure: a stacked bar chart of the percentage of people }
\PYG{l+s+sd}{            with heart disease and without heart disease, stratified by race}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}

    \PYG{c+c1}{\PYGZsh{} top bar \PYGZhy{}\PYGZgt{} sum all values(HeartDisease=No and HeartDisease=Yes) to find y position of the bars}
    \PYG{n}{total}\PYG{p}{:} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame} \PYG{o}{=} \PYG{n}{heart\PYGZus{}disease}\PYG{o}{.}\PYG{n}{groupby}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Race}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{o}{.}\PYG{n}{count}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{reset\PYGZus{}index}\PYG{p}{(}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} bar chart 1 \PYGZhy{}\PYGZgt{} top bars (group of \PYGZsq{}HeartDisease=No\PYGZsq{})}
    \PYG{n}{bar1}\PYG{p}{:} \PYG{n}{sns}\PYG{o}{.}\PYG{n}{barplot} \PYG{o}{=} \PYG{n}{sns}\PYG{o}{.}\PYG{n}{barplot}\PYG{p}{(}\PYG{n}{x}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Race}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}  \PYG{n}{y}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{HeartDisease}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{data}\PYG{o}{=}\PYG{n}{total}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{red}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} bottom bar \PYGZhy{}\PYGZgt{}  take only HeartDisease=Yes values from the data}
    \PYG{n}{HeartDisease}\PYG{p}{:} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame} \PYG{o}{=} \PYG{n}{heart\PYGZus{}disease}\PYG{p}{[}\PYG{n}{heart\PYGZus{}disease}\PYG{o}{.}\PYG{n}{HeartDisease}\PYG{o}{==}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Yes}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{groupby}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Race}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{o}{.}\PYG{n}{count}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{reset\PYGZus{}index}\PYG{p}{(}\PYG{p}{)}


    \PYG{c+c1}{\PYGZsh{} bar chart 2 \PYGZhy{}\PYGZgt{} bottom bars (group of \PYGZsq{}HeartDisease=Yes\PYGZsq{})}
    \PYG{n}{bar2}\PYG{p}{:} \PYG{n}{sns}\PYG{o}{.}\PYG{n}{barplot} \PYG{o}{=} \PYG{n}{sns}\PYG{o}{.}\PYG{n}{barplot}\PYG{p}{(}\PYG{n}{x}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Race}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{y}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{HeartDisease}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{data}\PYG{o}{=}\PYG{n}{HeartDisease}\PYG{p}{,} \PYG{n}{errorbar}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{,}  \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{green}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} add legend}
    \PYG{n}{top\PYGZus{}bar}\PYG{p}{:} \PYG{n}{mpatches}\PYG{o}{.}\PYG{n}{Patch} \PYG{o}{=} \PYG{n}{mpatches}\PYG{o}{.}\PYG{n}{Patch}\PYG{p}{(}\PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{red}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{HeartDisease = No}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
    \PYG{n}{bottom\PYGZus{}bar}\PYG{p}{:} \PYG{n}{mpatches}\PYG{o}{.}\PYG{n}{Patch} \PYG{o}{=} \PYG{n}{mpatches}\PYG{o}{.}\PYG{n}{Patch}\PYG{p}{(}\PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{green}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{HeartDisease = Yes}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{legend}\PYG{p}{(}\PYG{n}{handles}\PYG{o}{=}\PYG{p}{[}\PYG{n}{top\PYGZus{}bar}\PYG{p}{,} \PYG{n}{bottom\PYGZus{}bar}\PYG{p}{]}\PYG{p}{)}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Number of Patients}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{xticks}\PYG{p}{(}\PYG{n}{rotation}\PYG{o}{=}\PYG{l+m+mi}{45}\PYG{p}{,}\PYG{n}{ha}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{right}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
    \PYG{c+c1}{\PYGZsh{} show the graph}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{create\PYGZus{}tree\PYGZus{}diagram}\PYG{p}{(}\PYG{n}{heart\PYGZus{}disease}\PYG{p}{:}\PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{)} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZgt{}} \PYG{n}{squarify}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    Args:}
\PYG{l+s+sd}{        heart\PYGZus{}disease (pd.DataFrame): the original heart disease kaggle dataset}

\PYG{l+s+sd}{    Returns:}
\PYG{l+s+sd}{        squarify.plot: A tree diagram displaying the number of males and females }
\PYG{l+s+sd}{            that do and don\PYGZsq{}t have heart disease}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{n}{heart\PYGZus{}disease} \PYG{o}{=} \PYG{n}{heart\PYGZus{}disease}\PYG{o}{.}\PYG{n}{copy}\PYG{p}{(}\PYG{p}{)}
    \PYG{n}{tree\PYGZus{}diagram\PYGZus{}df}\PYG{p}{:} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame} \PYG{o}{=} \PYG{n}{heart\PYGZus{}disease}\PYG{o}{.}\PYG{n}{groupby}\PYG{p}{(}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Sex}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{HeartDisease}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}\PYG{o}{.}\PYG{n}{count}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{reset\PYGZus{}index}\PYG{p}{(}\PYG{p}{)}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Sex}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{HeartDisease}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{BMI}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}
    \PYG{n}{tree\PYGZus{}diagram\PYGZus{}df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Gender\PYGZus{}choice}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{tree\PYGZus{}diagram\PYGZus{}df}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Sex}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{HeartDisease}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}\PYG{o}{.}\PYG{n}{agg}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{, }\PYG{l+s+s1}{\PYGZsq{}}\PYG{o}{.}\PYG{n}{join}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
    \PYG{n}{tree\PYGZus{}diagram\PYGZus{}df}\PYG{o}{.}\PYG{n}{rename}\PYG{p}{(}\PYG{n}{columns}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{BMI}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{count}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{\PYGZcb{}}\PYG{p}{,} \PYG{n}{inplace}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
    \PYG{n}{squarify}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{sizes}\PYG{o}{=}\PYG{n}{tree\PYGZus{}diagram\PYGZus{}df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{count}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{n}{tree\PYGZus{}diagram\PYGZus{}df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Gender\PYGZus{}choice}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{alpha}\PYG{o}{=}\PYG{l+m+mf}{.4}\PYG{p}{)}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{axis}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{off}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}

\PYG{k}{def} \PYG{n+nf}{create\PYGZus{}histplot}\PYG{p}{(}\PYG{n}{heart\PYGZus{}disease}\PYG{p}{:}\PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{)} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZgt{}} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    Args:}
\PYG{l+s+sd}{        heart\PYGZus{}disease (pd.DataFrame): the original heart disease kaggle dataset}

\PYG{l+s+sd}{    Returns:}
\PYG{l+s+sd}{        plt.figure: Two histograms displaying the number of days in the past 30 days}
\PYG{l+s+sd}{                    in which patients were physically active. }
\PYG{l+s+sd}{                    Stratified by people\PYGZsq{}s reported physical health}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{n}{heart\PYGZus{}disease} \PYG{o}{=} \PYG{n}{heart\PYGZus{}disease}\PYG{o}{.}\PYG{n}{copy}\PYG{p}{(}\PYG{p}{)}
    \PYG{n}{sns}\PYG{o}{.}\PYG{n}{set}\PYG{p}{(}\PYG{n}{style}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{darkgrid}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
    \PYG{n}{heart\PYGZus{}disease} \PYG{o}{=} \PYG{n}{heart\PYGZus{}disease}\PYG{p}{[}\PYG{n}{heart\PYGZus{}disease}\PYG{o}{.}\PYG{n}{PhysicalHealth} \PYG{o}{\PYGZgt{}} \PYG{l+m+mi}{0}\PYG{p}{]}
    \PYG{c+c1}{\PYGZsh{} plotting both distibutions on the same figure}
    \PYG{n}{sns}\PYG{o}{.}\PYG{n}{histplot}\PYG{p}{(}\PYG{n}{heart\PYGZus{}disease}\PYG{p}{[}\PYG{n}{heart\PYGZus{}disease}\PYG{o}{.}\PYG{n}{PhysicalActivity} \PYG{o}{==} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Yes}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{PhysicalHealth}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{skyblue}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{People who report doing physical activity}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
    \PYG{n}{sns}\PYG{o}{.}\PYG{n}{histplot}\PYG{p}{(}\PYG{n}{heart\PYGZus{}disease}\PYG{p}{[}\PYG{n}{heart\PYGZus{}disease}\PYG{o}{.}\PYG{n}{PhysicalActivity} \PYG{o}{==} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{No}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{PhysicalHealth}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{red}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{People who do not physical activity}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{legend}\PYG{p}{(}\PYG{p}{)}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{People who report that their physical health (physical illness and injury) was not good? (0\PYGZhy{}30 days).}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
    
    
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{create\PYGZus{}corr\PYGZus{}plot}\PYG{p}{(}\PYG{n}{heart\PYGZus{}disease}\PYG{p}{:} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    Args:}
\PYG{l+s+sd}{        heart\PYGZus{}disease (pd.DataFrame): the original heart disease kaggle dataset}

\PYG{l+s+sd}{    Returns:}
\PYG{l+s+sd}{        squarify.plot: A bar chart displaying the correlation of risk factors vs  heart disease }
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{n}{heart\PYGZus{}disease} \PYG{o}{=} \PYG{n}{heart\PYGZus{}disease}\PYG{o}{.}\PYG{n}{copy}\PYG{p}{(}\PYG{p}{)}
    \PYG{n}{binary\PYGZus{}vars}\PYG{p}{:} \PYG{n}{List}\PYG{p}{[}\PYG{n+nb}{str}\PYG{p}{]} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{HeartDisease}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} 
                        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Smoking}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
                        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{AlcoholDrinking}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
                        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Stroke}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} 
                        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{DiffWalking}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
                        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Diabetic}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
                        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{PhysicalActivity}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
                        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Asthma}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
                        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{KidneyDisease}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
                        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{SkinCancer}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}
    \PYG{k}{for} \PYG{n}{binary\PYGZus{}var} \PYG{o+ow}{in} \PYG{n}{binary\PYGZus{}vars}\PYG{p}{:}
        \PYG{n}{heart\PYGZus{}disease}\PYG{p}{[}\PYG{n}{binary\PYGZus{}var}\PYG{p}{]} \PYG{o}{=} \PYG{n}{heart\PYGZus{}disease}\PYG{p}{[}\PYG{n}{binary\PYGZus{}var}\PYG{p}{]}\PYG{o}{.}\PYG{n}{replace}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Yes}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{No}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+m+mi}{0}\PYG{p}{\PYGZcb{}}\PYG{p}{)}
    \PYG{n}{heart\PYGZus{}disease}\PYG{o}{.}\PYG{n}{Diabetic} \PYG{o}{=} \PYG{n}{heart\PYGZus{}disease}\PYG{o}{.}\PYG{n}{Diabetic}\PYG{o}{.}\PYG{n}{replace}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{No, borderline diabetes}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Yes (during pregnancy)}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{:} \PYG{l+m+mi}{1}\PYG{p}{\PYGZcb{}}\PYG{p}{)}\PYG{o}{.}\PYG{n}{astype}\PYG{p}{(}\PYG{n+nb}{int}\PYG{p}{)}
    \PYG{n}{heart\PYGZus{}disease}\PYG{o}{.}\PYG{n}{corr}\PYG{p}{(}\PYG{p}{)}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{HeartDisease}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}\PYG{o}{.}\PYG{n}{plot}\PYG{o}{.}\PYG{n}{bar}\PYG{p}{(}\PYG{n}{legend}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Correlations of HeartDisease variable vs risk factors}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
    
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{create\PYGZus{}sankey\PYGZus{}chart}\PYG{p}{(}\PYG{n}{sankey\PYGZus{}data}\PYG{p}{:}\PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{)} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZgt{}} \PYG{n}{go}\PYG{o}{.}\PYG{n}{Figure}\PYG{p}{:}
  \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{  Args:}
\PYG{l+s+sd}{      heart\PYGZus{}disease (pd.DataFrame): contains three columns; source target value. }
\PYG{l+s+sd}{                                    Indicates the amount of patients moving through each stage of the sankey chart}

\PYG{l+s+sd}{  Returns:}
\PYG{l+s+sd}{      go.Figure: A sankey chart indicating the flow of patients in the following pattern:}
\PYG{l+s+sd}{                    Age \PYGZhy{}\PYGZgt{} Substance Abuse \PYGZhy{}\PYGZgt{} Alinement \PYGZhy{}\PYGZgt{} Heart Disease outcome}
\PYG{l+s+sd}{        }
\PYG{l+s+sd}{  \PYGZdq{}\PYGZdq{}\PYGZdq{}}
  \PYG{n}{sankey\PYGZus{}data} \PYG{o}{=} \PYG{n}{sankey\PYGZus{}data}\PYG{o}{.}\PYG{n}{copy}\PYG{p}{(}\PYG{p}{)}
  \PYG{c+c1}{\PYGZsh{} Get unique values for both source and target columns}
  \PYG{n}{unique\PYGZus{}source\PYGZus{}target\PYGZus{}values}  \PYG{o}{=} \PYG{n+nb}{list}\PYG{p}{(}\PYG{n}{pd}\PYG{o}{.}\PYG{n}{unique}\PYG{p}{(}\PYG{n}{sankey\PYGZus{}data}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{source}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{target}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}\PYG{o}{.}\PYG{n}{values}\PYG{o}{.}\PYG{n}{ravel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{K}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}

  \PYG{n}{source\PYGZus{}target\PYGZus{}mapping} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{n}{k}\PYG{p}{:}\PYG{n}{v} \PYG{k}{for} \PYG{n}{v}\PYG{p}{,}\PYG{n}{k} \PYG{o+ow}{in} \PYG{n+nb}{enumerate}\PYG{p}{(}\PYG{n}{unique\PYGZus{}source\PYGZus{}target\PYGZus{}values}\PYG{p}{)}\PYG{p}{\PYGZcb{}}
  \PYG{k}{with} \PYG{n+nb}{open}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{data/color\PYGZus{}sankey\PYGZus{}dict.json}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)} \PYG{k}{as} \PYG{n}{f}\PYG{p}{:}
      \PYG{n}{colour\PYGZus{}dict}\PYG{o}{=} \PYG{n}{json}\PYG{o}{.}\PYG{n}{loads}\PYG{p}{(}\PYG{n}{f}\PYG{o}{.}\PYG{n}{read}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
  \PYG{n}{links} \PYG{o}{=} \PYG{n}{sankey\PYGZus{}data}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{source}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{target}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{value}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}\PYG{o}{.}\PYG{n}{copy}\PYG{p}{(}\PYG{p}{)}

  \PYG{n}{links}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{source}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{links}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{source}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{map}\PYG{p}{(}\PYG{n}{source\PYGZus{}target\PYGZus{}mapping}\PYG{p}{)}
  \PYG{n}{links}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{target}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{links}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{target}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{map}\PYG{p}{(}\PYG{n}{source\PYGZus{}target\PYGZus{}mapping}\PYG{p}{)}
  \PYG{n}{links\PYGZus{}dict} \PYG{o}{=} \PYG{n}{links}\PYG{o}{.}\PYG{n}{to\PYGZus{}dict}\PYG{p}{(}\PYG{n}{orient}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{list}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

  \PYG{k}{with} \PYG{n+nb}{open}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{data/link\PYGZus{}colours.pkl}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{rb}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)} \PYG{k}{as} \PYG{n}{f}\PYG{p}{:}
      \PYG{n}{link\PYGZus{}colours} \PYG{o}{=} \PYG{n}{pickle}\PYG{o}{.}\PYG{n}{load}\PYG{p}{(}\PYG{n}{f}\PYG{p}{)}
      
  \PYG{n}{fig} \PYG{o}{=} \PYG{n}{go}\PYG{o}{.}\PYG{n}{Figure}\PYG{p}{(}\PYG{n}{data} \PYG{o}{=} \PYG{p}{[}\PYG{n}{go}\PYG{o}{.}\PYG{n}{Sankey}\PYG{p}{(}
      \PYG{n}{valueformat} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{.0f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{valuesuffix} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{TWh}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{c+c1}{\PYGZsh{} Define nodes}
      \PYG{n}{node} \PYG{o}{=} \PYG{n+nb}{dict}\PYG{p}{(}
        \PYG{n}{pad} \PYG{o}{=} \PYG{l+m+mi}{15}\PYG{p}{,}
        \PYG{n}{thickness} \PYG{o}{=} \PYG{l+m+mi}{15}\PYG{p}{,}
        \PYG{n}{line} \PYG{o}{=} \PYG{n+nb}{dict}\PYG{p}{(}\PYG{n}{color} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{black}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{width} \PYG{o}{=} \PYG{l+m+mf}{0.2}\PYG{p}{)}\PYG{p}{,}
        \PYG{n}{label} \PYG{o}{=}  \PYG{n}{unique\PYGZus{}source\PYGZus{}target\PYGZus{}values}\PYG{p}{,}
        \PYG{n}{color} \PYG{o}{=}  \PYG{n+nb}{list}\PYG{p}{(}\PYG{n}{colour\PYGZus{}dict}\PYG{o}{.}\PYG{n}{values}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
      \PYG{p}{)}\PYG{p}{,}
      \PYG{n}{link} \PYG{o}{=} \PYG{n+nb}{dict}\PYG{p}{(}
        \PYG{n}{source} \PYG{o}{=} \PYG{n}{links\PYGZus{}dict}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{source}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{,}
        \PYG{n}{target} \PYG{o}{=} \PYG{n}{links\PYGZus{}dict}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{target}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{,}
        \PYG{n}{value} \PYG{o}{=} \PYG{n}{links\PYGZus{}dict}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{value}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{,}
        \PYG{n}{color}\PYG{o}{=} \PYG{n}{link\PYGZus{}colours}\PYG{p}{,}
      \PYG{p}{)}
          
      \PYG{p}{)}\PYG{p}{]}
  \PYG{p}{)}
  \PYG{n}{fig}\PYG{o}{.}\PYG{n}{update\PYGZus{}layout}\PYG{p}{(}\PYG{n}{title\PYGZus{}text}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Patient Flow \PYGZlt{}br\PYGZgt{} Age \PYGZhy{}\PYGZgt{} Substance Abuse \PYGZhy{}\PYGZgt{} Alinement \PYGZhy{}\PYGZgt{} Heart Disease outcome}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
                    \PYG{n}{font\PYGZus{}size}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{)}
  \PYG{n}{fig}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{create\PYGZus{}stacked\PYGZus{}bar\PYGZus{}hart}\PYG{p}{(}\PYG{n}{heart\PYGZus{}disease}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{Initial_Data_Exploration_21_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
White people appear to both appear most often in this dataset and have the highest prevalence of heart disease in the their sub\sphinxhyphen{}population. The race with the second most heart disease prevalance is heart disease.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{create\PYGZus{}tree\PYGZus{}diagram}\PYG{p}{(}\PYG{n}{heart\PYGZus{}disease}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{Initial_Data_Exploration_23_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
It is evident that relataive rate of heart disease is higher in men than in women.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{create\PYGZus{}histplot}\PYG{p}{(}\PYG{n}{heart\PYGZus{}disease}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{Initial_Data_Exploration_25_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Surprisingly, it seems that peoples reported physical health does not necessarily align with their physical activity levels.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{sankey\PYGZus{}data}\PYG{p}{:} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{data/full\PYGZus{}sankey\PYGZus{}data.csv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{create\PYGZus{}sankey\PYGZus{}chart}\PYG{p}{(}\PYG{n}{sankey\PYGZus{}data}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}

\section{Feature Engineering}
\label{\detokenize{Initial_Data_Exploration:feature-engineering}}
\sphinxAtStartPar
A lot of our features contain categorical data. We will need to convert these to numerical values. Feature extraction projects the original high\sphinxhyphen{}dimensional features to a new feature space with low dimensionality, while feature selection directly selects a subset of relevant features. Both feature extraction
and feature selection can improve learning performance, increase computational efficiency, decrease memory storage, and build better
generalization models. However, as feature extraction creates a set of new features, further analysis is problematic as we cannot retain the
physical meanings of these features. In contrast, by keeping some of the original features, feature selection maintains the physical
meanings of the original features and gives models better readability and interpretability.

\sphinxAtStartPar
The columns broadly fall into three categories:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZhy{} Binary Categorical variables
  \PYGZhy{}  HeartDisease
  \PYGZhy{}  Smoking 
  \PYGZhy{}  AlcoholDrinking
  \PYGZhy{}  Stroke 
  \PYGZhy{}  DiffWalking
  \PYGZhy{}  Diabet   
  \PYGZhy{}  Physical Activity
  \PYGZhy{}  Asthma
  \PYGZhy{}  Kidney Disease
  \PYGZhy{}  Skin Cancer
  \PYGZhy{}  Sex
\PYGZhy{} Continuous variables
  \PYGZhy{} BMI
\PYGZhy{} Discrete variables
  \PYGZhy{} Sleeptime
  \PYGZhy{} PhsycialHealth
  \PYGZhy{} MentalHealth
\PYGZhy{} Polytomous variables; these are variables with more than two categories
  \PYGZhy{} AgeCategory
  \PYGZhy{} Race 
  \PYGZhy{} GenHealth
\end{sphinxVerbatim}

\sphinxAtStartPar
Binary variables with yes or no values are converted to 1 and 0 respectively. The diabetes column currently has four categories: ‘Yes’, ‘No’, ‘No, borderline diabetes’, ‘Yes (during pregnancy)’. Our earlier analysis revealed that  the presence of heart dieasease and diabetes are weakly correlated. Therefore, we will combine the ‘Yes (during pregnancy)’ and ‘No, borderline diabetes’ categories into the ‘Yes’ category.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{heart\PYGZus{}disease}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Diabetic}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{unique}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
array([\PYGZsq{}Yes\PYGZsq{}, \PYGZsq{}No\PYGZsq{}, \PYGZsq{}No, borderline diabetes\PYGZsq{}, \PYGZsq{}Yes (during pregnancy)\PYGZsq{}],
      dtype=object)
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{binary\PYGZus{}vars}\PYG{p}{:} \PYG{n}{List}\PYG{p}{[}\PYG{n+nb}{str}\PYG{p}{]} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{HeartDisease}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} 
                        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Smoking}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
                        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{AlcoholDrinking}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
                        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Stroke}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} 
                        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{DiffWalking}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
                        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Diabetic}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
                        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{PhysicalActivity}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
                        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Asthma}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
                        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{KidneyDisease}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
                        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{SkinCancer}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{for} \PYG{n}{binary\PYGZus{}var} \PYG{o+ow}{in} \PYG{n}{binary\PYGZus{}vars}\PYG{p}{:}
    \PYG{n}{heart\PYGZus{}disease}\PYG{p}{[}\PYG{n}{binary\PYGZus{}var}\PYG{p}{]} \PYG{o}{=} \PYG{n}{heart\PYGZus{}disease}\PYG{p}{[}\PYG{n}{binary\PYGZus{}var}\PYG{p}{]}\PYG{o}{.}\PYG{n}{replace}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Yes}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{No}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+m+mi}{0}\PYG{p}{\PYGZcb{}}\PYG{p}{)}
\PYG{n}{heart\PYGZus{}disease}\PYG{o}{.}\PYG{n}{Diabetic}\PYG{p}{:} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{Series} \PYG{o}{=} \PYG{n}{heart\PYGZus{}disease}\PYG{o}{.}\PYG{n}{Diabetic}\PYG{o}{.}\PYG{n}{replace}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{No, borderline diabetes}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Yes (during pregnancy)}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{:} \PYG{l+m+mi}{1}\PYG{p}{\PYGZcb{}}\PYG{p}{)}\PYG{o}{.}\PYG{n}{astype}\PYG{p}{(}\PYG{n+nb}{int}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} One\PYGZhy{}hot encoding }
\PYG{n}{sex\PYGZus{}one\PYGZus{}hot} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{get\PYGZus{}dummies}\PYG{p}{(}\PYG{n}{heart\PYGZus{}disease}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Sex}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{heart\PYGZus{}disease} \PYG{o}{=} \PYG{n}{heart\PYGZus{}disease}\PYG{o}{.}\PYG{n}{join}\PYG{p}{(}\PYG{n}{sex\PYGZus{}one\PYGZus{}hot}\PYG{p}{)}
\PYG{n}{heart\PYGZus{}disease}\PYG{o}{.}\PYG{n}{drop}\PYG{p}{(}\PYG{n}{columns}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Sex}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{inplace}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
In the dataset, ordinal attributes are present. There is a natural ordering to the categories (i.e. GenHealth, AgeCategory). Thus, we use the \sphinxcode{\sphinxupquote{replace()}} function to label encode them.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{heart\PYGZus{}disease}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{AgeCategory}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{replace}\PYG{p}{(}\PYG{p}{\PYGZob{}}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{18\PYGZhy{}24}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+m+mi}{1}\PYG{p}{,}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{25\PYGZhy{}29}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+m+mi}{2}\PYG{p}{,}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{30\PYGZhy{}34}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+m+mi}{3}\PYG{p}{,}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{35\PYGZhy{}39}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+m+mi}{4}\PYG{p}{,}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{40\PYGZhy{}44}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+m+mi}{5}\PYG{p}{,}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{45\PYGZhy{}49}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+m+mi}{6}\PYG{p}{,}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{50\PYGZhy{}54}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+m+mi}{7}\PYG{p}{,}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{55\PYGZhy{}59}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+m+mi}{8}\PYG{p}{,}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{60\PYGZhy{}64}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+m+mi}{9}\PYG{p}{,}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{65\PYGZhy{}69}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+m+mi}{10}\PYG{p}{,}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{70\PYGZhy{}74}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+m+mi}{11}\PYG{p}{,}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{75\PYGZhy{}79}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+m+mi}{12}\PYG{p}{,}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{80 or older}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+m+mi}{13}
\PYG{p}{\PYGZcb{}}\PYG{p}{,} \PYG{n}{inplace}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{heart\PYGZus{}disease}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{GenHealth}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{replace}\PYG{p}{(}\PYG{p}{\PYGZob{}}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Poor}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{1}\PYG{p}{,}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Fair}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+m+mi}{2}\PYG{p}{,} 
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Good}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+m+mi}{3}\PYG{p}{,} 
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Very good}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+m+mi}{4}\PYG{p}{,}  
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Excellent}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+m+mi}{5}\PYG{p}{\PYGZcb{}}\PYG{p}{,}
    \PYG{n}{inplace}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
Race has no order. Therefore, we decide to take two approaches.
\begin{itemize}
\item {} 
\sphinxAtStartPar
One\sphinxhyphen{}hot\sphinxhyphen{}encoding:Increase the dimensions to facilite a better feature selection space step later which produces a final lower dimension feature vector (i.e. every race is a binary feature)

\item {} 
\sphinxAtStartPar
Frequency encoding:  Frequency Encoding is an encoding technique that encodes categorical feature values to their respected frequencies. This will preserve the information about the values of distributions. We normalize the frequencies that result in getting the sum of unique values as 1. This is done to avoid the curse of dimensionality and reduce the sparsity in the dataset.

\end{itemize}

\sphinxAtStartPar
We will evaluate both approaches in our model evaluation.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} One\PYGZhy{}hot encoding }
\PYG{n}{race\PYGZus{}one\PYGZus{}hot}\PYG{p}{:} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{get\PYGZus{}dummies}\PYG{p}{(}\PYG{n}{heart\PYGZus{}disease}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Race}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{heart\PYGZus{}disease}\PYG{p}{:} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame} \PYG{o}{=} \PYG{n}{heart\PYGZus{}disease}\PYG{o}{.}\PYG{n}{join}\PYG{p}{(}\PYG{n}{race\PYGZus{}one\PYGZus{}hot}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Frequency encoding using value\PYGZus{}counts function }
\PYG{n}{race\PYGZus{}freq}\PYG{p}{:} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{Series} \PYG{o}{=} \PYG{n}{heart\PYGZus{}disease}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Race}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{value\PYGZus{}counts}\PYG{p}{(}\PYG{n}{normalize}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Mapping the encoded values with original data }
\PYG{n}{heart\PYGZus{}disease}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Race\PYGZus{}freq}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{:} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{Series} \PYG{o}{=} \PYG{n}{heart\PYGZus{}disease}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Race}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{apply}\PYG{p}{(}\PYG{k}{lambda} \PYG{n}{x} \PYG{p}{:} \PYG{n}{race\PYGZus{}freq}\PYG{p}{[}\PYG{n}{x}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{heart\PYGZus{}disease}\PYG{o}{.}\PYG{n}{drop}\PYG{p}{(}\PYG{n}{columns}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Race}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{inplace}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
Outliers are extreme values. Outliers can skew the distribution but these are exceptional but genuine data points.  Such distributions can impact certain algorithms such as regression type models (Lasso), k Nearest neighbors and Naive Bayes. However, decision trees and its ensemble (random forest) are not impacted.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{heart\PYGZus{}disease}\PYG{o}{.}\PYG{n}{hist}\PYG{p}{(}\PYG{n}{column}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{BMI}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
     \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{SleepTime}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
     \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{PhysicalHealth}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
     \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{MentalHealth}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
     \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{AgeCategory}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
     \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Race\PYGZus{}freq}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} 
     \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{GenHealth}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{layout}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{4}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{tight\PYGZus{}layout}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{Initial_Data_Exploration_40_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{green\PYGZus{}diamond}\PYG{p}{:} \PYG{n}{Dict}\PYG{p}{[}\PYG{n+nb}{str}\PYG{p}{,} \PYG{n+nb}{str}\PYG{p}{]} \PYG{o}{=} \PYG{n+nb}{dict}\PYG{p}{(}\PYG{n}{markerfacecolor}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{g}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{marker}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{D}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{fig1}\PYG{p}{,} \PYG{n}{ax1} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{ax1}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Changed Outlier Symbols}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{ax1}\PYG{o}{.}\PYG{n}{boxplot}\PYG{p}{(}\PYG{n}{heart\PYGZus{}disease}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{BMI}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{flierprops}\PYG{o}{=}\PYG{n}{green\PYGZus{}diamond}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZob{}\PYGZsq{}whiskers\PYGZsq{}: [\PYGZlt{}matplotlib.lines.Line2D at 0x7fb08bddb450\PYGZgt{},
  \PYGZlt{}matplotlib.lines.Line2D at 0x7fb08bddb790\PYGZgt{}],
 \PYGZsq{}caps\PYGZsq{}: [\PYGZlt{}matplotlib.lines.Line2D at 0x7fb08bddbad0\PYGZgt{},
  \PYGZlt{}matplotlib.lines.Line2D at 0x7fb08bddbe10\PYGZgt{}],
 \PYGZsq{}boxes\PYGZsq{}: [\PYGZlt{}matplotlib.lines.Line2D at 0x7fb08bddb150\PYGZgt{}],
 \PYGZsq{}medians\PYGZsq{}: [\PYGZlt{}matplotlib.lines.Line2D at 0x7fb08bde61d0\PYGZgt{}],
 \PYGZsq{}fliers\PYGZsq{}: [\PYGZlt{}matplotlib.lines.Line2D at 0x7fb08bde6510\PYGZgt{}],
 \PYGZsq{}means\PYGZsq{}: []\PYGZcb{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Initial_Data_Exploration_41_1}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
We note that many outliers exist in the BMI column. To rectify this, we take two approaches
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Turn it into a ordinal variable}: The WHO uses BMI as a convenient rule of thumb used to broadly categorize a person as underweight, normal weight, overweight, or obese based on tissue mass (muscle, fat, and bone) and height. Major adult BMI classifications are underweight (under 18.5 kg/m2), normal weight (18.5 to 24.9), overweight (25 to 29.9), and obese (30 or more). We will encode these describe caegories into ordinal values.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Take log(x) of the BMI column}: Taking the log of a feature is a common trick to reduce the effect of outliers. This is because the log function is monotonically increasing. Therefore, the effect of outliers is reduced.
Again, both aproaches will be evaluated in our model evaluation.

\end{itemize}

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{bins}\PYG{p}{:} \PYG{n}{List}\PYG{p}{[}\PYG{n+nb}{int}\PYG{p}{]} \PYG{o}{=} \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mf}{18.5}\PYG{p}{,} \PYG{l+m+mf}{24.9}\PYG{p}{,} \PYG{l+m+mf}{29.9}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{inf}\PYG{p}{]}
\PYG{n}{names}\PYG{p}{:} \PYG{n}{List}\PYG{p}{[}\PYG{n+nb}{int}\PYG{p}{]} \PYG{o}{=} \PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{,} \PYG{l+m+mi}{4}\PYG{p}{]}
\PYG{n}{heart\PYGZus{}disease}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{BMI\PYGZus{}Bin}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{:} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{Series} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{cut}\PYG{p}{(}\PYG{n}{heart\PYGZus{}disease}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{BMI}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{bins}\PYG{p}{,} \PYG{n}{labels}\PYG{o}{=}\PYG{n}{names}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{heart\PYGZus{}disease}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{LOG\PYGZus{}BMI}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{:} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{Series} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{log}\PYG{p}{(}\PYG{n}{heart\PYGZus{}disease}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{BMI}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{heart\PYGZus{}disease}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{LOG\PYGZus{}BMI}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{hist}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}AxesSubplot:\PYGZgt{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Initial_Data_Exploration_44_1}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}

\subsection{PCA}
\label{\detokenize{Initial_Data_Exploration:pca}}
\sphinxAtStartPar
Standardization is typically used for features of incomparable units. E.g. someone reporting the number of times they were physical active in the last thirty days and BMI. Standardisation will be applied to all features (both orginal and engineered) except the binary variables. We will also standardize the features due to k\sphinxhyphen{}means “isotropic” nature. In this case, if we left our variances unequal; we would inversely be putting more weight on features with high variance. In addition, we will perform principal component analysis due to avoid the curse of dimensionality that k\sphinxhyphen{}means can suffer from. The function of PCA is to reduce the dimensionality of a data set consisting of many variables correlated with each other, either heavily or lightly, while retaining the variation present in the data set to the maximum extent.

\sphinxAtStartPar
The same is done by transforming the variables (i.e. features) to a new set of variables, which are known as the principal components, ordered such that the retention of variation present decreases as we move down the order of components.

\sphinxAtStartPar
In addition, we will perform principal component analysis due to avoid the curse of dimensionality that some algorithims can suffer from. Initally, PCA is only perfoormed on the \sphinxstyleemphasis{features present within the orginal dataset}. Later on,  we will perform A feature extraction method u by applying different subsets  of training  data to estimate the accuracy of these subsets for all used
classifiers and measure the quality of the generated subsets  per classification algorithm, and the results of the classifier are shown. We plan to use PCA again for those best performing subets to see if any improvement is made.

\sphinxAtStartPar
The function of PCA is to reduce the dimensionality of a data set consisting of many variables correlated with each other,
either heavily or lightly, while retaining the variation present in the data set to the maximum extent.

\sphinxAtStartPar
The same is done by transforming the variables (i.e. features) to a new set of variables, which are known as the principal components, ordered such that the retention of variation present decreases as we move down the order of components.

\sphinxAtStartPar
The procedure of PCA involves five steps: 
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Standardize the data 

\item {} 
\sphinxAtStartPar
Compute covariance matrix 

\item {} 
\sphinxAtStartPar
Identify the eigenvalues and eigenvectors of the covariance matrix and order them according to the eigenvalues 

\item {} 
\sphinxAtStartPar
Compute a feature vector 

\item {} 
\sphinxAtStartPar
Recast the data 

\end{enumerate}


\subsubsection{Standardisation}
\label{\detokenize{Initial_Data_Exploration:standardisation}}
\sphinxAtStartPar
We now standardize the data using the following formulae:
\begin{equation*}
\begin{split}
X_i = X_i - \bar{X}~~~~~~~~~~~~~~~~~~X_i = \frac{X_i}{\sigma}
\end{split}
\end{equation*}
\sphinxAtStartPar
The standard deviation should equal 1 after standardization

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{heart\PYGZus{}disease}\PYG{p}{[}
    \PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{PhysicalHealth}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{MentalHealth}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{AgeCategory}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{GenHealth}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{SleepTime}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{BMI\PYGZus{}Bin}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
    \PYG{p}{]} \PYG{o}{=} \PYG{n}{StandardScaler}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{fit\PYGZus{}transform}\PYG{p}{(}\PYG{n}{heart\PYGZus{}disease}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{PhysicalHealth}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{MentalHealth}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{AgeCategory}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{GenHealth}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{SleepTime}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{BMI\PYGZus{}Bin}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{heart\PYGZus{}disease}\PYG{o}{.}\PYG{n}{to\PYGZus{}csv}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{data/standardised\PYGZus{}heart\PYGZus{}disease.csv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{index}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
We will use the \sphinxcode{\sphinxupquote{PCA}} function supplied by the \sphinxcode{\sphinxupquote{Scikit\sphinxhyphen{}learn}} library for dimensionality reduction.  But how do we find the optimal number of components? Which eigenvalues are important?  The scree plot below describes the cumulative explained variance for each component. We reach 80\% explained variance at the three component mark.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{orginal\PYGZus{}features\PYGZus{}df}\PYG{p}{:} \PYG{n}{List}\PYG{p}{[}\PYG{n+nb}{str}\PYG{p}{]} \PYG{o}{=} \PYG{n}{heart\PYGZus{}disease}\PYG{p}{[}\PYG{p}{[} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Smoking}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{AlcoholDrinking}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Stroke}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
       \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{PhysicalHealth}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{MentalHealth}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{DiffWalking}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{AgeCategory}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{PhysicalActivity}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{GenHealth}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{SleepTime}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Asthma}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
       \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{KidneyDisease}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{SkinCancer}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Female}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Male}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
       \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{American Indian/Alaskan Native}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Asian}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Black}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Hispanic}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Other}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
       \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{White}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{pca} \PYG{o}{=} \PYG{n}{PCA}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{orginal\PYGZus{}features\PYGZus{}df}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{cumsum}\PYG{p}{(}\PYG{n}{pca}\PYG{o}{.}\PYG{n}{explained\PYGZus{}variance\PYGZus{}ratio\PYGZus{}}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{number of components}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{cumulative explained variance}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Scree plot}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{Initial_Data_Exploration_53_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
We note a slight indent at the the 5th principal compent mark. According to the average\sphinxhyphen{}eigenvalue test (Kaiser\sphinxhyphen{}Guttman test) we should retain only those eigenvalues that are above the average which is 1.0. 
Jolliffe relaxes this criterium and suggest to retain eigenvalues greater than 0.7.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{kasier\PYGZus{}criterion}\PYG{p}{:} \PYG{n}{np}\PYG{o}{.}\PYG{n}{int64} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{where}\PYG{p}{(}\PYG{n}{pca}\PYG{o}{.}\PYG{n}{explained\PYGZus{}variance\PYGZus{}} \PYG{o}{\PYGZgt{}} \PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}
\PYG{n+nb}{print}\PYG{p}{(}
        \PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Kasier criterion optimal component number: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{kasier\PYGZus{}criterion}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{, explained variance: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{np}\PYG{o}{.}\PYG{n}{cumsum}\PYG{p}{(}\PYG{n}{pca}\PYG{o}{.}\PYG{n}{explained\PYGZus{}variance\PYGZus{}ratio\PYGZus{}}\PYG{p}{)}\PYG{p}{[}\PYG{n}{kasier\PYGZus{}criterion}\PYG{p}{]}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}
    \PYG{p}{)}
\PYG{n}{jolliffe\PYGZus{}criterion}\PYG{p}{:} \PYG{n}{np}\PYG{o}{.}\PYG{n}{int64}  \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{where}\PYG{p}{(}\PYG{n}{pca}\PYG{o}{.}\PYG{n}{explained\PYGZus{}variance\PYGZus{}} \PYG{o}{\PYGZgt{}} \PYG{l+m+mf}{0.7}\PYG{p}{)}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}
\PYG{n+nb}{print}\PYG{p}{(}
    \PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Jolliffe criterion optimal component number: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{jolliffe\PYGZus{}criterion}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{ , explained variance: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{np}\PYG{o}{.}\PYG{n}{cumsum}\PYG{p}{(}\PYG{n}{pca}\PYG{o}{.}\PYG{n}{explained\PYGZus{}variance\PYGZus{}ratio\PYGZus{}}\PYG{p}{)}\PYG{p}{[}\PYG{n}{jolliffe\PYGZus{}criterion}\PYG{p}{]}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}
    \PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
Kasier criterion optimal component number: 1, explained variance: 0.44862010192451435
Jolliffe criterion optimal component number: 2 , explained variance: 0.5825444541168425
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
For the purpose of this investigation, we decide to go with \sphinxstylestrong{both} the Jolaliffe criterion, we will retain the first two  components.

\sphinxAtStartPar
Finally, we fit the \sphinxcode{\sphinxupquote{pca}} model with the dataframes containing top 2 components , apply the dimensionality reduction on those respective dataframe and save the resulting dataframes.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{pca\PYGZus{}2d} \PYG{o}{=} \PYG{n}{PCA}\PYG{p}{(}\PYG{n}{n\PYGZus{}components}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{n}{dim\PYGZus{}reduced\PYGZus{}2d}\PYG{p}{:} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame} \PYG{o}{=} \PYG{n}{pca\PYGZus{}2d}\PYG{o}{.}\PYG{n}{fit\PYGZus{}transform}\PYG{p}{(}\PYG{n}{orginal\PYGZus{}features\PYGZus{}df}\PYG{p}{)}
\PYG{n}{dim\PYGZus{}reduced\PYGZus{}2d} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{(}\PYG{n}{data}\PYG{o}{=}\PYG{n}{dim\PYGZus{}reduced\PYGZus{}2d}\PYG{p}{,} \PYG{n}{columns}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{component\PYGZus{}}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{num}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}} \PYG{k}{for} \PYG{n}{num} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{dim\PYGZus{}reduced\PYGZus{}2d} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{merge}\PYG{p}{(}
        \PYG{n}{heart\PYGZus{}disease}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{HeartDisease}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}\PYG{p}{,} \PYG{n}{dim\PYGZus{}reduced\PYGZus{}2d}\PYG{p}{,} \PYG{n}{left\PYGZus{}index}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,} \PYG{n}{right\PYGZus{}index}\PYG{o}{=}\PYG{k+kc}{True}
        \PYG{p}{)}
\PYG{n}{dim\PYGZus{}reduced\PYGZus{}2d}\PYG{o}{.}\PYG{n}{to\PYGZus{}csv}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{data/dim\PYGZus{}reduced\PYGZus{}2d.csv}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{index}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
\sphinxstylestrong{Note}, later on we will use PCA to identify the variables that contribute most to the variation in the dataset.

\sphinxstepscope


\chapter{Cluster Analysis}
\label{\detokenize{Clustering_Analysis:cluster-analysis}}\label{\detokenize{Clustering_Analysis::doc}}
\sphinxAtStartPar
The purpose of our cluster analysis is to:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Measure clustering \& central tendency.

\item {} 
\sphinxAtStartPar
Perform k\sphinxhyphen{}means

\item {} 
\sphinxAtStartPar
Evaluate the clusters, \sphinxstylestrong{particularly}:
\begin{itemize}
\item {} 
\sphinxAtStartPar
does the dataset naturally cluster into people who do and do not have heart disease?

\item {} 
\sphinxAtStartPar
people with alchol abuse issues

\item {} 
\sphinxAtStartPar
underweight vs normal weight vs overweight vs obese

\end{itemize}

\end{itemize}


\section{Import libaries}
\label{\detokenize{Clustering_Analysis:import-libaries}}

\subsection{Data Processing}
\label{\detokenize{Clustering_Analysis:data-processing}}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}

\subsection{Scientific computing}
\label{\detokenize{Clustering_Analysis:scientific-computing}}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{neighbors} \PYG{k+kn}{import} \PYG{n}{NearestNeighbors}
\PYG{k+kn}{from} \PYG{n+nn}{random} \PYG{k+kn}{import} \PYG{n}{sample}
\PYG{k+kn}{from} \PYG{n+nn}{numpy}\PYG{n+nn}{.}\PYG{n+nn}{random} \PYG{k+kn}{import} \PYG{n}{uniform}
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{from} \PYG{n+nn}{math} \PYG{k+kn}{import} \PYG{n}{isnan}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{preprocessing} \PYG{k+kn}{import} \PYG{n}{StandardScaler}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}

\subsection{Clustering}
\label{\detokenize{Clustering_Analysis:clustering}}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{cluster} \PYG{k+kn}{import} \PYG{n}{KMeans}\PYG{p}{,} \PYG{n}{MiniBatchKMeans}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{metrics} \PYG{k+kn}{import} \PYG{n}{silhouette\PYGZus{}score}
\PYG{k+kn}{from} \PYG{n+nn}{pyclustertend} \PYG{k+kn}{import} \PYG{n}{ivat}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}

\subsection{Data Visualisation}
\label{\detokenize{Clustering_Analysis:data-visualisation}}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}
\PYG{k+kn}{from} \PYG{n+nn}{yellowbrick}\PYG{n+nn}{.}\PYG{n+nn}{cluster} \PYG{k+kn}{import} \PYG{n}{SilhouetteVisualizer}
\PYG{k+kn}{import} \PYG{n+nn}{seaborn} \PYG{k}{as} \PYG{n+nn}{sns}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}

\section{Measure Cluster Tendency}
\label{\detokenize{Clustering_Analysis:measure-cluster-tendency}}
\sphinxAtStartPar
Clustering algorithms such as k\sphinxhyphen{}means are used to determine the structure of multi\sphinxhyphen{}dimensional data. Clusters are disjoint natural groups. However, K\sphinxhyphen{}means will find clusters in data even if none “actually” exist. Therefore, a fundamental question before applying any clustering algorithms is: Are clusters present at all?
We will measure the clustering tendency of both datasets before subjecting it to k\sphinxhyphen{}means. These datasets contain the top \sphinxstylestrong{two principal components (2D)}. To do this, we employ
\begin{itemize}
\item {} 
\sphinxAtStartPar
Hopkins’s statistic  of randomness

\end{itemize}


\subsection{Hopkins statistics}
\label{\detokenize{Clustering_Analysis:hopkins-statistics}}
\sphinxAtStartPar
Hopkins statistics {[}\hyperlink{cite.References:id7}{Banerjee and Dave, 2004}{]} tests the spatial randomness of a dataset i.e. it measures the probability that a given dataset aligns with a uniform distribution. It is based on the difference between the distance from a real point to its nearest neighbour, U, and the distance from a uniformly generated point within the data space to the nearest real data point, W.
\begin{itemize}
\item {} 
\sphinxAtStartPar
\(H_{0}\): The dataset \sphinxstylestrong{is} uniformly distributed

\item {} 
\sphinxAtStartPar
\(H_{1}\): The dataset \sphinxstylestrong{is not} uniformly distributed

\end{itemize}
\begin{equation*}
\begin{split}
H = \frac{\sum_{i=1}^{m} u_{i}^{d}}{\sum_{i=1}^{m} u_{i}^{d} + \sum_{i=1}^{m} w_{i}^{d}}
\end{split}
\end{equation*}
\sphinxAtStartPar
If the value of the Hopkins statistic(H) is close to 1 (above 0.5), we reject \(H_{0}\) and can conclude that the dataset is considered significantly clusterable.  Otherwise, we fail to reject \(H_{0}\) and can conclude that the dataset is considered significantly uniformly distributed.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{hopkins}\PYG{p}{(}\PYG{n}{X}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    Hopkins statistic. Code snippet:}
\PYG{l+s+sd}{        https://matevzkunaver.wordpress.com/2017/06/20/hopkins\PYGZhy{}test\PYGZhy{}for\PYGZhy{}cluster\PYGZhy{}tendency/}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{n}{d} \PYG{o}{=} \PYG{n}{X}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}
    \PYG{n}{n} \PYG{o}{=} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{X}\PYG{p}{)} 
    \PYG{n}{m} \PYG{o}{=} \PYG{n+nb}{int}\PYG{p}{(}\PYG{l+m+mf}{0.1} \PYG{o}{*} \PYG{n}{n}\PYG{p}{)} 
    \PYG{n}{nbrs} \PYG{o}{=} \PYG{n}{NearestNeighbors}\PYG{p}{(}\PYG{n}{n\PYGZus{}neighbors}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{X}\PYG{o}{.}\PYG{n}{values}\PYG{p}{)}
 
    \PYG{n}{rand\PYGZus{}X} \PYG{o}{=} \PYG{n}{sample}\PYG{p}{(}\PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{n}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{,} \PYG{n}{m}\PYG{p}{)}
 
    \PYG{n}{ujd} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}
    \PYG{n}{wjd} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}
    \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{m}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{u\PYGZus{}dist}\PYG{p}{,} \PYG{n}{\PYGZus{}} \PYG{o}{=} \PYG{n}{nbrs}\PYG{o}{.}\PYG{n}{kneighbors}\PYG{p}{(}\PYG{n}{uniform}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{amin}\PYG{p}{(}\PYG{n}{X}\PYG{p}{,}\PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}\PYG{p}{,}\PYG{n}{np}\PYG{o}{.}\PYG{n}{amax}\PYG{p}{(}\PYG{n}{X}\PYG{p}{,}\PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}\PYG{p}{,}\PYG{n}{d}\PYG{p}{)}\PYG{o}{.}\PYG{n}{reshape}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{return\PYGZus{}distance}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
        \PYG{n}{ujd}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{u\PYGZus{}dist}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}
        \PYG{n}{w\PYGZus{}dist}\PYG{p}{,} \PYG{n}{\PYGZus{}} \PYG{o}{=} \PYG{n}{nbrs}\PYG{o}{.}\PYG{n}{kneighbors}\PYG{p}{(}\PYG{n}{X}\PYG{o}{.}\PYG{n}{iloc}\PYG{p}{[}\PYG{n}{rand\PYGZus{}X}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}\PYG{p}{]}\PYG{o}{.}\PYG{n}{values}\PYG{o}{.}\PYG{n}{reshape}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{return\PYGZus{}distance}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
        \PYG{n}{wjd}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{w\PYGZus{}dist}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}
 
    \PYG{n}{H} \PYG{o}{=} \PYG{n+nb}{sum}\PYG{p}{(}\PYG{n}{ujd}\PYG{p}{)} \PYG{o}{/} \PYG{p}{(}\PYG{n+nb}{sum}\PYG{p}{(}\PYG{n}{ujd}\PYG{p}{)} \PYG{o}{+} \PYG{n+nb}{sum}\PYG{p}{(}\PYG{n}{wjd}\PYG{p}{)}\PYG{p}{)}
    \PYG{k}{if} \PYG{n}{isnan}\PYG{p}{(}\PYG{n}{H}\PYG{p}{)}\PYG{p}{:}
        \PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{ujd}\PYG{p}{,} \PYG{n}{wjd}\PYG{p}{)}
        \PYG{n}{H} \PYG{o}{=} \PYG{l+m+mi}{0}
 
    \PYG{k}{return} \PYG{n}{H}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{dim\PYGZus{}reduced\PYGZus{}2d} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{data/dim\PYGZus{}reduced\PYGZus{}2d.csv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} Get labels as defined in the markdown cell above to compare how k\PYGZhy{}means cluster patients}
\PYG{n}{original\PYGZus{}heart\PYGZus{}disease} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{data/heart\PYGZus{}2020\PYGZus{}cleaned.csv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{bins} \PYG{o}{=} \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mf}{18.5}\PYG{p}{,} \PYG{l+m+mf}{24.9}\PYG{p}{,} \PYG{l+m+mf}{29.9}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{inf}\PYG{p}{]}
\PYG{n}{names} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{UnderWeught}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{NormalWeight}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Overweight}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Obese}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}
\PYG{n}{dim\PYGZus{}reduced\PYGZus{}2d}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{BMI\PYGZus{}Bin}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{cut}\PYG{p}{(}\PYG{n}{original\PYGZus{}heart\PYGZus{}disease}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{BMI}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{bins}\PYG{p}{,} \PYG{n}{labels}\PYG{o}{=}\PYG{n}{names}\PYG{p}{)}
\PYG{n}{dim\PYGZus{}reduced\PYGZus{}2d}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{AlcoholDrinking}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{original\PYGZus{}heart\PYGZus{}disease}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{AlcoholDrinking}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\PYG{n}{dim\PYGZus{}reduced\PYGZus{}2d}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{BMI\PYGZus{}Bin}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{original\PYGZus{}heart\PYGZus{}disease}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Smoking}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
For both datasets, we reject \(H_{0}\) and can conclude that the datasets have a significant tendency to cluster.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{2D}\PYG{l+s+s2}{\PYGZsq{}}\PYG{l+s+s2}{s hopkins statistic }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{hopkins}\PYG{p}{(}\PYG{n}{dim\PYGZus{}reduced\PYGZus{}2d}\PYG{o}{.}\PYG{n}{iloc}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{:}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gt}{\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}}
\PYG{n+ne}{ValueError}\PYG{g+gWhitespace}{                                }Traceback (most recent call last)
\PYG{o}{/}\PYG{n}{var}\PYG{o}{/}\PYG{n}{folders}\PYG{o}{/}\PYG{l+m+mi}{0}\PYG{n}{f}\PYG{o}{/}\PYG{n}{wgt07yjn5gnfhmk719tfc2gm1vjz80}\PYG{o}{/}\PYG{n}{T}\PYG{o}{/}\PYG{n}{ipykernel\PYGZus{}81731}\PYG{o}{/}\PYG{l+m+mf}{103584931.}\PYG{n}{py} \PYG{o+ow}{in} \PYG{o}{\PYGZlt{}}\PYG{n}{module}\PYG{o}{\PYGZgt{}}
\PYG{n+ne}{\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZgt{} }\PYG{l+m+mi}{1} \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{2D}\PYG{l+s+s2}{\PYGZsq{}}\PYG{l+s+s2}{s hopkins statistic }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{hopkins}\PYG{p}{(}\PYG{n}{dim\PYGZus{}reduced\PYGZus{}2d}\PYG{o}{.}\PYG{n}{iloc}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{:}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{n+nn}{/var/folders/0f/wgt07yjn5gnfhmk719tfc2gm1vjz80/T/ipykernel\PYGZus{}81731/1089192470.py} in \PYG{n+ni}{hopkins}\PYG{n+nt}{(X)}
\PYG{g+gWhitespace}{      }\PYG{l+m+mi}{7}     \PYG{n}{n} \PYG{o}{=} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{X}\PYG{p}{)}
\PYG{g+gWhitespace}{      }\PYG{l+m+mi}{8}     \PYG{n}{m} \PYG{o}{=} \PYG{n+nb}{int}\PYG{p}{(}\PYG{l+m+mf}{0.1} \PYG{o}{*} \PYG{n}{n}\PYG{p}{)}
\PYG{n+ne}{\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZgt{} }\PYG{l+m+mi}{9}     \PYG{n}{nbrs} \PYG{o}{=} \PYG{n}{NearestNeighbors}\PYG{p}{(}\PYG{n}{n\PYGZus{}neighbors}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{X}\PYG{o}{.}\PYG{n}{values}\PYG{p}{)}
\PYG{g+gWhitespace}{     }\PYG{l+m+mi}{10} 
\PYG{g+gWhitespace}{     }\PYG{l+m+mi}{11}     \PYG{n}{rand\PYGZus{}X} \PYG{o}{=} \PYG{n}{sample}\PYG{p}{(}\PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{n}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{,} \PYG{n}{m}\PYG{p}{)}

\PYG{n+nn}{\PYGZti{}/Library/CloudStorage/OneDrive\PYGZhy{}UHG/repos/learn\PYGZhy{}hub/Anthony/heart\PYGZus{}disease\PYGZus{}task/venv/lib/python3.7/site\PYGZhy{}packages/sklearn/neighbors/\PYGZus{}unsupervised.py} in \PYG{n+ni}{fit}\PYG{n+nt}{(self, X, y)}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{164}             \PYG{n}{The} \PYG{n}{fitted} \PYG{n}{nearest} \PYG{n}{neighbors} \PYG{n}{estimator}\PYG{o}{.}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{165}         \PYG{l+s+s2}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{n+ne}{\PYGZhy{}\PYGZhy{}\PYGZgt{} }\PYG{l+m+mi}{166}\PYG{l+s+s2}{         return self.\PYGZus{}fit(X)}

\PYG{n+nn}{\PYGZti{}/Library/CloudStorage/OneDrive\PYGZhy{}UHG/repos/learn\PYGZhy{}hub/Anthony/heart\PYGZus{}disease\PYGZus{}task/venv/lib/python3.7/site\PYGZhy{}packages/sklearn/neighbors/\PYGZus{}base.py} in \PYG{n+ni}{\PYGZus{}fit}\PYG{n+nt}{(self, X, y)}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{433}\PYG{l+s+s2}{         else:}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{434}\PYG{l+s+s2}{             if not isinstance(X, (KDTree, BallTree, NeighborsBase)):}
\PYG{n+ne}{\PYGZhy{}\PYGZhy{}\PYGZgt{} }\PYG{l+m+mi}{435}\PYG{l+s+s2}{                 X = self.\PYGZus{}validate\PYGZus{}data(X, accept\PYGZus{}sparse=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{csr}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{)}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{436}\PYG{l+s+s2}{ }
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{437}\PYG{l+s+s2}{         self.\PYGZus{}check\PYGZus{}algorithm\PYGZus{}metric()}

\PYG{n+nn}{\PYGZti{}/Library/CloudStorage/OneDrive\PYGZhy{}UHG/repos/learn\PYGZhy{}hub/Anthony/heart\PYGZus{}disease\PYGZus{}task/venv/lib/python3.7/site\PYGZhy{}packages/sklearn/base.py} in \PYG{n+ni}{\PYGZus{}validate\PYGZus{}data}\PYG{n+nt}{(self, X, y, reset, validate\PYGZus{}separately, **check\PYGZus{}params)}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{564}\PYG{l+s+s2}{             raise ValueError(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Validation should be done on X, y or both.}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{)}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{565}\PYG{l+s+s2}{         elif not no\PYGZus{}val\PYGZus{}X and no\PYGZus{}val\PYGZus{}y:}
\PYG{n+ne}{\PYGZhy{}\PYGZhy{}\PYGZgt{} }\PYG{l+m+mi}{566}\PYG{l+s+s2}{             X = check\PYGZus{}array(X, **check\PYGZus{}params)}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{567}\PYG{l+s+s2}{             out = X}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{568}\PYG{l+s+s2}{         elif no\PYGZus{}val\PYGZus{}X and not no\PYGZus{}val\PYGZus{}y:}

\PYG{n+nn}{\PYGZti{}/Library/CloudStorage/OneDrive\PYGZhy{}UHG/repos/learn\PYGZhy{}hub/Anthony/heart\PYGZus{}disease\PYGZus{}task/venv/lib/python3.7/site\PYGZhy{}packages/sklearn/utils/validation.py} in \PYG{n+ni}{check\PYGZus{}array}\PYG{n+nt}{(array, accept\PYGZus{}sparse, accept\PYGZus{}large\PYGZus{}sparse, dtype, order, copy, force\PYGZus{}all\PYGZus{}finite, ensure\PYGZus{}2d, allow\PYGZus{}nd, ensure\PYGZus{}min\PYGZus{}samples, ensure\PYGZus{}min\PYGZus{}features, estimator)}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{744}\PYG{l+s+s2}{                     array = array.astype(dtype, casting=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{unsafe}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{, copy=False)}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{745}\PYG{l+s+s2}{                 else:}
\PYG{n+ne}{\PYGZhy{}\PYGZhy{}\PYGZgt{} }\PYG{l+m+mi}{746}\PYG{l+s+s2}{                     array = np.asarray(array, order=order, dtype=dtype)}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{747}\PYG{l+s+s2}{             except ComplexWarning as complex\PYGZus{}warning:}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{748}\PYG{l+s+s2}{                 raise ValueError(}

\PYG{n+ne}{ValueError}: could not convert string to float: \PYGZsq{}Yes\PYGZsq{}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
From the result of implementing IVAT, it is observed that both datsets seem to be inconclusive. However, as this algorithm is just meant to help us decide if we should go ahead with the cluster analysis or not, we will go ahead with the K\sphinxhyphen{}means cluster analysis as both Hopkins statistic results were significant.


\section{K\sphinxhyphen{}Means}
\label{\detokenize{Clustering_Analysis:k-means}}
\sphinxAtStartPar
K\sphinxhyphen{}means is a common clustering algorithm. Although a simple clustering algorithm, it has vast application areas, including customer segmentation and image compression. K\sphinxhyphen{}means is a centroid based algorithm that aims to minimize the sum of distances between the points and their respective cluster centroid.  The main steps of this algorithm are:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Step 1}: Choose the number (k) of clusters

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Step 2}: Select k random points, which will become the initial centroids

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Step 3}: Assign all data points to the nearest centroid.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Step 4}: Compute the centroid of the newly formed clusters by taking the
mean of data instances currently associated with that cluster.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Step 5}: Repeat steps 3 and 4 until either:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Centroids of newly formed clusters do not change

\item {} 
\sphinxAtStartPar
Points remain in the same cluster

\item {} 
\sphinxAtStartPar
Maximum number of iterations are reached

\end{itemize}

\end{itemize}

\sphinxAtStartPar
We utlise the \sphinxcode{\sphinxupquote{MiniBatchKMeans()}} from the sckit\sphinxhyphen{}learn package, given the largness of this dataset.

\sphinxAtStartPar
But how do we find the optimal number of clusters?
\begin{itemize}
\item {} 
\sphinxAtStartPar
Elbow method

\item {} 
\sphinxAtStartPar
Silhouette coefficient

\end{itemize}


\subsection{Elbow method}
\label{\detokenize{Clustering_Analysis:elbow-method}}
\sphinxAtStartPar
The Elbow method calculates the error or ‘distortion’ between the data points (\(y_{i}\)) and their corresponding centroid (\(ŷ_{i}\)) of N data points for k clusters where k ⋹ \{1…10\}. The error metric used is the Sum of Squared Error (SSE):
\begin{equation*}
\begin{split}
SSE = \sum_{i=1}^{N} {(y_i - ŷ_i)^2}
\end{split}
\end{equation*}
\sphinxAtStartPar
We plot these values in an attempt to find an ‘elbow’ within the curve.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{Sum\PYGZus{}of\PYGZus{}squared\PYGZus{}distances} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}
\PYG{k}{for} \PYG{n}{k} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{10}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{km\PYGZus{}2d} \PYG{o}{=} \PYG{n}{MiniBatchKMeans}\PYG{p}{(}\PYG{n}{n\PYGZus{}clusters}\PYG{o}{=}\PYG{n}{k}\PYG{p}{,} \PYG{n}{random\PYGZus{}state}\PYG{o}{=}\PYG{l+m+mi}{42}\PYG{p}{)}
    \PYG{n}{km\PYGZus{}2d} \PYG{o}{=} \PYG{n}{km\PYGZus{}2d}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{dim\PYGZus{}reduced\PYGZus{}2d}\PYG{o}{.}\PYG{n}{iloc}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{:}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}
    \PYG{n}{Sum\PYGZus{}of\PYGZus{}squared\PYGZus{}distances}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{km\PYGZus{}2d}\PYG{o}{.}\PYG{n}{inertia\PYGZus{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{10}\PYG{p}{)}\PYG{p}{,} \PYG{n}{Sum\PYGZus{}of\PYGZus{}squared\PYGZus{}distances}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{bx\PYGZhy{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{No of Clusters}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Sum\PYGZus{}of\PYGZus{}squared\PYGZus{}distances}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Elbow Method For Optimal k for 2D dataset}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{Clustering_Analysis_16_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
We can see that the optimal number of clusters occur at k=2. A more suitable dip is noted at k\sphinxhyphen{}4.


\section{Sillhoute method}
\label{\detokenize{Clustering_Analysis:sillhoute-method}}
\sphinxAtStartPar
This method is another method of finding the correct number of clusters(k). Silhouette coefficient for a particular data point (\(i\)) is defined as:
\begin{equation*}
\begin{split}
s_{i} = \frac{b_{i} - a_{i}}{max(b_{i}, a_{i})}
\end{split}
\end{equation*}
\sphinxAtStartPar
where:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\(s_{i}\): the silhouette coefficient, ranging from \sphinxhyphen{}1 to 1. A score of 1 (the best) means that data point \(i\) is compact in its cluster and far away from other clusters. Conversely, the worst value is \sphinxhyphen{}1, while values near 0 denote overlapping clusters.

\item {} 
\sphinxAtStartPar
\(b_{i}\): average distance between \(i\) and all the other data points in its cluster.

\item {} 
\sphinxAtStartPar
\(a_{i}\): minimum average distance from \(i\) to all clusters to which \(i\) does not belong to

\end{itemize}

\sphinxAtStartPar
We evaluate using silhouette plots. These plots display how close each point in one cluster is to points in the neighbouring clusters.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{yellowbrick}\PYG{n+nn}{.}\PYG{n+nn}{cluster} \PYG{k+kn}{import} \PYG{n}{SilhouetteVisualizer}
\PYG{k}{for} \PYG{n}{k} \PYG{o+ow}{in}  \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{5}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{km} \PYG{o}{=} \PYG{n}{MiniBatchKMeans}\PYG{p}{(}\PYG{n}{n\PYGZus{}clusters}\PYG{o}{=}\PYG{n}{k}\PYG{p}{,} \PYG{n}{random\PYGZus{}state}\PYG{o}{=}\PYG{l+m+mi}{42}\PYG{p}{)}
    \PYG{n}{visualizer} \PYG{o}{=} \PYG{n}{SilhouetteVisualizer}\PYG{p}{(}\PYG{n}{km}\PYG{p}{,} \PYG{n}{colors}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{yellowbrick}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
    \PYG{n}{visualizer}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{dim\PYGZus{}reduced\PYGZus{}2d}\PYG{o}{.}\PYG{n}{iloc}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{:}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}
    \PYG{n}{visualizer}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{Clustering_Analysis_20_0}.png}

\noindent\sphinxincludegraphics{{Clustering_Analysis_20_1}.png}

\noindent\sphinxincludegraphics{{Clustering_Analysis_20_2}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
The results of the silhoute analysis are more ambiguous.  K=2 seem to be sub\sphinxhyphen{}optimal due to wide fluctuations in size of the silhouette plot.  However, the fluctuation at k=4 seems to be more uniform compared to 2. Thus, we select the optimal number of clusters as 4.


\section{Findings}
\label{\detokenize{Clustering_Analysis:findings}}
\sphinxAtStartPar
As mentioned previously, clusters can be considered as disjoint groups. In this context, these clusters seek to represent people with similar latent physiological processes and/or possible health statuses. We attempt to relate the groupings to the health disease status of individual pateints.


\subsection{Top 2 principal component dataset}
\label{\detokenize{Clustering_Analysis:top-2-principal-component-dataset}}
\sphinxAtStartPar
Healthy vs Unhealthy

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{km\PYGZus{}2d} \PYG{o}{=} \PYG{n}{KMeans}\PYG{p}{(}\PYG{n}{n\PYGZus{}clusters}\PYG{o}{=}\PYG{l+m+mi}{4}\PYG{p}{,} \PYG{n}{random\PYGZus{}state}\PYG{o}{=}\PYG{l+m+mi}{42}\PYG{p}{)}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{dim\PYGZus{}reduced\PYGZus{}2d}\PYG{o}{.}\PYG{n}{iloc}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{:}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{dim\PYGZus{}reduced\PYGZus{}2d}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{clusters}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{km\PYGZus{}2d}\PYG{o}{.}\PYG{n}{labels\PYGZus{}} \PYG{o}{+} \PYG{l+m+mi}{1}
\PYG{n}{f}\PYG{p}{,} \PYG{p}{(}\PYG{n}{ax1}\PYG{p}{,} \PYG{n}{ax2}\PYG{p}{)} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{sharey}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{scatterplot}\PYG{p}{(}\PYG{n}{x}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{component\PYGZus{}1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{y}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{component\PYGZus{}2}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{data}\PYG{o}{=}\PYG{n}{dim\PYGZus{}reduced\PYGZus{}2d}\PYG{p}{,} \PYG{n}{hue}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{clusters}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{ax}\PYG{o}{=}\PYG{n}{ax1}\PYG{p}{)}
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{scatterplot}\PYG{p}{(}\PYG{n}{x}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{component\PYGZus{}1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{y}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{component\PYGZus{}2}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{data}\PYG{o}{=}\PYG{n}{dim\PYGZus{}reduced\PYGZus{}2d}\PYG{p}{,} \PYG{n}{hue}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{HeartDisease}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{ax}\PYG{o}{=}\PYG{n}{ax2}\PYG{p}{)}
\PYG{n}{results\PYGZus{}df} \PYG{o}{=} \PYG{n}{dim\PYGZus{}reduced\PYGZus{}2d}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{HeartDisease}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{clusters}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}index}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{HeartDisease}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{results\PYGZus{}df} \PYG{o}{=} \PYG{n}{results\PYGZus{}df}\PYG{o}{.}\PYG{n}{apply}\PYG{p}{(}\PYG{n}{pd}\PYG{o}{.}\PYG{n}{Series}\PYG{o}{.}\PYG{n}{value\PYGZus{}counts}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{.}\PYG{n}{groupby}\PYG{p}{(}\PYG{n}{by}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{HeartDisease}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{)}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{results\PYGZus{}df}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
                     1        2        3         4
HeartDisease                                      
0             116237.0  37359.0  22403.0  116423.0
1              16315.0   1832.0   7538.0    1688.0
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Clustering_Analysis_23_1}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
The majority of people with heart disease fall in cluster 1. Although a significant proportion also fall in cluster 2. This lines with our hypothesis that people who suffer from herat disease exhibit similar risk factors.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{scatterplot}\PYG{p}{(}\PYG{n}{x}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{component\PYGZus{}1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{y}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{component\PYGZus{}2}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{data}\PYG{o}{=}\PYG{n}{dim\PYGZus{}reduced\PYGZus{}2d}\PYG{p}{,} \PYG{n}{hue}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{BMI\PYGZus{}Bin}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{results\PYGZus{}df} \PYG{o}{=} \PYG{n}{dim\PYGZus{}reduced\PYGZus{}2d}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{BMI\PYGZus{}Bin}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{clusters}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}index}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{BMI\PYGZus{}Bin}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{results\PYGZus{}df} \PYG{o}{=} \PYG{n}{results\PYGZus{}df}\PYG{o}{.}\PYG{n}{apply}\PYG{p}{(}\PYG{n}{pd}\PYG{o}{.}\PYG{n}{Series}\PYG{o}{.}\PYG{n}{value\PYGZus{}counts}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{.}\PYG{n}{groupby}\PYG{p}{(}\PYG{n}{by}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{BMI\PYGZus{}Bin}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{)}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{results\PYGZus{}df}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
                    1        2        3        4
BMI\PYGZus{}Bin                                         
UnderWeught    1805.0    816.0    803.0   1690.0
NormalWeight  36996.0  10932.0   6614.0  40592.0
Overweight    50829.0  11545.0   8885.0  43493.0
Obese         42922.0  15898.0  13639.0  32336.0
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Clustering_Analysis_25_1}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Besides the majority of overweight and obese patients falling in clusters 1 and 4, participants in each BMI group seem to be equally distributed among the clusters. If patients  could be accurately clustered by study, this would suggest that BMI would be a strong indicator of heart disease. However, this is not the case. It is possible that the clustering is not accurate enough to make such a conclusion, or that BMI has a mild influence on heart disease outcome.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{visualise\PYGZus{}cluster\PYGZus{}heat\PYGZus{}map}\PYG{p}{(}\PYG{n}{focus\PYGZus{}feature}\PYG{p}{,} \PYG{n}{k}\PYG{p}{,} \PYG{n}{df}\PYG{p}{,} \PYG{n}{cmap}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    Visualizes the clusters as heat maps , where the squares represents }
\PYG{l+s+sd}{        the number of particpants in k clusters grouped by the focus feature }
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    :param focus\PYGZus{}feature: focus\PYGZus{}feature respresents the feature we want to}
\PYG{l+s+sd}{        drill down by i.e. health status or study}
\PYG{l+s+sd}{    :param k: Number of desired clusters}
\PYG{l+s+sd}{    :param df: Dataframe containing study, health status and the top 3 principal}
\PYG{l+s+sd}{        components}
\PYG{l+s+sd}{    :param cmap: matplotlib colormap name or object, or list of colors, optional}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    

    \PYG{n}{km\PYGZus{}3d} \PYG{o}{=} \PYG{n}{KMeans}\PYG{p}{(}\PYG{n}{n\PYGZus{}clusters}\PYG{o}{=}\PYG{n}{k}\PYG{p}{,} \PYG{n}{random\PYGZus{}state}\PYG{o}{=}\PYG{l+m+mi}{42}\PYG{p}{)}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{dim\PYGZus{}reduced\PYGZus{}2d}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{component\PYGZus{}1}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{component\PYGZus{}2}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}
    \PYG{n}{df}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{clusters}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{km\PYGZus{}3d}\PYG{o}{.}\PYG{n}{labels\PYGZus{}} \PYG{o}{+} \PYG{l+m+mi}{1}
    \PYG{n}{df} \PYG{o}{=} \PYG{n}{df}\PYG{p}{[}\PYG{p}{[}\PYG{n}{focus\PYGZus{}feature}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{clusters}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}index}\PYG{p}{(}\PYG{n}{focus\PYGZus{}feature}\PYG{p}{)}
    \PYG{n}{df} \PYG{o}{=} \PYG{n}{df}\PYG{o}{.}\PYG{n}{apply}\PYG{p}{(}\PYG{n}{pd}\PYG{o}{.}\PYG{n}{Series}\PYG{o}{.}\PYG{n}{value\PYGZus{}counts}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{.}\PYG{n}{groupby}\PYG{p}{(}\PYG{n}{by}\PYG{o}{=}\PYG{p}{[}\PYG{n}{focus\PYGZus{}feature}\PYG{p}{]}\PYG{p}{)}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{p}{)}
    \PYG{n}{akws} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{ha}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{left}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{va}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{top}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{\PYGZcb{}}
    \PYG{n}{ax} \PYG{o}{=} \PYG{n}{sns}\PYG{o}{.}\PYG{n}{heatmap}\PYG{p}{(}\PYG{n}{df}\PYG{p}{,} \PYG{n}{annot}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,} \PYG{n}{cmap}\PYG{o}{=}\PYG{n}{cmap}\PYG{p}{,} \PYG{n}{fmt}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{g}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{annot\PYGZus{}kws}\PYG{o}{=}\PYG{n}{akws}\PYG{p}{)}

    \PYG{k}{for} \PYG{n}{t} \PYG{o+ow}{in} \PYG{n}{ax}\PYG{o}{.}\PYG{n}{texts}\PYG{p}{:}
        \PYG{n}{trans} \PYG{o}{=} \PYG{n}{t}\PYG{o}{.}\PYG{n}{get\PYGZus{}transform}\PYG{p}{(}\PYG{p}{)}
        \PYG{n}{offs} \PYG{o}{=} \PYG{n}{matplotlib}\PYG{o}{.}\PYG{n}{transforms}\PYG{o}{.}\PYG{n}{ScaledTranslation}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.34}\PYG{p}{,} \PYG{l+m+mf}{0.34}\PYG{p}{,}
                        \PYG{n}{matplotlib}\PYG{o}{.}\PYG{n}{transforms}\PYG{o}{.}\PYG{n}{IdentityTransform}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
        \PYG{n}{t}\PYG{o}{.}\PYG{n}{set\PYGZus{}transform}\PYG{p}{(} \PYG{n}{offs} \PYG{o}{+} \PYG{n}{trans} \PYG{p}{)}
    
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{clusters}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{visualise\PYGZus{}cluster\PYGZus{}heat\PYGZus{}map}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{AlcoholDrinking}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+m+mi}{4}\PYG{p}{,} \PYG{n}{dim\PYGZus{}reduced\PYGZus{}2d}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Blues}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{Clustering_Analysis_28_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
The healthy participants seem to be evenly spread among the two clusters. Both types of unhealthy participants have a tendency to appear in cluster 1. K\sphinxhyphen{}means seems to be reasonable able to cluster unhealthy patients, with 83\% of unhealthy patients being clustered in the same group. However, k\sphinxhyphen{}means is unable to model the distinction between healthy and unhealthy individuals.

\sphinxAtStartPar
Ahn, Wood and Worthy seem to exhibit a clear preference for one cluster over another. Otherwise, participants in the other studies are reasonably evenly spread across the two clusters.

\sphinxAtStartPar
Healthy participants appear not to have a tendency to group in any cluster. For example, amphetamines users tend to group in clusters 2 and 3 and 4 whilst heroin users tend to be located in clusters 2 ,3 and 5. Interestingly, even with a more significant number of cluster’s, k\sphinxhyphen{}means groups the majority of unhealthy individuals in the same cluster (ie.e no distinction between opioid and stimulant dependents).

\sphinxAtStartPar
Finally, we decide to append the clusters to our standardized dataset. Our feature selection method later on, will tell us if they were of use or not.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{standradised\PYGZus{}dataset} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{data/standardised\PYGZus{}heart\PYGZus{}disease.csv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{standradised\PYGZus{}dataset}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{clusters}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]} \PYG{o}{=} \PYG{p}{(}\PYG{n}{dim\PYGZus{}reduced\PYGZus{}2d}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{clusters}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{n}{dim\PYGZus{}reduced\PYGZus{}2d}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{clusters}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}\PYG{o}{/}\PYG{n}{dim\PYGZus{}reduced\PYGZus{}2d}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{clusters}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{std}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{standradised\PYGZus{}dataset}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{clusters}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{to\PYGZus{}csv}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{data/standardised\PYGZus{}heart\PYGZus{}disease.csv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{index}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
/opt/anaconda3/lib/python3.7/site\PYGZhy{}packages/ipykernel\PYGZus{}launcher.py:3: FutureWarning: The signature of `Series.to\PYGZus{}csv` was aligned to that of `DataFrame.to\PYGZus{}csv`, and argument \PYGZsq{}header\PYGZsq{} will change its default value from False to True: please pass an explicit value to suppress this warning.
  This is separate from the ipykernel package so we can avoid doing imports until
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxstepscope


\chapter{Model Building, Evaluation \& Sensitivity Analysis}
\label{\detokenize{Model_evaluation:model-building-evaluation-sensitivity-analysis}}\label{\detokenize{Model_evaluation::doc}}
\sphinxAtStartPar
In this section, we will present a comparative analysis of the heart disease classification problem using different classification algorithms. We use the 80:20 train\sphinxhyphen{}test split rule to evaluate our models.
\sphinxstylestrong{Note}, a small amount of patients were used as  as hold\sphinxhyphen{}out set for hyper\sphinxhyphen{}parameter sensitivity analysis.


\section{Models}
\label{\detokenize{Model_evaluation:models}}
\sphinxAtStartPar
We choose numerous shallow predictive methods to predict heart disease.
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxhref{https://en.wikipedia.org/wiki/Logistic\_regression}{Logistic Regression}

\item {} 
\sphinxAtStartPar
\sphinxhref{https://en.wikipedia.org/wiki/Gradient\_boosting}{Gradient Boost Classifier}

\item {} 
\sphinxAtStartPar
\sphinxhref{https://en.wikipedia.org/wiki/Decision\_tree\_learning}{Decision Tree Classifier}

\item {} 
\sphinxAtStartPar
\sphinxhref{https://en.wikipedia.org/wiki/Random\_forest}{Random Forest Classifier}

\item {} 
\sphinxAtStartPar
\sphinxhref{https://en.wikipedia.org/wiki/Naive\_Bayes\_classifier}{Naive Bayes}

\end{itemize}

\sphinxAtStartPar
For more information on these algorithms, please click on the relevant links


\section{Evaluation metrics}
\label{\detokenize{Model_evaluation:evaluation-metrics}}
\sphinxAtStartPar
One of the key requirements in developing any algorithm is to measure it’s effectiveness. Accuracy is the most simple measure. It tells us the he number of correctly classified examples over the total number of examples. More formally,
\$\(
 Accuracy = \frac{TruePositive + TrueNegative}{TruePositive + TrueNegative + FalsePositive + FalseNegative  }
\)\$
But is accuracy telling the whole picture ?
Well, let’s consider those two examples:
\begin{itemize}
\item {} 
\sphinxAtStartPar
A classifier which, if a person has the heart disease, will always correctly diagnose it, but gets half of the healthy people wrong. You can see that announcing to a healthy person that he or she has the disease could lead to adverse consequences.

\item {} 
\sphinxAtStartPar
A classifier that gets the diagnose right for every healthy person, but also miss half of the disease cases. That wouldn’t be a very good algorithm would it?

\end{itemize}

\sphinxAtStartPar
Depending on the distribution of sick to healthy patients those two classifiers could have high accuracy while not being considered very good. Therefore, we decide to employ three further metrics
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleemphasis{Precision:} dertimnes what proportion of the negative class got correctly classified.
\$\(
\frac{TruePositive}{TruePositive +   FalsePositive    }
\)\$

\item {} 
\sphinxAtStartPar
\sphinxstyleemphasis{Recall}: determine what proportion of the actual sick people were correctly detected by the model.
\$\(
\frac{TruePositive}{TruePositive +    FalseNegative   }
\)\$

\end{itemize}


\section{Import libaries}
\label{\detokenize{Model_evaluation:import-libaries}}

\subsection{Data Processing}
\label{\detokenize{Model_evaluation:data-processing}}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{pandas}  \PYG{k}{as} \PYG{n+nn}{pd}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{model\PYGZus{}selection} \PYG{k+kn}{import} \PYG{n}{train\PYGZus{}test\PYGZus{}split}
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{decomposition} \PYG{k+kn}{import} \PYG{n}{PCA}
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{preprocessing} \PYG{k+kn}{import} \PYG{n}{StandardScaler}
\PYG{k+kn}{import} \PYG{n+nn}{warnings}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{feature\PYGZus{}selection} \PYG{k+kn}{import} \PYG{n}{mutual\PYGZus{}info\PYGZus{}regression}\PYG{p}{,} \PYG{n}{mutual\PYGZus{}info\PYGZus{}classif}
\PYG{n}{warnings}\PYG{o}{.}\PYG{n}{filterwarnings}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ignore}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}

\subsection{Model building and evaluation}
\label{\detokenize{Model_evaluation:model-building-and-evaluation}}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{linear\PYGZus{}model} \PYG{k+kn}{import} \PYG{n}{LogisticRegression}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{neighbors} \PYG{k+kn}{import} \PYG{n}{KNeighborsClassifier}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{tree} \PYG{k+kn}{import} \PYG{n}{DecisionTreeClassifier}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{ensemble} \PYG{k+kn}{import} \PYG{n}{RandomForestClassifier}\PYG{p}{,} \PYG{n}{GradientBoostingClassifier}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{neighbors} \PYG{k+kn}{import} \PYG{n}{KNeighborsClassifier}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{svm} \PYG{k+kn}{import} \PYG{n}{SVC}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{metrics} \PYG{k+kn}{import} \PYG{n}{plot\PYGZus{}confusion\PYGZus{}matrix}

\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{metrics} \PYG{k+kn}{import} \PYG{n}{confusion\PYGZus{}matrix}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{metrics} \PYG{k+kn}{import} \PYG{n}{ConfusionMatrixDisplay}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{feature\PYGZus{}selection} \PYG{k+kn}{import} \PYG{n}{mutual\PYGZus{}info\PYGZus{}regression}\PYG{p}{,} \PYG{n}{mutual\PYGZus{}info\PYGZus{}classif}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{naive\PYGZus{}bayes} \PYG{k+kn}{import} \PYG{n}{GaussianNB}
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{metrics} \PYG{k+kn}{import} \PYG{n}{classification\PYGZus{}report}\PYG{p}{,} \PYG{n}{roc\PYGZus{}curve}\PYG{p}{,} \PYG{n}{auc}\PYG{p}{,} \PYG{n}{roc\PYGZus{}auc\PYGZus{}score}


\PYG{k+kn}{from} \PYG{n+nn}{imblearn}\PYG{n+nn}{.}\PYG{n+nn}{under\PYGZus{}sampling} \PYG{k+kn}{import} \PYG{p}{(}
    \PYG{n}{RandomUnderSampler}\PYG{p}{,}
    \PYG{n}{CondensedNearestNeighbour}\PYG{p}{,}
    \PYG{n}{TomekLinks}\PYG{p}{,}
    \PYG{n}{OneSidedSelection}\PYG{p}{,}
    \PYG{n}{EditedNearestNeighbours}\PYG{p}{,}
    \PYG{n}{RepeatedEditedNearestNeighbours}\PYG{p}{,}
    \PYG{n}{AllKNN}\PYG{p}{,}
    \PYG{n}{NeighbourhoodCleaningRule}\PYG{p}{,}
    \PYG{n}{NearMiss}
\PYG{p}{)}
\PYG{k+kn}{from} \PYG{n+nn}{imblearn}\PYG{n+nn}{.}\PYG{n+nn}{over\PYGZus{}sampling} \PYG{k+kn}{import} \PYG{p}{(}
    \PYG{n}{RandomOverSampler}\PYG{p}{,}
    \PYG{n}{SMOTE}\PYG{p}{,}
    \PYG{n}{ADASYN}\PYG{p}{,}
    \PYG{n}{BorderlineSMOTE}\PYG{p}{,}
    \PYG{n}{SVMSMOTE}\PYG{p}{,}
\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}

\subsection{Model Serialisation}
\label{\detokenize{Model_evaluation:model-serialisation}}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{pickle}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}

\subsection{Data Visualisation}
\label{\detokenize{Model_evaluation:data-visualisation}}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{dict\PYGZus{}classifiers} \PYG{o}{=} \PYG{p}{\PYGZob{}}
    \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Logistic Regression }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{n}{LogisticRegression}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,}
    \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Gradient Boost Classifier}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{n}{GradientBoostingClassifier}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,}
    \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Decision Tree Classifier}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{n}{DecisionTreeClassifier}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,}
    \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Random Forest Classifier}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{n}{RandomForestClassifier}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,}
    \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Naive Bayes}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{n}{GaussianNB}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,}
\PYG{p}{\PYGZcb{}}

\PYG{n}{heart\PYGZus{}disease\PYGZus{}dataset\PYGZus{}standardized} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{data/standardised\PYGZus{}heart\PYGZus{}disease.csv}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Initial Attempt}
\PYG{n}{train\PYGZus{}heart\PYGZus{}disease\PYGZus{}df} \PYG{o}{=} \PYG{n}{heart\PYGZus{}disease\PYGZus{}dataset\PYGZus{}standardized}\PYG{o}{.}\PYG{n}{drop}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{HeartDisease}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{axis}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{columns}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{test\PYGZus{}heart\PYGZus{}diease\PYGZus{}df} \PYG{o}{=} \PYG{n}{heart\PYGZus{}disease\PYGZus{}dataset\PYGZus{}standardized}\PYG{o}{.}\PYG{n}{HeartDisease}\PYG{o}{.}\PYG{n}{astype}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{float32}\PYG{p}{)}
\PYG{n}{X\PYGZus{}train}\PYG{p}{,} \PYG{n}{X\PYGZus{}test}\PYG{p}{,} \PYG{n}{y\PYGZus{}train}\PYG{p}{,} \PYG{n}{y\PYGZus{}test} \PYG{o}{=} \PYG{n}{train\PYGZus{}test\PYGZus{}split}\PYG{p}{(}\PYG{n}{train\PYGZus{}heart\PYGZus{}disease\PYGZus{}df}\PYG{p}{,} \PYG{n}{test\PYGZus{}heart\PYGZus{}diease\PYGZus{}df}\PYG{p}{,} \PYG{n}{test\PYGZus{}size}\PYG{o}{=}\PYG{l+m+mf}{0.2}\PYG{p}{,} \PYG{n}{random\PYGZus{}state}\PYG{o}{=}\PYG{l+m+mi}{15}\PYG{p}{,} \PYG{n}{stratify}\PYG{o}{=}\PYG{n}{test\PYGZus{}heart\PYGZus{}diease\PYGZus{}df}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Ratio of classes in training set:}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{y\PYGZus{}train}\PYG{o}{.}\PYG{n}{value\PYGZus{}counts}\PYG{p}{(}\PYG{n}{normalize}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Ratio of classes in training set:}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{y\PYGZus{}test}\PYG{o}{.}\PYG{n}{value\PYGZus{}counts}\PYG{p}{(}\PYG{n}{normalize}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
      
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
Ratio of classes in training set:
0.0    0.914406
1.0    0.085594
Name: HeartDisease, dtype: float64

Ratio of classes in training set:
0.0    0.914398
1.0    0.085602
Name: HeartDisease, dtype: float64
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{run\PYGZus{}exps}\PYG{p}{(}
             \PYG{n}{X\PYGZus{}train}\PYG{p}{,} \PYG{n}{X\PYGZus{}test}\PYG{p}{,} \PYG{n}{y\PYGZus{}train}\PYG{p}{,} \PYG{n}{y\PYGZus{}test}\PYG{p}{)} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZgt{}} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZsq{}\PYGZsq{}\PYGZsq{}}
\PYG{l+s+sd}{    Lightweight script to test many models and find winners}
\PYG{l+s+sd}{    :param X\PYGZus{}train: training split}
\PYG{l+s+sd}{    :param y\PYGZus{}train: training target vector}
\PYG{l+s+sd}{    :param X\PYGZus{}test: test split}
\PYG{l+s+sd}{    :param y\PYGZus{}test: test target vector}
\PYG{l+s+sd}{    :return: None}
\PYG{l+s+sd}{    \PYGZsq{}\PYGZsq{}\PYGZsq{}}
    
    \PYG{n}{results} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{(}\PYG{p}{)}
    \PYG{k}{for} \PYG{n}{model\PYGZus{}name}\PYG{p}{,} \PYG{n}{model} \PYG{o+ow}{in} \PYG{n}{dict\PYGZus{}classifiers}\PYG{o}{.}\PYG{n}{items}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{model}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{X\PYGZus{}train}\PYG{p}{,} \PYG{n}{y\PYGZus{}train}\PYG{p}{)}
        \PYG{n}{y\PYGZus{}pred} \PYG{o}{=} \PYG{n}{model}\PYG{o}{.}\PYG{n}{predict}\PYG{p}{(}\PYG{n}{X\PYGZus{}test}\PYG{p}{)}
        \PYG{n}{class\PYGZus{}report} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{(}\PYG{n}{classification\PYGZus{}report}\PYG{p}{(}\PYG{n}{y\PYGZus{}test}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred}\PYG{p}{,} \PYG{n}{target\PYGZus{}names}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{No Heart Disease}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Heart Disease}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,}\PYG{n}{output\PYGZus{}dict}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}\PYG{p}{)}\PYG{o}{.}\PYG{n}{transpose}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{reset\PYGZus{}index}\PYG{p}{(}\PYG{p}{)}
        \PYG{n}{class\PYGZus{}report}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{model}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{p}{[}\PYG{n}{model\PYGZus{}name}\PYG{p}{]} \PYG{o}{*} \PYG{n}{class\PYGZus{}report}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}
        \PYG{n}{class\PYGZus{}report}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{auc}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{p}{[}\PYG{n}{roc\PYGZus{}auc\PYGZus{}score}\PYG{p}{(}\PYG{n}{y\PYGZus{}test}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred}\PYG{p}{)}\PYG{p}{]} \PYG{o}{*} \PYG{n}{class\PYGZus{}report}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}
        \PYG{n}{class\PYGZus{}report} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{concat}\PYG{p}{(}\PYG{p}{[}\PYG{n}{class\PYGZus{}report}\PYG{p}{]}\PYG{p}{,} \PYG{n}{keys}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Model}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{names}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Firstlevel}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
        \PYG{n}{results} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{concat}\PYG{p}{(}\PYG{p}{[}\PYG{n}{results}\PYG{p}{,} \PYG{n}{class\PYGZus{}report}\PYG{p}{]}\PYG{p}{,} \PYG{n}{ignore\PYGZus{}index}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
    \PYG{n}{results}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{dummy}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{k+kc}{None}

    \PYG{n}{fig}\PYG{p}{,} \PYG{n}{axes} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{n}{nrows}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{n}{ncols}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{15}\PYG{p}{,}\PYG{l+m+mi}{10}\PYG{p}{)}\PYG{p}{)}

    \PYG{k}{for} \PYG{n+nb+bp}{cls}\PYG{p}{,} \PYG{n}{ax} \PYG{o+ow}{in} \PYG{n+nb}{zip}\PYG{p}{(}\PYG{n}{dict\PYGZus{}classifiers}\PYG{o}{.}\PYG{n}{values}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{axes}\PYG{o}{.}\PYG{n}{flatten}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{plot\PYGZus{}confusion\PYGZus{}matrix}\PYG{p}{(}\PYG{n+nb+bp}{cls}\PYG{p}{,} 
                            \PYG{n}{X\PYGZus{}test}\PYG{p}{,} 
                            \PYG{n}{y\PYGZus{}test}\PYG{p}{,} 
                            \PYG{n}{ax}\PYG{o}{=}\PYG{n}{ax}\PYG{p}{,} 
                            \PYG{n}{cmap}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Blues}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
                            \PYG{n}{display\PYGZus{}labels}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{No Heart Disease}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Heart Disease}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
        \PYG{n}{ax}\PYG{o}{.}\PYG{n}{title}\PYG{o}{.}\PYG{n}{set\PYGZus{}text}\PYG{p}{(}\PYG{n+nb}{type}\PYG{p}{(}\PYG{n+nb+bp}{cls}\PYG{p}{)}\PYG{o}{.}\PYG{n+nv+vm}{\PYGZus{}\PYGZus{}name\PYGZus{}\PYGZus{}}\PYG{p}{)}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{tight\PYGZus{}layout}\PYG{p}{(}\PYG{p}{)}  
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}

    \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}\PYG{l+s+s2}{Algorithm with the highest accuracy: }\PYG{l+s+si}{\PYGZob{}}
        \PYG{n}{results}\PYG{p}{[}\PYG{n}{results}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{index}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{==} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{accuracy}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{sort\PYGZus{}values}\PYG{p}{(}
            \PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{support}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{ascending}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{model}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{support}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}\PYG{o}{.}\PYG{n}{values}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{tolist}\PYG{p}{(}\PYG{p}{)}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}\PYG{p}{)}
    
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}\PYG{l+s+s2}{Algorithm with the highest macro recall:}
\PYG{l+s+s2}{        }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{results}\PYG{p}{[}\PYG{n}{results}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{index}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{==} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{macro avg}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{sort\PYGZus{}values}\PYG{p}{(}
            \PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{recall}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{ascending}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{model}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{recall}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}\PYG{o}{.}\PYG{n}{values}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{tolist}\PYG{p}{(}\PYG{p}{)}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}\PYG{p}{)}
    
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}\PYG{l+s+s2}{Algorithm with the highest macro precision:}
\PYG{l+s+s2}{        }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{results}\PYG{p}{[}\PYG{n}{results}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{index}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{==} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{macro avg}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{sort\PYGZus{}values}\PYG{p}{(}
            \PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{precision}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{ascending}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{model}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{precision}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}\PYG{o}{.}\PYG{n}{values}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{tolist}\PYG{p}{(}\PYG{p}{)}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}\PYG{p}{)}
    
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}\PYG{l+s+s2}{Algorithm with the highest AUC:}
\PYG{l+s+s2}{        }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{results}\PYG{o}{.}\PYG{n}{sort\PYGZus{}values}\PYG{p}{(}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{auc}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{ascending}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{[}
            \PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{model}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{auc}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}\PYG{o}{.}\PYG{n}{values}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{tolist}\PYG{p}{(}\PYG{p}{)}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}\PYG{p}{)}
    
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{results}\PYG{o}{.}\PYG{n}{groupby}\PYG{p}{(}
        \PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{model}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{index}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{precision}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{recall}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{f1\PYGZhy{}score}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{support}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}  \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{auc}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
        \PYG{p}{)}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{dummy}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{count}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
    
    \PYG{k}{return} \PYG{n}{results}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}

\section{Imbalanced data}
\label{\detokenize{Model_evaluation:imbalanced-data}}
\sphinxAtStartPar
As you can see above, our data is extremely imbalanced. Imbalanced datasets are those where there is a severe skew in the class distribution, such as 1:100 or 1:1000 examples in the minority class to the majority class.

\sphinxAtStartPar
This bias in the training dataset can influence many machine learning algorithms, leading some to ignore the minority class entirely. This is a problem as it is typically the minority class on which predictions are most important (i.e. predicting heart disease in our case).

\sphinxAtStartPar
One approach to addressing the problem of class imbalance is to randomly resample the training dataset. The two main approaches to randomly resampling an imbalanced dataset are to delete examples from the majority class, called undersampling, and to duplicate examples from the minority class, called oversampling.


\section{Undersampling}
\label{\detokenize{Model_evaluation:undersampling}}
\sphinxAtStartPar
The follwoing undersampling methods were choosen:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{RandomUnderSampler}}: Random undersampling consists in extracting at random samples from the majority class, until they reach a certain proportion compared to the minority class, typically 50:50.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{CondensedNearestNeighbour}}: The algorithms works as follows:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Put all minority class observations in a group, typically group O

\item {} 
\sphinxAtStartPar
Add 1 sample (at random) from the majority class to group O

\item {} 
\sphinxAtStartPar
Train a KNN with group O

\item {} 
\sphinxAtStartPar
Take a sample of the majority class that is not in group O yet

\item {} 
\sphinxAtStartPar
Predict its class with the KNN from point 3

\item {} 
\sphinxAtStartPar
If the prediction was correct, go to 4 and repeat

\item {} 
\sphinxAtStartPar
If the prediction was incorrect, add that sample to group O, go to 3 and repeat

\item {} 
\sphinxAtStartPar
Continue until all samples of the majority class were either assigned to O or left out

\item {} 
\sphinxAtStartPar
Final version of Group O is our undersampled dataset

\end{itemize}

\sphinxAtStartPar
This algorithm tends to pick points near the fuzzy boundary between the classes, and transfer those to the group O, in our example. If the classes are similar, group O will contain a fair amount of both classes. If the classes are very different, group O would contain mostly 1 class, the minority class.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{TomekLinks}}: Tomek links are 2 samples from a different class, which are nearest neighbours to each other. In other words, if 2 observations are nearest neighbours, and from a different class, they are Tomek Links. This procedures removes either the sample from the majority class if it is a Tomek Link, or alternatively, both observations, the one from the majority and the one from the minority class.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{OneSidedSelection}}: First finds the hardest instances to classify correctly from the majority class. Then removes noisy observations with Tomek Links.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{EditedNearestNeighbours}}: Train a KNN algorithm on the data (user defines number of neighbours, typically 3)
\begin{itemize}
\item {} 
\sphinxAtStartPar
Find the 3 nearest neighbour to each observation (or the number defined by the user in 1)

\item {} 
\sphinxAtStartPar
Find the label of each of the neighbours (we know it, is the target in the dataset)

\item {} 
\sphinxAtStartPar
if the majority of the neighbours show the same label as the observation, then we keep the observation

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{RepeatedEditedNearestNeighbours}}: Extends Edited Nearest neighbours in that it repeats the procedure over an over, until no further observation is removed from the dataset, or alternatively until a maximum number of iterations is reached.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{AllKNN}}: Adapts the functionality of Edited Nearest Neighbours in that, at each round, it increases the number of neighbours utilised to exclude or retain the observations.
It starts by looking at the 1 closest neighbour.
It finishes at a maximum number of neighbours to examine, determined by the user
it stops prematurely if the majority class becomes the minority

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{NeighbourhoodCleaningRule}}: The Neighbourhood Cleaning Rule works as follows:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Remove noisy observations from the majority class with ENN:

\end{enumerate}
\begin{itemize}
\item {} 
\sphinxAtStartPar
explores the 3 closest neighbours\textbackslash{}n

\item {} 
\sphinxAtStartPar
uses majority vote of neighbours to retain observations

\end{itemize}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Remove observations from the majority class if:,

\end{enumerate}
\begin{itemize}
\item {} 
\sphinxAtStartPar
they are 1 of the 3 closest neighbours to a minority sample, and,

\item {} 
\sphinxAtStartPar
most / all of those 3 closest neighbours are not minority, and,

\item {} 
\sphinxAtStartPar
the majority class has at least half as many observations as those in the minority (this can be regulated)

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{NearMiss}}: This procedures aims to select samples that are somewhat similar to the minority class, using 1 of three alternative procedures:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Select observations closer to the closest minority class

\item {} 
\sphinxAtStartPar
Select observations closer to the farthest minority class

\item {} 
\sphinxAtStartPar
Select observations furthest from their nearest neighbours

\end{itemize}

\end{itemize}

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
We train the models on a portion of the data that is under\sphinxhyphen{}sampled
We evaluate the model performance on another portion of the data that was not resampled, and thus contains the original class distribution.
\end{sphinxadmonition}

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{undersampler\PYGZus{}dict} \PYG{o}{=} \PYG{p}{\PYGZob{}}

    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{random}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{RandomUnderSampler}\PYG{p}{(}
        \PYG{n}{sampling\PYGZus{}strategy}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{auto}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
        \PYG{n}{random\PYGZus{}state}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,}
        \PYG{n}{replacement}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}\PYG{p}{,}

    \PYG{c+c1}{\PYGZsh{} \PYGZsq{}cnn\PYGZsq{}: CondensedNearestNeighbour(}
    \PYG{c+c1}{\PYGZsh{}     sampling\PYGZus{}strategy=\PYGZsq{}auto\PYGZsq{},}
    \PYG{c+c1}{\PYGZsh{}     random\PYGZus{}state=0,}
    \PYG{c+c1}{\PYGZsh{}     n\PYGZus{}neighbors=1,}
    \PYG{c+c1}{\PYGZsh{}     n\PYGZus{}jobs=4),}

    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{tomek}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{TomekLinks}\PYG{p}{(}
        \PYG{n}{sampling\PYGZus{}strategy}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{auto}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
        \PYG{n}{n\PYGZus{}jobs}\PYG{o}{=}\PYG{l+m+mi}{4}\PYG{p}{)}\PYG{p}{,}

    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{oss}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{OneSidedSelection}\PYG{p}{(}
        \PYG{n}{sampling\PYGZus{}strategy}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{auto}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
        \PYG{n}{random\PYGZus{}state}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,}
        \PYG{n}{n\PYGZus{}neighbors}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,}
        \PYG{n}{n\PYGZus{}jobs}\PYG{o}{=}\PYG{l+m+mi}{4}\PYG{p}{)}\PYG{p}{,}

    \PYG{c+c1}{\PYGZsh{} \PYGZsq{}enn\PYGZsq{}: EditedNearestNeighbours(}
    \PYG{c+c1}{\PYGZsh{}     sampling\PYGZus{}strategy=\PYGZsq{}auto\PYGZsq{},}
    \PYG{c+c1}{\PYGZsh{}     n\PYGZus{}neighbors=3,}
    \PYG{c+c1}{\PYGZsh{}     kind\PYGZus{}sel=\PYGZsq{}all\PYGZsq{},}
    \PYG{c+c1}{\PYGZsh{}     n\PYGZus{}jobs=4),}

    \PYG{c+c1}{\PYGZsh{} \PYGZsq{}renn\PYGZsq{}: RepeatedEditedNearestNeighbours(}
    \PYG{c+c1}{\PYGZsh{}     sampling\PYGZus{}strategy=\PYGZsq{}auto\PYGZsq{},}
    \PYG{c+c1}{\PYGZsh{}     n\PYGZus{}neighbors=3,}
    \PYG{c+c1}{\PYGZsh{}     kind\PYGZus{}sel=\PYGZsq{}all\PYGZsq{},}
    \PYG{c+c1}{\PYGZsh{}     n\PYGZus{}jobs=4,}
    \PYG{c+c1}{\PYGZsh{}     max\PYGZus{}iter=100),}

    \PYG{c+c1}{\PYGZsh{} \PYGZsq{}allknn\PYGZsq{}: AllKNN(}
    \PYG{c+c1}{\PYGZsh{}     sampling\PYGZus{}strategy=\PYGZsq{}auto\PYGZsq{},}
    \PYG{c+c1}{\PYGZsh{}     n\PYGZus{}neighbors=3,}
    \PYG{c+c1}{\PYGZsh{}     kind\PYGZus{}sel=\PYGZsq{}all\PYGZsq{},}
    \PYG{c+c1}{\PYGZsh{}     n\PYGZus{}jobs=4),}

    \PYG{c+c1}{\PYGZsh{} \PYGZsq{}ncr\PYGZsq{}: NeighbourhoodCleaningRule(}
    \PYG{c+c1}{\PYGZsh{}     sampling\PYGZus{}strategy=\PYGZsq{}auto\PYGZsq{},}
    \PYG{c+c1}{\PYGZsh{}     n\PYGZus{}neighbors=3,}
    \PYG{c+c1}{\PYGZsh{}     kind\PYGZus{}sel=\PYGZsq{}all\PYGZsq{},}
    \PYG{c+c1}{\PYGZsh{}     n\PYGZus{}jobs=4,}
    \PYG{c+c1}{\PYGZsh{}     threshold\PYGZus{}cleaning=0.5),}

    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{nm1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{NearMiss}\PYG{p}{(}
        \PYG{n}{sampling\PYGZus{}strategy}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{auto}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
        \PYG{n}{version}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,}
        \PYG{n}{n\PYGZus{}neighbors}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{,}
        \PYG{n}{n\PYGZus{}jobs}\PYG{o}{=}\PYG{l+m+mi}{4}\PYG{p}{)}\PYG{p}{,}

    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{nm2}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{NearMiss}\PYG{p}{(}
        \PYG{n}{sampling\PYGZus{}strategy}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{auto}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
        \PYG{n}{version}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{,}
        \PYG{n}{n\PYGZus{}neighbors}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{,}
        \PYG{n}{n\PYGZus{}jobs}\PYG{o}{=}\PYG{l+m+mi}{4}\PYG{p}{)}\PYG{p}{,}
\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} train a model on the original data without under\PYGZhy{}sampling}
\PYG{c+c1}{\PYGZsh{} and determine model performance}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{No UnderSampling}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{run\PYGZus{}exps}\PYG{p}{(}\PYG{n}{X\PYGZus{}train}\PYG{p}{,} \PYG{n}{X\PYGZus{}test}\PYG{p}{,} \PYG{n}{y\PYGZus{}train}\PYG{p}{,} \PYG{n}{y\PYGZus{}test}\PYG{p}{)}

\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{UnderSampling Methods}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} now, we test the different under\PYGZhy{}samplers, 1 at a time}
\PYG{k}{for} \PYG{n}{undersampler} \PYG{o+ow}{in} \PYG{n}{undersampler\PYGZus{}dict}\PYG{o}{.}\PYG{n}{keys}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
    
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{undersampler}\PYG{p}{)}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
    
    \PYG{c+c1}{\PYGZsh{} resample the train set only}
    \PYG{n}{X\PYGZus{}resampled}\PYG{p}{,} \PYG{n}{y\PYGZus{}resampled} \PYG{o}{=} \PYG{n}{undersampler\PYGZus{}dict}\PYG{p}{[}\PYG{n}{undersampler}\PYG{p}{]}\PYG{o}{.}\PYG{n}{fit\PYGZus{}resample}\PYG{p}{(}\PYG{n}{X\PYGZus{}train}\PYG{p}{,} \PYG{n}{y\PYGZus{}train}\PYG{p}{)}
    
    \PYG{c+c1}{\PYGZsh{} train model and evaluate performance}
    
    \PYG{c+c1}{\PYGZsh{} Note the performance returned is using the}
    \PYG{c+c1}{\PYGZsh{} test set, which was not under\PYGZhy{}sampled}
    
    \PYG{n}{run\PYGZus{}exps}\PYG{p}{(}\PYG{n}{X\PYGZus{}resampled}\PYG{p}{,} \PYG{n}{X\PYGZus{}test}\PYG{p}{,} \PYG{n}{y\PYGZus{}resampled}\PYG{p}{,} \PYG{n}{y\PYGZus{}test}\PYG{p}{)}
    
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{p}{)}
    
\PYG{n+nb}{print}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
No UnderSampling
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Model_evaluation_15_1}.png}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Algorithm with the highest accuracy: [\PYGZsq{}Gradient Boost Classifier\PYGZsq{}, 0.9157898028424459]
Algorithm with the highest macro recall:
        [\PYGZsq{}Naive Bayes\PYGZsq{}, 0.6833829351601921]
Algorithm with the highest macro precision:
        [\PYGZsq{}Gradient Boost Classifier\PYGZsq{}, 0.7359577899388909]
Algorithm with the highest AUC:
        [\PYGZsq{}Naive Bayes\PYGZsq{}, 0.6833829351601921]
model                      index             precision  recall    f1\PYGZhy{}score  support       auc     
Decision Tree Classifier   Heart Disease     0.224438   0.242557  0.233146  5475.000000   0.582046    0
                           No Heart Disease  0.928552   0.921534  0.925030  58484.000000  0.582046    0
                           accuracy          0.863412   0.863412  0.863412  0.863412      0.582046    0
                           macro avg         0.576495   0.582046  0.579088  63959.000000  0.582046    0
                           weighted avg      0.868278   0.863412  0.865803  63959.000000  0.582046    0
Gradient Boost Classifier  Heart Disease     0.551091   0.087671  0.151276  5475.000000   0.540493    0
                           No Heart Disease  0.920825   0.993314  0.955697  58484.000000  0.540493    0
                           accuracy          0.915790   0.915790  0.915790  0.915790      0.540493    0
                           macro avg         0.735958   0.540493  0.553487  63959.000000  0.540493    0
                           weighted avg      0.889175   0.915790  0.886837  63959.000000  0.540493    0
Logistic Regression        Heart Disease     0.533333   0.109589  0.181818  5475.000000   0.550306    0
                           No Heart Disease  0.922415   0.991023  0.955489  58484.000000  0.550306    0
                           accuracy          0.915571   0.915571  0.915571  0.915571      0.550306    0
                           macro avg         0.727874   0.550306  0.568654  63959.000000  0.550306    0
                           weighted avg      0.889109   0.915571  0.889261  63959.000000  0.550306    0
Naive Bayes                Heart Disease     0.256661   0.503196  0.339935  5475.000000   0.683383    0
                           No Heart Disease  0.948896   0.863570  0.904224  58484.000000  0.683383    0
                           accuracy          0.832721   0.832721  0.832721  0.832721      0.683383    0
                           macro avg         0.602779   0.683383  0.622079  63959.000000  0.683383    0
                           weighted avg      0.889640   0.832721  0.855920  63959.000000  0.683383    0
Random Forest Classifier   Heart Disease     0.380066   0.104475  0.163897  5475.000000   0.544261    0
                           No Heart Disease  0.921494   0.984047  0.951744  58484.000000  0.544261    0
                           accuracy          0.908754   0.908754  0.908754  0.908754      0.544261    0
                           macro avg         0.650780   0.544261  0.557820  63959.000000  0.544261    0
                           weighted avg      0.875147   0.908754  0.884303  63959.000000  0.544261    0
Name: dummy, dtype: int64
UnderSampling Methods
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}

random
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Model_evaluation_15_3}.png}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Algorithm with the highest accuracy: [\PYGZsq{}Naive Bayes\PYGZsq{}, 0.777842055066527]
Algorithm with the highest macro recall:
        [\PYGZsq{}Gradient Boost Classifier\PYGZsq{}, 0.7636522622274398]
Algorithm with the highest macro precision:
        [\PYGZsq{}Logistic Regression \PYGZsq{}, 0.5987409474695906]
Algorithm with the highest AUC:
        [\PYGZsq{}Gradient Boost Classifier\PYGZsq{}, 0.7636522622274399]
model                      index             precision  recall    f1\PYGZhy{}score  support       auc     
Decision Tree Classifier   Heart Disease     0.163305   0.672877  0.262824  5475.000000   0.675069    0
                           No Heart Disease  0.956739   0.677262  0.793100  58484.000000  0.675069    0
                           accuracy          0.676887   0.676887  0.676887  0.676887      0.675069    0
                           macro avg         0.560022   0.675069  0.527962  63959.000000  0.675069    0
                           weighted avg      0.888820   0.676887  0.747707  63959.000000  0.675069    0
Gradient Boost Classifier  Heart Disease     0.217049   0.796164  0.341107  5475.000000   0.763652    0
                           No Heart Disease  0.974565   0.731140  0.835483  58484.000000  0.763652    0
                           accuracy          0.736706   0.736706  0.736706  0.736706      0.763652    0
                           macro avg         0.595807   0.763652  0.588295  63959.000000  0.763652    0
                           weighted avg      0.909720   0.736706  0.793163  63959.000000  0.763652    0
Logistic Regression        Heart Disease     0.224622   0.776804  0.348478  5475.000000   0.762889    0
                           No Heart Disease  0.972860   0.748974  0.846361  58484.000000  0.762889    0
                           accuracy          0.751356   0.751356  0.751356  0.751356      0.762889    0
                           macro avg         0.598741   0.762889  0.597420  63959.000000  0.762889    0
                           weighted avg      0.908809   0.751356  0.803742  63959.000000  0.762889    0
Naive Bayes                Heart Disease     0.223082   0.642557  0.331184  5475.000000   0.716532    0
                           No Heart Disease  0.959389   0.790507  0.866799  58484.000000  0.716532    0
                           accuracy          0.777842   0.777842  0.777842  0.777842      0.716532    0
                           macro avg         0.591235   0.716532  0.598991  63959.000000  0.716532    0
                           weighted avg      0.896360   0.777842  0.820949  63959.000000  0.716532    0
Random Forest Classifier   Heart Disease     0.205081   0.779909  0.324764  5475.000000   0.748454    0
                           No Heart Disease  0.972066   0.717000  0.825274  58484.000000  0.748454    0
                           accuracy          0.722385   0.722385  0.722385  0.722385      0.748454    0
                           macro avg         0.588574   0.748454  0.575019  63959.000000  0.748454    0
                           weighted avg      0.906411   0.722385  0.782430  63959.000000  0.748454    0
Name: dummy, dtype: int64

tomek
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gt}{\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}}
\PYG{n+ne}{KeyboardInterrupt}\PYG{g+gWhitespace}{                         }Traceback (most recent call last)
\PYG{o}{/}\PYG{n}{var}\PYG{o}{/}\PYG{n}{folders}\PYG{o}{/}\PYG{l+m+mi}{0}\PYG{n}{f}\PYG{o}{/}\PYG{n}{wgt07yjn5gnfhmk719tfc2gm1vjz80}\PYG{o}{/}\PYG{n}{T}\PYG{o}{/}\PYG{n}{ipykernel\PYGZus{}77434}\PYG{o}{/}\PYG{l+m+mf}{219296026.}\PYG{n}{py} \PYG{o+ow}{in} \PYG{o}{\PYGZlt{}}\PYG{n}{module}\PYG{o}{\PYGZgt{}}
\PYG{g+gWhitespace}{     }\PYG{l+m+mi}{16} 
\PYG{g+gWhitespace}{     }\PYG{l+m+mi}{17}     \PYG{c+c1}{\PYGZsh{} resample the train set only}
\PYG{n+ne}{\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZgt{} }\PYG{l+m+mi}{18}     \PYG{n}{X\PYGZus{}resampled}\PYG{p}{,} \PYG{n}{y\PYGZus{}resampled} \PYG{o}{=} \PYG{n}{undersampler\PYGZus{}dict}\PYG{p}{[}\PYG{n}{undersampler}\PYG{p}{]}\PYG{o}{.}\PYG{n}{fit\PYGZus{}resample}\PYG{p}{(}\PYG{n}{X\PYGZus{}train}\PYG{p}{,} \PYG{n}{y\PYGZus{}train}\PYG{p}{)}
\PYG{g+gWhitespace}{     }\PYG{l+m+mi}{19} 
\PYG{g+gWhitespace}{     }\PYG{l+m+mi}{20}     \PYG{c+c1}{\PYGZsh{} train model and evaluate performance}

\PYG{n+nn}{\PYGZti{}/Library/CloudStorage/OneDrive\PYGZhy{}UHG/repos/learn\PYGZhy{}hub/Anthony/heart\PYGZus{}disease\PYGZus{}task/venv/lib/python3.7/site\PYGZhy{}packages/imblearn/base.py} in \PYG{n+ni}{fit\PYGZus{}resample}\PYG{n+nt}{(self, X, y)}
\PYG{g+gWhitespace}{     }\PYG{l+m+mi}{81}         \PYG{p}{)}
\PYG{g+gWhitespace}{     }\PYG{l+m+mi}{82} 
\PYG{n+ne}{\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZgt{} }\PYG{l+m+mi}{83}         \PYG{n}{output} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{\PYGZus{}fit\PYGZus{}resample}\PYG{p}{(}\PYG{n}{X}\PYG{p}{,} \PYG{n}{y}\PYG{p}{)}
\PYG{g+gWhitespace}{     }\PYG{l+m+mi}{84} 
\PYG{g+gWhitespace}{     }\PYG{l+m+mi}{85}         \PYG{n}{y\PYGZus{}} \PYG{o}{=} \PYG{p}{(}

\PYG{n+nn}{\PYGZti{}/Library/CloudStorage/OneDrive\PYGZhy{}UHG/repos/learn\PYGZhy{}hub/Anthony/heart\PYGZus{}disease\PYGZus{}task/venv/lib/python3.7/site\PYGZhy{}packages/imblearn/under\PYGZus{}sampling/\PYGZus{}prototype\PYGZus{}selection/\PYGZus{}tomek\PYGZus{}links.py} in \PYG{n+ni}{\PYGZus{}fit\PYGZus{}resample}\PYG{n+nt}{(self, X, y)}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{127}         \PYG{n}{nn} \PYG{o}{=} \PYG{n}{NearestNeighbors}\PYG{p}{(}\PYG{n}{n\PYGZus{}neighbors}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{n\PYGZus{}jobs}\PYG{o}{=}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{n\PYGZus{}jobs}\PYG{p}{)}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{128}         \PYG{n}{nn}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{X}\PYG{p}{)}
\PYG{n+ne}{\PYGZhy{}\PYGZhy{}\PYGZgt{} }\PYG{l+m+mi}{129}         \PYG{n}{nns} \PYG{o}{=} \PYG{n}{nn}\PYG{o}{.}\PYG{n}{kneighbors}\PYG{p}{(}\PYG{n}{X}\PYG{p}{,} \PYG{n}{return\PYGZus{}distance}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{130} 
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{131}         \PYG{n}{links} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{is\PYGZus{}tomek}\PYG{p}{(}\PYG{n}{y}\PYG{p}{,} \PYG{n}{nns}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{sampling\PYGZus{}strategy\PYGZus{}}\PYG{p}{)}

\PYG{n+nn}{\PYGZti{}/Library/CloudStorage/OneDrive\PYGZhy{}UHG/repos/learn\PYGZhy{}hub/Anthony/heart\PYGZus{}disease\PYGZus{}task/venv/lib/python3.7/site\PYGZhy{}packages/sklearn/neighbors/\PYGZus{}base.py} in \PYG{n+ni}{kneighbors}\PYG{n+nt}{(self, X, n\PYGZus{}neighbors, return\PYGZus{}distance)}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{757}                     \PYG{n}{metric}\PYG{o}{=}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{effective\PYGZus{}metric\PYGZus{}}\PYG{p}{,}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{758}                     \PYG{n}{n\PYGZus{}jobs}\PYG{o}{=}\PYG{n}{n\PYGZus{}jobs}\PYG{p}{,}
\PYG{n+ne}{\PYGZhy{}\PYGZhy{}\PYGZgt{} }\PYG{l+m+mi}{759}                     \PYG{o}{*}\PYG{o}{*}\PYG{n}{kwds}\PYG{p}{,}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{760}                 \PYG{p}{)}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{761}             \PYG{p}{)}

\PYG{n+nn}{\PYGZti{}/Library/CloudStorage/OneDrive\PYGZhy{}UHG/repos/learn\PYGZhy{}hub/Anthony/heart\PYGZus{}disease\PYGZus{}task/venv/lib/python3.7/site\PYGZhy{}packages/sklearn/metrics/pairwise.py} in \PYG{n+ni}{pairwise\PYGZus{}distances\PYGZus{}chunked}\PYG{n+nt}{(X, Y, reduce\PYGZus{}func, metric, n\PYGZus{}jobs, working\PYGZus{}memory, **kwds)}
\PYG{g+gWhitespace}{   }\PYG{l+m+mi}{1724}         \PYG{k}{if} \PYG{n}{reduce\PYGZus{}func} \PYG{o+ow}{is} \PYG{o+ow}{not} \PYG{k+kc}{None}\PYG{p}{:}
\PYG{g+gWhitespace}{   }\PYG{l+m+mi}{1725}             \PYG{n}{chunk\PYGZus{}size} \PYG{o}{=} \PYG{n}{D\PYGZus{}chunk}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}
\PYG{n+ne}{\PYGZhy{}\PYGZgt{} }\PYG{l+m+mi}{1726}             \PYG{n}{D\PYGZus{}chunk} \PYG{o}{=} \PYG{n}{reduce\PYGZus{}func}\PYG{p}{(}\PYG{n}{D\PYGZus{}chunk}\PYG{p}{,} \PYG{n}{sl}\PYG{o}{.}\PYG{n}{start}\PYG{p}{)}
\PYG{g+gWhitespace}{   }\PYG{l+m+mi}{1727}             \PYG{n}{\PYGZus{}check\PYGZus{}chunk\PYGZus{}size}\PYG{p}{(}\PYG{n}{D\PYGZus{}chunk}\PYG{p}{,} \PYG{n}{chunk\PYGZus{}size}\PYG{p}{)}
\PYG{g+gWhitespace}{   }\PYG{l+m+mi}{1728}         \PYG{k}{yield} \PYG{n}{D\PYGZus{}chunk}

\PYG{n+nn}{\PYGZti{}/Library/CloudStorage/OneDrive\PYGZhy{}UHG/repos/learn\PYGZhy{}hub/Anthony/heart\PYGZus{}disease\PYGZus{}task/venv/lib/python3.7/site\PYGZhy{}packages/sklearn/neighbors/\PYGZus{}base.py} in \PYG{n+ni}{\PYGZus{}kneighbors\PYGZus{}reduce\PYGZus{}func}\PYG{n+nt}{(self, dist, start, n\PYGZus{}neighbors, return\PYGZus{}distance)}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{632}         \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{633}\PYG{l+s+sd}{         sample\PYGZus{}range = np.arange(dist.shape[0])[:, None]}
\PYG{n+ne}{\PYGZhy{}\PYGZhy{}\PYGZgt{} }\PYG{l+m+mi}{634}\PYG{l+s+sd}{         neigh\PYGZus{}ind = np.argpartition(dist, n\PYGZus{}neighbors \PYGZhy{} 1, axis=1)}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{635}\PYG{l+s+sd}{         neigh\PYGZus{}ind = neigh\PYGZus{}ind[:, :n\PYGZus{}neighbors]}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{636}\PYG{l+s+sd}{         \PYGZsh{} argpartition doesn\PYGZsq{}t guarantee sorted order, so we sort again}

\PYG{n+nn}{\PYGZlt{}\PYGZus{}\PYGZus{}array\PYGZus{}function\PYGZus{}\PYGZus{} internals\PYGZgt{}} in \PYG{n+ni}{argpartition}\PYG{n+nt}{(*args, **kwargs)}

\PYG{n+nn}{\PYGZti{}/Library/CloudStorage/OneDrive\PYGZhy{}UHG/repos/learn\PYGZhy{}hub/Anthony/heart\PYGZus{}disease\PYGZus{}task/venv/lib/python3.7/site\PYGZhy{}packages/numpy/core/fromnumeric.py} in \PYG{n+ni}{argpartition}\PYG{n+nt}{(a, kth, axis, kind, order)}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{837}\PYG{l+s+sd}{ }
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{838}\PYG{l+s+sd}{     \PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{n+ne}{\PYGZhy{}\PYGZhy{}\PYGZgt{} }\PYG{l+m+mi}{839}     \PYG{k}{return} \PYG{n}{\PYGZus{}wrapfunc}\PYG{p}{(}\PYG{n}{a}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{argpartition}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{kth}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{n}{axis}\PYG{p}{,} \PYG{n}{kind}\PYG{o}{=}\PYG{n}{kind}\PYG{p}{,} \PYG{n}{order}\PYG{o}{=}\PYG{n}{order}\PYG{p}{)}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{840} 
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{841} 

\PYG{n+nn}{\PYGZti{}/Library/CloudStorage/OneDrive\PYGZhy{}UHG/repos/learn\PYGZhy{}hub/Anthony/heart\PYGZus{}disease\PYGZus{}task/venv/lib/python3.7/site\PYGZhy{}packages/numpy/core/fromnumeric.py} in \PYG{n+ni}{\PYGZus{}wrapfunc}\PYG{n+nt}{(obj, method, *args, **kwds)}
\PYG{g+gWhitespace}{     }\PYG{l+m+mi}{55} 
\PYG{g+gWhitespace}{     }\PYG{l+m+mi}{56}     \PYG{k}{try}\PYG{p}{:}
\PYG{n+ne}{\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZgt{} }\PYG{l+m+mi}{57}         \PYG{k}{return} \PYG{n}{bound}\PYG{p}{(}\PYG{o}{*}\PYG{n}{args}\PYG{p}{,} \PYG{o}{*}\PYG{o}{*}\PYG{n}{kwds}\PYG{p}{)}
\PYG{g+gWhitespace}{     }\PYG{l+m+mi}{58}     \PYG{k}{except} \PYG{n+ne}{TypeError}\PYG{p}{:}
\PYG{g+gWhitespace}{     }\PYG{l+m+mi}{59}         \PYG{c+c1}{\PYGZsh{} A TypeError occurs if the object does have such a method in its}

\PYG{n+ne}{KeyboardInterrupt}: 
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{Algorithm} \PYG{k}{with} \PYG{n}{the} \PYG{n}{highest} \PYG{n}{accuracy}\PYG{p}{:} \PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Naive Bayes}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+m+mf}{0.45100767679294546}\PYG{p}{]}
\PYG{n}{Algorithm} \PYG{k}{with} \PYG{n}{the} \PYG{n}{highest} \PYG{n}{macro} \PYG{n}{recall}\PYG{p}{:}
        \PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Logistic Regression }\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+m+mf}{0.5912552564819664}\PYG{p}{]}
\PYG{n}{Algorithm} \PYG{k}{with} \PYG{n}{the} \PYG{n}{highest} \PYG{n}{macro} \PYG{n}{precision}\PYG{p}{:}
        \PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Logistic Regression }\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+m+mf}{0.5313939861738548}\PYG{p}{]}
\PYG{n}{Algorithm} \PYG{k}{with} \PYG{n}{the} \PYG{n}{highest} \PYG{n}{AUC}\PYG{p}{:}
        \PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Logistic Regression }\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+m+mf}{0.5912552564819664}\PYG{p}{]}
        
\PYG{n}{Algorithm} \PYG{k}{with} \PYG{n}{the} \PYG{n}{highest} \PYG{n}{accuracy}\PYG{p}{:} \PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Gradient Boost Classifier}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+m+mf}{0.9157428977938992}\PYG{p}{]}
\PYG{n}{Algorithm} \PYG{k}{with} \PYG{n}{the} \PYG{n}{highest} \PYG{n}{macro} \PYG{n}{recall}\PYG{p}{:}
        \PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Naive Bayes}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+m+mf}{0.687578840593017}\PYG{p}{]}
\PYG{n}{Algorithm} \PYG{k}{with} \PYG{n}{the} \PYG{n}{highest} \PYG{n}{macro} \PYG{n}{precision}\PYG{p}{:}
        \PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Gradient Boost Classifier}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+m+mf}{0.7293058306944802}\PYG{p}{]}
\PYG{n}{Algorithm} \PYG{k}{with} \PYG{n}{the} \PYG{n}{highest} \PYG{n}{AUC}\PYG{p}{:}
        \PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Naive Bayes}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+m+mf}{0.6875788405930171}\PYG{p}{]}
        
\PYG{n}{Algorithm} \PYG{k}{with} \PYG{n}{the} \PYG{n}{highest} \PYG{n}{accuracy}\PYG{p}{:} \PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Gradient Boost Classifier}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+m+mf}{0.9158679779233572}\PYG{p}{]}
\PYG{n}{Algorithm} \PYG{k}{with} \PYG{n}{the} \PYG{n}{highest} \PYG{n}{macro} \PYG{n}{recall}\PYG{p}{:}
        \PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Naive Bayes}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+m+mf}{0.6871222195884508}\PYG{p}{]}
\PYG{n}{Algorithm} \PYG{k}{with} \PYG{n}{the} \PYG{n}{highest} \PYG{n}{macro} \PYG{n}{precision}\PYG{p}{:}
        \PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Gradient Boost Classifier}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+m+mf}{0.7313563006184678}\PYG{p}{]}
\PYG{n}{Algorithm} \PYG{k}{with} \PYG{n}{the} \PYG{n}{highest} \PYG{n}{AUC}\PYG{p}{:}
        \PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Naive Bayes}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+m+mf}{0.6871222195884508}\PYG{p}{]}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
As we can see in the verbose model output, we achieved the follwoig


\section{Oversampling}
\label{\detokenize{Model_evaluation:oversampling}}
\sphinxAtStartPar
The following undersampling methods were choosen:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{Random Oversampling:}} Random over\sphinxhyphen{}sampling consists in extracting at random samples from the minority class, until they reach a certain proportion compared to the majority class, typically 50:50, or in other words, a balancing ratio of 1.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{SMOTE}}: Creates new samples by interpolation of samples of the minority class and any of its k nearest neighbours (also from the minority class). K is typically 5.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{ADASYN}}: Creates new samples by interpolation of samples of the minority class and its closest neighbours. It creates more samples from samples that are harder to classify.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{Borderline SMOTE}}: Creates new samples by interpolation between samples of the minority class and their closest neighbours.
\begin{itemize}
\item {} 
\sphinxAtStartPar
It does not use all observations from the minority class as templates, unllike SMOTE.

\item {} 
\sphinxAtStartPar
It selects those observations (from the minority) for which, most of their neighbours belong to a different class (DANGER group)
\begin{itemize}
\item {} 
\sphinxAtStartPar
Variant 1 creates new examples, as SMOTE, between samples in the Danger group and their closest neighbours from the minority

\item {} 
\sphinxAtStartPar
Variant 2 creates new examples between samples in the Danger group and neighbours from minority and majority class

\end{itemize}

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{SVM SMOTE}}: Creates new samples by interpolation of samples of the support vectors from minority class and its closest neighbours.

\end{itemize}

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
We train the models on a portion of the data that is over\sphinxhyphen{}sampled
We evaluate the model performance on another portion of the data that was not resampled, and thus contains the original class distribution.
\end{sphinxadmonition}

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{oversampler\PYGZus{}dict} \PYG{o}{=} \PYG{p}{\PYGZob{}}

    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{random}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{RandomOverSampler}\PYG{p}{(}
        \PYG{n}{sampling\PYGZus{}strategy}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{auto}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
        \PYG{n}{random\PYGZus{}state}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}\PYG{p}{,}

    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{smote}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{SMOTE}\PYG{p}{(}
        \PYG{n}{sampling\PYGZus{}strategy}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{auto}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}  \PYG{c+c1}{\PYGZsh{} samples only the minority class}
        \PYG{n}{random\PYGZus{}state}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,}  \PYG{c+c1}{\PYGZsh{} for reproducibility}
        \PYG{n}{k\PYGZus{}neighbors}\PYG{o}{=}\PYG{l+m+mi}{5}\PYG{p}{,}
        \PYG{n}{n\PYGZus{}jobs}\PYG{o}{=}\PYG{l+m+mi}{4}\PYG{p}{)}\PYG{p}{,}

    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{adasyn}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{ADASYN}\PYG{p}{(}
        \PYG{n}{sampling\PYGZus{}strategy}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{auto}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}  \PYG{c+c1}{\PYGZsh{} samples only the minority class}
        \PYG{n}{random\PYGZus{}state}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,}  \PYG{c+c1}{\PYGZsh{} for reproducibility}
        \PYG{n}{n\PYGZus{}neighbors}\PYG{o}{=}\PYG{l+m+mi}{5}\PYG{p}{,}
        \PYG{n}{n\PYGZus{}jobs}\PYG{o}{=}\PYG{l+m+mi}{4}\PYG{p}{)}\PYG{p}{,}

    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{border1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{BorderlineSMOTE}\PYG{p}{(}
        \PYG{n}{sampling\PYGZus{}strategy}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{auto}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}  \PYG{c+c1}{\PYGZsh{} samples only the minority class}
        \PYG{n}{random\PYGZus{}state}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,}  \PYG{c+c1}{\PYGZsh{} for reproducibility}
        \PYG{n}{k\PYGZus{}neighbors}\PYG{o}{=}\PYG{l+m+mi}{5}\PYG{p}{,}
        \PYG{n}{m\PYGZus{}neighbors}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{,}
        \PYG{n}{kind}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{borderline\PYGZhy{}1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
        \PYG{n}{n\PYGZus{}jobs}\PYG{o}{=}\PYG{l+m+mi}{4}\PYG{p}{)}\PYG{p}{,}

    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{border2}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{BorderlineSMOTE}\PYG{p}{(}
        \PYG{n}{sampling\PYGZus{}strategy}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{auto}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}  \PYG{c+c1}{\PYGZsh{} samples only the minority class}
        \PYG{n}{random\PYGZus{}state}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,}  \PYG{c+c1}{\PYGZsh{} for reproducibility}
        \PYG{n}{k\PYGZus{}neighbors}\PYG{o}{=}\PYG{l+m+mi}{5}\PYG{p}{,}
        \PYG{n}{m\PYGZus{}neighbors}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{,}
        \PYG{n}{kind}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{borderline\PYGZhy{}2}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
        \PYG{n}{n\PYGZus{}jobs}\PYG{o}{=}\PYG{l+m+mi}{4}\PYG{p}{)}\PYG{p}{,}

    \PYG{c+c1}{\PYGZsh{} \PYGZsq{}svm\PYGZsq{}: SVMSMOTE(}
    \PYG{c+c1}{\PYGZsh{}     sampling\PYGZus{}strategy=\PYGZsq{}auto\PYGZsq{},  \PYGZsh{} samples only the minority class}
    \PYG{c+c1}{\PYGZsh{}     random\PYGZus{}state=0,  \PYGZsh{} for reproducibility}
    \PYG{c+c1}{\PYGZsh{}     k\PYGZus{}neighbors=5,}
    \PYG{c+c1}{\PYGZsh{}     m\PYGZus{}neighbors=10,}
    \PYG{c+c1}{\PYGZsh{}     n\PYGZus{}jobs=4,}
    \PYG{c+c1}{\PYGZsh{}     svm\PYGZus{}estimator=SVC(kernel=\PYGZsq{}linear\PYGZsq{})),}
\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} train a model on the original data without under\PYGZhy{}sampling}
\PYG{c+c1}{\PYGZsh{} and determine model performance}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{No OverSampling}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{run\PYGZus{}exps}\PYG{p}{(}\PYG{n}{X\PYGZus{}train}\PYG{p}{,} \PYG{n}{X\PYGZus{}test}\PYG{p}{,} \PYG{n}{y\PYGZus{}train}\PYG{p}{,} \PYG{n}{y\PYGZus{}test}\PYG{p}{)}

\PYG{n+nb}{print}\PYG{p}{(}\PYG{p}{)}

\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{OverSampling Methods}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} now, we test the different under\PYGZhy{}samplers, 1 at a time}
\PYG{k}{for} \PYG{n}{oversampler} \PYG{o+ow}{in} \PYG{n}{oversampler\PYGZus{}dict}\PYG{o}{.}\PYG{n}{keys}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
    
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{oversampler}\PYG{p}{)}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
    
    \PYG{c+c1}{\PYGZsh{} resample the train set only}
    \PYG{n}{X\PYGZus{}resampled}\PYG{p}{,} \PYG{n}{y\PYGZus{}resampled} \PYG{o}{=} \PYG{n}{oversampler\PYGZus{}dict}\PYG{p}{[}\PYG{n}{oversampler}\PYG{p}{]}\PYG{o}{.}\PYG{n}{fit\PYGZus{}resample}\PYG{p}{(}\PYG{n}{X\PYGZus{}train}\PYG{p}{,} \PYG{n}{y\PYGZus{}train}\PYG{p}{)}
    
    \PYG{c+c1}{\PYGZsh{} train model and evaluate performance}
    
    \PYG{c+c1}{\PYGZsh{} Note the performance returned is using the}
    \PYG{c+c1}{\PYGZsh{} test set, which was not under\PYGZhy{}sampled}
    
    \PYG{n}{run\PYGZus{}exps}\PYG{p}{(}\PYG{n}{X\PYGZus{}resampled}\PYG{p}{,} \PYG{n}{X\PYGZus{}test}\PYG{p}{,} \PYG{n}{y\PYGZus{}resampled}\PYG{p}{,} \PYG{n}{y\PYGZus{}test}\PYG{p}{)}
    
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{p}{)}
    
\PYG{n+nb}{print}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
No OverSampling
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Model_evaluation_20_1}.png}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Algorithm with the highest accuracy: [\PYGZsq{}Gradient Boost Classifier\PYGZsq{}, 0.9157898028424459]
Algorithm with the highest macro recall:
        [\PYGZsq{}Naive Bayes\PYGZsq{}, 0.6833829351601921]
Algorithm with the highest macro precision:
        [\PYGZsq{}Gradient Boost Classifier\PYGZsq{}, 0.7359577899388909]
Algorithm with the highest AUC:
        [\PYGZsq{}Naive Bayes\PYGZsq{}, 0.6833829351601921]
model                      index             precision  recall    f1\PYGZhy{}score  support       auc     
Decision Tree Classifier   Heart Disease     0.223938   0.243653  0.233380  5475.000000   0.582303    0
                           No Heart Disease  0.928606   0.920953  0.924763  58484.000000  0.582303    0
                           accuracy          0.862975   0.862975  0.862975  0.862975      0.582303    0
                           macro avg         0.576272   0.582303  0.579072  63959.000000  0.582303    0
                           weighted avg      0.868285   0.862975  0.865580  63959.000000  0.582303    0
Gradient Boost Classifier  Heart Disease     0.551091   0.087671  0.151276  5475.000000   0.540493    0
                           No Heart Disease  0.920825   0.993314  0.955697  58484.000000  0.540493    0
                           accuracy          0.915790   0.915790  0.915790  0.915790      0.540493    0
                           macro avg         0.735958   0.540493  0.553487  63959.000000  0.540493    0
                           weighted avg      0.889175   0.915790  0.886837  63959.000000  0.540493    0
Logistic Regression        Heart Disease     0.532860   0.109589  0.181791  5475.000000   0.550298    0
                           No Heart Disease  0.922413   0.991006  0.955480  58484.000000  0.550298    0
                           accuracy          0.915555   0.915555  0.915555  0.915555      0.550298    0
                           macro avg         0.727637   0.550298  0.568635  63959.000000  0.550298    0
                           weighted avg      0.889067   0.915555  0.889251  63959.000000  0.550298    0
Naive Bayes                Heart Disease     0.256661   0.503196  0.339935  5475.000000   0.683383    0
                           No Heart Disease  0.948896   0.863570  0.904224  58484.000000  0.683383    0
                           accuracy          0.832721   0.832721  0.832721  0.832721      0.683383    0
                           macro avg         0.602779   0.683383  0.622079  63959.000000  0.683383    0
                           weighted avg      0.889640   0.832721  0.855920  63959.000000  0.683383    0
Random Forest Classifier   Heart Disease     0.375333   0.102831  0.161434  5475.000000   0.543405    0
                           No Heart Disease  0.921356   0.983979  0.951638  58484.000000  0.543405    0
                           accuracy          0.908551   0.908551  0.908551  0.908551      0.543405    0
                           macro avg         0.648345   0.543405  0.556536  63959.000000  0.543405    0
                           weighted avg      0.874616   0.908551  0.883996  63959.000000  0.543405    0
Name: dummy, dtype: int64

OverSampling Methods
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}
random
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Model_evaluation_20_3}.png}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Algorithm with the highest accuracy: [\PYGZsq{}Random Forest Classifier\PYGZsq{}, 0.8951203114495223]
Algorithm with the highest macro recall:
        [\PYGZsq{}Gradient Boost Classifier\PYGZsq{}, 0.7644372640341237]
Algorithm with the highest macro precision:
        [\PYGZsq{}Random Forest Classifier\PYGZsq{}, 0.625360074756374]
Algorithm with the highest AUC:
        [\PYGZsq{}Gradient Boost Classifier\PYGZsq{}, 0.7644372640341237]
model                      index             precision  recall    f1\PYGZhy{}score  support       auc     
Decision Tree Classifier   Heart Disease     0.218871   0.227945  0.223316  5475.000000   0.575894    0
                           No Heart Disease  0.927442   0.923842  0.925639  58484.000000  0.575894    0
                           accuracy          0.864272   0.864272  0.864272  0.864272      0.575894    0
                           macro avg         0.573156   0.575894  0.574477  63959.000000  0.575894    0
                           weighted avg      0.866787   0.864272  0.865519  63959.000000  0.575894    0
Gradient Boost Classifier  Heart Disease     0.217277   0.797991  0.341555  5475.000000   0.764437    0
                           No Heart Disease  0.974778   0.730884  0.835394  58484.000000  0.764437    0
                           accuracy          0.736628   0.736628  0.736628  0.736628      0.764437    0
                           macro avg         0.596027   0.764437  0.588474  63959.000000  0.764437    0
                           weighted avg      0.909935   0.736628  0.793120  63959.000000  0.764437    0
Logistic Regression        Heart Disease     0.224819   0.776621  0.348696  5475.000000   0.762969    0
                           No Heart Disease  0.972850   0.749316  0.846576  58484.000000  0.762969    0
                           accuracy          0.751653   0.751653  0.751653  0.751653      0.762969    0
                           macro avg         0.598834   0.762969  0.597636  63959.000000  0.762969    0
                           weighted avg      0.908817   0.751653  0.803957  63959.000000  0.762969    0
Naive Bayes                Heart Disease     0.223181   0.647489  0.331944  5475.000000   0.718254    0
                           No Heart Disease  0.959854   0.789019  0.866093  58484.000000  0.718254    0
                           accuracy          0.776904   0.776904  0.776904  0.776904      0.718254    0
                           macro avg         0.591517   0.718254  0.599019  63959.000000  0.718254    0
                           weighted avg      0.896794   0.776904  0.820369  63959.000000  0.718254    0
Random Forest Classifier   Heart Disease     0.322692   0.204932  0.250670  5475.000000   0.582332    0
                           No Heart Disease  0.928028   0.959733  0.943614  58484.000000  0.582332    0
                           accuracy          0.895120   0.895120  0.895120  0.895120      0.582332    0
                           macro avg         0.625360   0.582332  0.597142  63959.000000  0.582332    0
                           weighted avg      0.876210   0.895120  0.884297  63959.000000  0.582332    0
Name: dummy, dtype: int64

smote
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Model_evaluation_20_5}.png}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Algorithm with the highest accuracy: [\PYGZsq{}Random Forest Classifier\PYGZsq{}, 0.8920245782454385]
Algorithm with the highest macro recall:
        [\PYGZsq{}Logistic Regression \PYGZsq{}, 0.7382036799511805]
Algorithm with the highest macro precision:
        [\PYGZsq{}Gradient Boost Classifier\PYGZsq{}, 0.6204770738858411]
Algorithm with the highest AUC:
        [\PYGZsq{}Logistic Regression \PYGZsq{}, 0.7382036799511805]
model                      index             precision  recall    f1\PYGZhy{}score  support       auc     
Decision Tree Classifier   Heart Disease     0.220809   0.260091  0.238846  5475.000000   0.587085    0
                           No Heart Disease  0.929560   0.914079  0.921755  58484.000000  0.587085    0
                           accuracy          0.858097   0.858097  0.858097  0.858097      0.587085    0
                           macro avg         0.575185   0.587085  0.580300  63959.000000  0.587085    0
                           weighted avg      0.868890   0.858097  0.863296  63959.000000  0.587085    0
Gradient Boost Classifier  Heart Disease     0.292123   0.487671  0.365378  5475.000000   0.688521    0
                           No Heart Disease  0.948832   0.889371  0.918140  58484.000000  0.688521    0
                           accuracy          0.854985   0.854985  0.854985  0.854985      0.688521    0
                           macro avg         0.620477   0.688521  0.641759  63959.000000  0.688521    0
                           weighted avg      0.892616   0.854985  0.870822  63959.000000  0.688521    0
Logistic Regression        Heart Disease     0.216397   0.720731  0.332855  5475.000000   0.738204    0
                           No Heart Disease  0.966560   0.755677  0.848207  58484.000000  0.738204    0
                           accuracy          0.752685   0.752685  0.752685  0.752685      0.738204    0
                           macro avg         0.591479   0.738204  0.590531  63959.000000  0.738204    0
                           weighted avg      0.902345   0.752685  0.804092  63959.000000  0.738204    0
Naive Bayes                Heart Disease     0.153681   0.798721  0.257766  5475.000000   0.693475    0
                           No Heart Disease  0.968961   0.588229  0.732051  58484.000000  0.693475    0
                           accuracy          0.606248   0.606248  0.606248  0.606248      0.693475    0
                           macro avg         0.561321   0.693475  0.494908  63959.000000  0.693475    0
                           weighted avg      0.899172   0.606248  0.691451  63959.000000  0.693475    0
Random Forest Classifier   Heart Disease     0.301526   0.198539  0.239427  5475.000000   0.577742    0
                           No Heart Disease  0.927296   0.956945  0.941887  58484.000000  0.577742    0
                           accuracy          0.892025   0.892025  0.892025  0.892025      0.577742    0
                           macro avg         0.614411   0.577742  0.590657  63959.000000  0.577742    0
                           weighted avg      0.873729   0.892025  0.881755  63959.000000  0.577742    0
Name: dummy, dtype: int64

adasyn
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Model_evaluation_20_7}.png}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Algorithm with the highest accuracy: [\PYGZsq{}Random Forest Classifier\PYGZsq{}, 0.8920089432292563]
Algorithm with the highest macro recall:
        [\PYGZsq{}Logistic Regression \PYGZsq{}, 0.7313252830497449]
Algorithm with the highest macro precision:
        [\PYGZsq{}Gradient Boost Classifier\PYGZsq{}, 0.6193063466262022]
Algorithm with the highest AUC:
        [\PYGZsq{}Logistic Regression \PYGZsq{}, 0.7313252830497449]
model                      index             precision  recall    f1\PYGZhy{}score  support       auc     
Decision Tree Classifier   Heart Disease     0.223033   0.262466  0.241148  5475.000000   0.588435    0
                           No Heart Disease  0.929793   0.914404  0.922034  58484.000000  0.588435    0
                           accuracy          0.858597   0.858597  0.858597  0.858597      0.588435    0
                           macro avg         0.576413   0.588435  0.581591  63959.000000  0.588435    0
                           weighted avg      0.869294   0.858597  0.863749  63959.000000  0.588435    0
Gradient Boost Classifier  Heart Disease     0.291242   0.470137  0.359673  5475.000000   0.681515    0
                           No Heart Disease  0.947370   0.892894  0.919326  58484.000000  0.681515    0
                           accuracy          0.856705   0.856705  0.856705  0.856705      0.681515    0
                           macro avg         0.619306   0.681515  0.639499  63959.000000  0.681515    0
                           weighted avg      0.891205   0.856705  0.871419  63959.000000  0.681515    0
Logistic Regression        Heart Disease     0.210001   0.714155  0.324562  5475.000000   0.731325    0
                           No Heart Disease  0.965483   0.748495  0.843254  58484.000000  0.731325    0
                           accuracy          0.745556   0.745556  0.745556  0.745556      0.731325    0
                           macro avg         0.587742   0.731325  0.583908  63959.000000  0.731325    0
                           weighted avg      0.900812   0.745556  0.798853  63959.000000  0.731325    0
Naive Bayes                Heart Disease     0.145136   0.809132  0.246125  5475.000000   0.681488    0
                           No Heart Disease  0.968746   0.553844  0.704765  58484.000000  0.681488    0
                           accuracy          0.575697   0.575697  0.575697  0.575697      0.681488    0
                           macro avg         0.556941   0.681488  0.475445  63959.000000  0.681488    0
                           weighted avg      0.898244   0.575697  0.665505  63959.000000  0.681488    0
Random Forest Classifier   Heart Disease     0.297626   0.192329  0.233662  5475.000000   0.574919    0
                           No Heart Disease  0.926814   0.957510  0.941912  58484.000000  0.574919    0
                           accuracy          0.892009   0.892009  0.892009  0.892009      0.574919    0
                           macro avg         0.612220   0.574919  0.587787  63959.000000  0.574919    0
                           weighted avg      0.872954   0.892009  0.881284  63959.000000  0.574919    0
Name: dummy, dtype: int64

border1
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Model_evaluation_20_9}.png}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Algorithm with the highest accuracy: [\PYGZsq{}Random Forest Classifier\PYGZsq{}, 0.8934786347503869]
Algorithm with the highest macro recall:
        [\PYGZsq{}Logistic Regression \PYGZsq{}, 0.7351688039252979]
Algorithm with the highest macro precision:
        [\PYGZsq{}Random Forest Classifier\PYGZsq{}, 0.6200922423261308]
Algorithm with the highest AUC:
        [\PYGZsq{}Logistic Regression \PYGZsq{}, 0.7351688039252979]
model                      index             precision  recall    f1\PYGZhy{}score  support       auc     
Decision Tree Classifier   Heart Disease     0.217154   0.254795  0.234473  5475.000000   0.584403    0
                           No Heart Disease  0.929087   0.914011  0.921487  58484.000000  0.584403    0
                           accuracy          0.857581   0.857581  0.857581  0.857581      0.584403    0
                           macro avg         0.573121   0.584403  0.577980  63959.000000  0.584403    0
                           weighted avg      0.868144   0.857581  0.862677  63959.000000  0.584403    0
Gradient Boost Classifier  Heart Disease     0.287149   0.522374  0.370586  5475.000000   0.700487    0
                           No Heart Disease  0.951573   0.878599  0.913631  58484.000000  0.700487    0
                           accuracy          0.848106   0.848106  0.848106  0.848106      0.700487    0
                           macro avg         0.619361   0.700487  0.642109  63959.000000  0.700487    0
                           weighted avg      0.894697   0.848106  0.867146  63959.000000  0.700487    0
Logistic Regression        Heart Disease     0.225544   0.693151  0.340343  5475.000000   0.735169    0
                           No Heart Disease  0.964356   0.777187  0.860714  58484.000000  0.735169    0
                           accuracy          0.769993   0.769993  0.769993  0.769993      0.735169    0
                           macro avg         0.594950   0.735169  0.600529  63959.000000  0.735169    0
                           weighted avg      0.901113   0.769993  0.816169  63959.000000  0.735169    0
Naive Bayes                Heart Disease     0.164702   0.781005  0.272036  5475.000000   0.705101    0
                           No Heart Disease  0.968445   0.629198  0.762803  58484.000000  0.705101    0
                           accuracy          0.642193   0.642193  0.642193  0.642193      0.705101    0
                           macro avg         0.566574   0.705101  0.517420  63959.000000  0.705101    0
                           weighted avg      0.899643   0.642193  0.720793  63959.000000  0.705101    0
Random Forest Classifier   Heart Disease     0.312395   0.203470  0.246433  5475.000000   0.580772    0
                           No Heart Disease  0.927790   0.958074  0.942689  58484.000000  0.580772    0
                           accuracy          0.893479   0.893479  0.893479  0.893479      0.580772    0
                           macro avg         0.620092   0.580772  0.594561  63959.000000  0.580772    0
                           weighted avg      0.875111   0.893479  0.883088  63959.000000  0.580772    0
Name: dummy, dtype: int64

border2
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Model_evaluation_20_11}.png}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Algorithm with the highest accuracy: [\PYGZsq{}Random Forest Classifier\PYGZsq{}, 0.8956362669835364]
Algorithm with the highest macro recall:
        [\PYGZsq{}Logistic Regression \PYGZsq{}, 0.7133874620198195]
Algorithm with the highest macro precision:
        [\PYGZsq{}Gradient Boost Classifier\PYGZsq{}, 0.6324596168451473]
Algorithm with the highest AUC:
        [\PYGZsq{}Logistic Regression \PYGZsq{}, 0.7133874620198194]
model                      index             precision  recall    f1\PYGZhy{}score  support       auc     
Decision Tree Classifier   Heart Disease     0.210330   0.246941  0.227170  5475.000000   0.580074    0
                           No Heart Disease  0.928334   0.913207  0.920709  58484.000000  0.580074    0
                           accuracy          0.856173   0.856173  0.856173  0.856173      0.580074    0
                           macro avg         0.569332   0.580074  0.573939  63959.000000  0.580074    0
                           weighted avg      0.866872   0.856173  0.861340  63959.000000  0.580074    0
Gradient Boost Classifier  Heart Disease     0.321763   0.408037  0.359800  5475.000000   0.663759    0
                           No Heart Disease  0.943156   0.919482  0.931169  58484.000000  0.663759    0
                           accuracy          0.875702   0.875702  0.875702  0.875702      0.663759    0
                           macro avg         0.632460   0.663759  0.645485  63959.000000  0.663759    0
                           weighted avg      0.889964   0.875702  0.882259  63959.000000  0.663759    0
Logistic Regression        Heart Disease     0.235463   0.613151  0.340259  5475.000000   0.713387    0
                           No Heart Disease  0.957386   0.813624  0.879670  58484.000000  0.713387    0
                           accuracy          0.796463   0.796463  0.796463  0.796463      0.713387    0
                           macro avg         0.596425   0.713387  0.609965  63959.000000  0.713387    0
                           weighted avg      0.895588   0.796463  0.833496  63959.000000  0.713387    0
Naive Bayes                Heart Disease     0.131047   0.788676  0.224750  5475.000000   0.649553    0
                           No Heart Disease  0.962688   0.510430  0.667136  58484.000000  0.649553    0
                           accuracy          0.534249   0.534249  0.534249  0.534249      0.649553    0
                           macro avg         0.546868   0.649553  0.445943  63959.000000  0.649553    0
                           weighted avg      0.891498   0.534249  0.629267  63959.000000  0.649553    0
Random Forest Classifier   Heart Disease     0.319059   0.193242  0.240701  5475.000000   0.577317    0
                           No Heart Disease  0.927164   0.961391  0.943967  58484.000000  0.577317    0
                           accuracy          0.895636   0.895636  0.895636  0.895636      0.577317    0
                           macro avg         0.623112   0.577317  0.592334  63959.000000  0.577317    0
                           weighted avg      0.875109   0.895636  0.883767  63959.000000  0.577317    0
Name: dummy, dtype: int64

svm
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}

\section{Feature Selection}
\label{\detokenize{Model_evaluation:feature-selection}}
\sphinxAtStartPar
The idea of feature selection and extraction is to avoid the curse of
dimensionality. This refers to the fact that as we move to higher
dimension input feature spaces the volume of the space grows rapidly
and we end up with very few instances per unit volume, i.e. we have
very sparse sampling of the space of possible instances making
modelling difficult.

\sphinxAtStartPar
Feature Selection: It is clear from what we have seen that a good feature engineering
idea might be to choose a subset of the features available to reduce
the dimension of the feature space. This act is called feature
selection. One way of doing this is to try out different permutations of
features increasing the numbers of features involved as you proceed
and calculate machine learning performance. This is rarely practical
though. More efficient approaches include wrapper, filter and
embedded methods.

\sphinxAtStartPar
We decide to explore the following methods:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Perform PCA anaylsis and idenitify the variables that most contribute to the

\item {} 
\sphinxAtStartPar
A simple filter method:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Identify input features having high correlation with target variable.

\item {} 
\sphinxAtStartPar
Identify input features that have a low correlation with other independent variables

\item {} 
\sphinxAtStartPar
Find the information gain or mutual information of the independent variable with respect to a target variable

\end{itemize}

\item {} 
\sphinxAtStartPar
Permutation Based Feature Importance: ill randomly shuffle each feature and compute the change in the model’s performance

\end{itemize}


\subsection{PCA}
\label{\detokenize{Model_evaluation:pca}}
\sphinxAtStartPar
PCA is mathematically defined as an orthogonal linear transformation that transforms the data to a new coordinate system such that the greatest variance by some projection of the data comes to lie on the first coordinate (called the first principal component), the second greatest variance on the second coordinate, and so on.

\sphinxAtStartPar
We decide to get the top 5 features that contribute most to the first principal component and the top 5 features that contribute most to the second principal compoenent.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{pca\PYGZus{}most\PYGZus{}important\PYGZus{}features}\PYG{p}{(}\PYG{n}{df}\PYG{p}{:} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{Dataframe}\PYG{p}{)} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZgt{}} \PYG{n+nb}{list}\PYG{p}{[}\PYG{n+nb}{str}\PYG{p}{]}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    Retrieve the top 10 features that contribute most }
\PYG{l+s+sd}{        variation to the top 2 principal components}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{n}{model} \PYG{o}{=} \PYG{n}{PCA}\PYG{p}{(}\PYG{n}{n\PYGZus{}components}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{df}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} number of components}
    \PYG{n}{n\PYGZus{}pcs}\PYG{p}{:} \PYG{n+nb}{int} \PYG{o}{=} \PYG{n}{model}\PYG{o}{.}\PYG{n}{components\PYGZus{}}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}

    \PYG{n}{most\PYGZus{}important\PYGZus{}features\PYGZus{}indicies}\PYG{p}{:} \PYG{n}{List}\PYG{p}{[}\PYG{n+nb}{int}\PYG{p}{]} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}
    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{n\PYGZus{}pcs}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{top\PYGZus{}5} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{argpartition}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{abs}\PYG{p}{(}\PYG{n}{model}\PYG{o}{.}\PYG{n}{components\PYGZus{}}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{5}\PYG{p}{)}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{5}\PYG{p}{:}\PYG{p}{]}\PYG{o}{.}\PYG{n}{tolist}\PYG{p}{(}\PYG{p}{)}
        \PYG{n}{most\PYGZus{}important\PYGZus{}features\PYGZus{}indicies}\PYG{o}{.}\PYG{n}{extend}\PYG{p}{(}\PYG{n}{top\PYGZus{}5}\PYG{p}{)}
        
    \PYG{n}{most\PYGZus{}important\PYGZus{}features\PYGZus{}indicies} \PYG{o}{=} \PYG{n+nb}{list}\PYG{p}{(}\PYG{n+nb}{set}\PYG{p}{(}\PYG{n}{most\PYGZus{}important\PYGZus{}features\PYGZus{}indicies}\PYG{p}{)}\PYG{p}{)}
    \PYG{n}{initial\PYGZus{}feature\PYGZus{}names} \PYG{o}{=} \PYG{n}{df}\PYG{o}{.}\PYG{n}{columns}
    \PYG{n}{most\PYGZus{}important\PYGZus{}names}  \PYG{o}{=} \PYG{p}{[}\PYG{n}{initial\PYGZus{}feature\PYGZus{}names}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]} \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n}{most\PYGZus{}important\PYGZus{}features\PYGZus{}indicies}\PYG{p}{]}
    
    \PYG{k}{return} \PYG{n}{most\PYGZus{}important\PYGZus{}names}
    
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{pca\PYGZus{}df} \PYG{o}{=} \PYG{n}{pca\PYGZus{}most\PYGZus{}important\PYGZus{}features}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}

\subsection{Filter Methods}
\label{\detokenize{Model_evaluation:filter-methods}}
\sphinxAtStartPar
A simple filter method:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{Identify input features having high correlation with target variable}}: We want to keep features with only a high correlation with the target variable. This implies that the input feature has a high influence in predicting the target variable. We set the threshold to the absolute value of 0.2. We keep input features only if the correlation of the input feature with the target variable is greater than 0.2. Our analysis reveled most variables have little if all correlation to our target variable

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{Identify input features that have a low correlation with other independent variables}}

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{Find the information gain or mutual information of the independent variable with respect to a target variable}}

\end{itemize}

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{idenity\PYGZus{}high\PYGZus{}corr\PYGZus{}features}\PYG{p}{(}\PYG{n}{df}\PYG{p}{:} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{)} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZgt{}} \PYG{n}{List}\PYG{p}{[}\PYG{n+nb}{str}\PYG{p}{]}\PYG{p}{:}
    \PYG{n}{importances} \PYG{o}{=} \PYG{n}{df}\PYG{o}{.}\PYG{n}{drop}\PYG{p}{(}
        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{HeartDisease}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{.}\PYG{n}{apply}\PYG{p}{(}
            \PYG{k}{lambda} \PYG{n}{x}\PYG{p}{:} \PYG{n}{x}\PYG{o}{.}\PYG{n}{corr}\PYG{p}{(}\PYG{n}{df}\PYG{o}{.}\PYG{n}{HeartDisease}\PYG{p}{)}\PYG{p}{)}
    
    \PYG{n}{indices} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{argsort}\PYG{p}{(}\PYG{n}{importances}\PYG{p}{)}
    \PYG{n}{important\PYGZus{}feature\PYGZus{}names} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}
    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{indices}\PYG{p}{)}\PYG{p}{)}\PYG{p}{:}
        \PYG{k}{if} \PYG{n}{np}\PYG{o}{.}\PYG{n}{abs}\PYG{p}{(}\PYG{n}{importances}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{)} \PYG{o}{\PYGZgt{}} \PYG{l+m+mf}{0.2}\PYG{p}{:}
            \PYG{n}{important\PYGZus{}feature\PYGZus{}names}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}
                \PYG{n}{df}\PYG{o}{.}\PYG{n}{columns}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{)}
    \PYG{k}{return} \PYG{n}{important\PYGZus{}feature\PYGZus{}names}
    
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gt}{\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}}
\PYG{n+ne}{NameError}\PYG{g+gWhitespace}{                                 }Traceback (most recent call last)
\PYG{o}{\PYGZlt{}}\PYG{n}{ipython}\PYG{o}{\PYGZhy{}}\PYG{n+nb}{input}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{149}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{n}{df01e1de439}\PYG{o}{\PYGZgt{}} \PYG{o+ow}{in} \PYG{o}{\PYGZlt{}}\PYG{n}{module}\PYG{o}{\PYGZgt{}}
\PYG{n+ne}{\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZgt{} }\PYG{l+m+mi}{1} \PYG{k}{def} \PYG{n+nf}{idenity\PYGZus{}high\PYGZus{}corr\PYGZus{}features}\PYG{p}{(}\PYG{n}{df}\PYG{p}{:} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{)} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZgt{}} \PYG{n}{List}\PYG{p}{[}\PYG{n+nb}{str}\PYG{p}{]}\PYG{p}{:}
\PYG{g+gWhitespace}{      }\PYG{l+m+mi}{2}     \PYG{n}{importances} \PYG{o}{=} \PYG{n}{df}\PYG{o}{.}\PYG{n}{drop}\PYG{p}{(}
\PYG{g+gWhitespace}{      }\PYG{l+m+mi}{3}         \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{HeartDisease}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{.}\PYG{n}{apply}\PYG{p}{(}
\PYG{g+gWhitespace}{      }\PYG{l+m+mi}{4}             \PYG{k}{lambda} \PYG{n}{x}\PYG{p}{:} \PYG{n}{x}\PYG{o}{.}\PYG{n}{corr}\PYG{p}{(}\PYG{n}{df}\PYG{o}{.}\PYG{n}{HeartDisease}\PYG{p}{)}\PYG{p}{)}
\PYG{g+gWhitespace}{      }\PYG{l+m+mi}{5} 

\PYG{n+ne}{NameError}: name \PYGZsq{}List\PYGZsq{} is not defined
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{mi} \PYG{o}{=} \PYG{n}{mutual\PYGZus{}info\PYGZus{}classif}\PYG{p}{(}\PYG{n}{heart\PYGZus{}disease\PYGZus{}dataset\PYGZus{}standardized}\PYG{o}{.}\PYG{n}{drop}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{HeartDisease}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{,}
                         \PYG{n}{heart\PYGZus{}disease\PYGZus{}dataset\PYGZus{}standardized}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{HeartDisease}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{mi} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{Series}\PYG{p}{(}\PYG{n}{mi}\PYG{p}{)}
\PYG{n}{mi}\PYG{o}{.}\PYG{n}{index} \PYG{o}{=} \PYG{n}{heart\PYGZus{}disease\PYGZus{}dataset\PYGZus{}standardized}\PYG{o}{.}\PYG{n}{drop}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{HeartDisease}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{.}\PYG{n}{columns}
\PYG{n}{mi}\PYG{o}{.}\PYG{n}{sort\PYGZus{}values}\PYG{p}{(}\PYG{n}{ascending}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}\PYG{o}{.}\PYG{n}{plot}\PYG{o}{.}\PYG{n}{bar}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{10}\PYG{p}{,} \PYG{l+m+mi}{4}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}matplotlib.axes.\PYGZus{}subplots.AxesSubplot at 0x7faf3596b710\PYGZgt{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Model_evaluation_27_1}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}

\section{Hyperparameter finetuning}
\label{\detokenize{Model_evaluation:hyperparameter-finetuning}}
\sphinxAtStartPar
Our best algorithm was

\sphinxAtStartPar
Our final step is to fine\sphinxhyphen{}tune our model. We will use GridSearch to achieve this.


\section{Pickle Model}
\label{\detokenize{Model_evaluation:pickle-model}}
\sphinxAtStartPar
Pickle is the standard way of serializing objects in Python.

\sphinxAtStartPar
You can use the pickle operation to serialize your machine learning algorithms and save the serialized format to a file.

\sphinxAtStartPar
Later we will load this file to deserialize your model and use it to make new predictions in our web app.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{model}  \PYG{o}{=}  \PYG{n}{GaussianNB}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{model}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{X\PYGZus{}train}\PYG{p}{,} \PYG{n}{y\PYGZus{}train}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
GaussianNB()
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{set}\PYG{p}{(}\PYG{n}{X\PYGZus{}train}\PYG{o}{.}\PYG{n}{columns}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZob{}\PYGZsq{}AgeCategory\PYGZsq{},
 \PYGZsq{}AlcoholDrinking\PYGZsq{},
 \PYGZsq{}American Indian/Alaskan Native\PYGZsq{},
 \PYGZsq{}Asian\PYGZsq{},
 \PYGZsq{}Asthma\PYGZsq{},
 \PYGZsq{}BMI\PYGZsq{},
 \PYGZsq{}BMI\PYGZus{}Bin\PYGZsq{},
 \PYGZsq{}Black\PYGZsq{},
 \PYGZsq{}Diabetic\PYGZsq{},
 \PYGZsq{}DiffWalking\PYGZsq{},
 \PYGZsq{}Female\PYGZsq{},
 \PYGZsq{}GenHealth\PYGZsq{},
 \PYGZsq{}Hispanic\PYGZsq{},
 \PYGZsq{}KidneyDisease\PYGZsq{},
 \PYGZsq{}LOG\PYGZus{}BMI\PYGZsq{},
 \PYGZsq{}Male\PYGZsq{},
 \PYGZsq{}MentalHealth\PYGZsq{},
 \PYGZsq{}Other\PYGZsq{},
 \PYGZsq{}PhysicalActivity\PYGZsq{},
 \PYGZsq{}PhysicalHealth\PYGZsq{},
 \PYGZsq{}Race\PYGZus{}freq\PYGZsq{},
 \PYGZsq{}SkinCancer\PYGZsq{},
 \PYGZsq{}SleepTime\PYGZsq{},
 \PYGZsq{}Smoking\PYGZsq{},
 \PYGZsq{}Stroke\PYGZsq{},
 \PYGZsq{}White\PYGZsq{}\PYGZcb{}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{pickle}\PYG{o}{.}\PYG{n}{dump}\PYG{p}{(}\PYG{n}{model}\PYG{p}{,} \PYG{n+nb}{open}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{../app/model/finalized\PYGZus{}model.sav}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{wb}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxstepscope


\chapter{Conclusion}
\label{\detokenize{Conclusion:conclusion}}\label{\detokenize{Conclusion::doc}}

\bigskip\hrule\bigskip


\sphinxAtStartPar
In this investigation, we investigate the predictability of patients who have or did not have heart disease. Engineered features included a log transformed BMI variable to offset the effect of outliers in our model, a substance abuse binary variable (whether they were a heavy smoker or drinker) and two principal components of the dataset, in an attempt to reduce the dimensionality.  In addition, we augment the existing data by adding participants general BMI classification (underweight, normal weight, overweight obese). This enables us, to examine not only the the risk factors for heart disease but also the risk factors for heart disease in the context of a patient’s BMI.  Furthermore, our method of using K\sphinxhyphen{}means clustering to segment patients into groups with similar risk factors for heart disease is a our approach to understanding the risk factors for heart disease. We propose that this method of segmenting patients into groups with similar risk factors for heart disease can be used to identify patients that are at higher risk of developing heart disease and thus, we added this feature into our final model

\sphinxAtStartPar
Our most significant results are the following:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Exploratory data analysis contradicts  patients own self\sphinxhyphen{}reported health status. The majority of patients who have heart disease report having good or very good health, and the majority of patients who do not have heart disease report having fair or poor health. This suggests that the patients themselves are not aware of their heart disease status.

\item {} 
\sphinxAtStartPar
The most significant risk factors for heart disease for the general population are smoking, alcohol drinking, obesity, and diabetes.

\item {} 
\sphinxAtStartPar
Similarity, our findings suggest that patients who are obese and have a substance abuse problem are at a significantly higher risk of developing heart disease. This is not surprising as obesity is a well known risk factor for heart disease. However, the fact that patients who are obese and have a substance abuse problem are at a significantly higher risk of developing heart disease in the context of their BMI is more interesting. While this is not surprising, it does suggest that there are other factors that impact the risk of heart disease in obese patients.

\item {} 
\sphinxAtStartPar
Out of all the undersupplying and oversampling techniques we investigated to overcome the class imbalance problem in our dataset,  seemed to be most fruitful. This is because it oversampled the minority class (patients who have heart disease) to the same number of samples as the majority class (patients who do not have heart disease). This is important because it ensures that the model is not biased towards the majority class.

\item {} 
\sphinxAtStartPar
Our final model was serialized using the  pickle library. This allowed us to create a responsive web application. More details on the web app can be found in the \DUrole{xref,myst}{app directory}.

\end{enumerate}


\section{Future work}
\label{\detokenize{Conclusion:future-work}}
\sphinxAtStartPar
As an extension to this work, and some sort of limitation to the work performed here,
different types of classifiers can be included in the analysis and more in depth sensitivity analysis can be performed on these classifiers, also an extension can be made by applying same analysis to other bioinformatics diseases’ datasets, and see the performance of these classifiers to classify and predict these diseases. In addition, we would like to investigate the use of deeper models. Similar endeavours have shown to be fruitful, albeit often decreasing the interoperability of results.  Finally, we are interested in incorporating other features about the subjects such as socio\sphinxhyphen{}economic status,  heart disease prevalence in their family (measured on some continuum), their blood pressure and  cholesterol levels, and their dietary habitats. We hope such features might uncover specific genetic components  patterns or behavioural aspects that might increase or decrease the likehood of heart disease.

\sphinxstepscope


\chapter{References}
\label{\detokenize{References:references}}\label{\detokenize{References::doc}}

\bigskip\hrule\bigskip


\sphinxAtStartPar
@misc\{laya\_healthcare,
title=\{Worrying statistics about heart disease\}, url=\{\sphinxurl{https://www.layahealthcare.ie/pressandmedia/pressreleases/worrying-stats-highlight-heart-disease-risk-among-irish-adults-.html\#\_ftn3}\},
year=\{2020\},
journal=\{Laya Healthcare\}\}

\begin{sphinxthebibliography}{1}
\bibitem[1]{References:id7}
\sphinxAtStartPar
Amit Banerjee and Rajesh N Dave. Validating clusters using the hopkins statistic. In \sphinxstyleemphasis{2004 IEEE International conference on fuzzy systems (IEEE Cat. No. 04CH37542)}, volume 1, 149–153. IEEE, 2004.
\end{sphinxthebibliography}







\renewcommand{\indexname}{Index}
\printindex
\end{document}